{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CP2_03_Clean_Body_Text.ipynb","provenance":[{"file_id":"12PjhOUiDi-N0FXtlN7AFXyyGAiYi7AWj","timestamp":1601078329171},{"file_id":"1U1u9m5zkvqaI5GElpVlY4tU7zTNShmq9","timestamp":1599859230341},{"file_id":"1JbxiYiHD6fO2SZ9yLDoyqe9IkvHwJxp8","timestamp":1599777923777}],"authorship_tag":"ABX9TyMJ325YRpRmXAjkiTmMEmFS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vldA80BR2gik","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371259938,"user_tz":420,"elapsed":289,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"77c01c01-d5df-4934-956f-8e0686b70022"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BMCdDFsXr5rR"},"source":["# PseudoCode and Task List"]},{"cell_type":"markdown","metadata":{"id":"kxaQxoftDTp3"},"source":["1. Load the pickled pandas dataframe from 02 notebook \n","2. Examine contents to assure everything transferred properly - no issues found\n","3. Examine the feature 'Body' which are the questions to gain statistical insights and identify cleaning tasks needed\n","4.  Get counts of certain special characters to add to features prior to cleaning:\n",">4a. Questions marks\n",">4b. Text bolding\n",">4c. Number of paragraphs\n",">4d. Code examples\n","5. Cleaning tasks identified - (am skipping over mispellings because these will come out in the wash eventually)\n",">5a. Remove code snippets\n",">5b. Remove html formatting\n",">5c. Expand contractions\n",">5d. Language detection to make sure everything is in English\n",">5e. Remove special characters\n",">5f. Simple Lemmatization\n",">5g. Named Entity Recognition with Spacy\n",">5h. POS tagging\n",">5i. Convert to lowercase\n",">5j. Remove stop words\n","6. Export the dataframe with cleaned text and features for further analysis \n"]},{"cell_type":"markdown","metadata":{"id":"gJ3VoPrnsIkz"},"source":["# Tasks 1 and 2 Load file and examine contents"]},{"cell_type":"code","metadata":{"id":"J_JxbXJpYVoS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371268746,"user_tz":420,"elapsed":5290,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"aaf3eef3-1780-4d3d-9593-de3b47fc1839"},"source":["'''\n","Install required modules\n","'''\n","!pip install contractions\n","!pip install fasttext"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.45)\n","Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n","Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.2)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n","Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.2)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (51.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vt8U0qUv28dY"},"source":["'''\n","Import all modules that are needed\n","'''\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from bs4 import BeautifulSoup\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n","from nltk import StanfordTagger\n","import contractions\n","from contractions import contractions_dict\n","import fasttext\n","import collections\n","import unicodedata\n","import textwrap\n","import string\n","import spacy\n","from spacy import displacy\n","from collections import Counter\n","import en_core_web_sm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iOVoxuC4qBkG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371297530,"user_tz":420,"elapsed":455,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"e23fc0d8-3c48-4098-c9e2-e9150268d4ca"},"source":["'''\n","Download all nltk tools\n","'''\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"LIOI0tDD3Ke7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609360346987,"user_tz":420,"elapsed":1753,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"03e2ff4d-8183-4d7d-df59-6e3dec8f5bbb"},"source":["'''\n","1. Load the pickled pandas dataframe \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 24353 entries, 0 to 24352\n","Data columns (total 40 columns):\n"," #   Column               Non-Null Count  Dtype          \n","---  ------               --------------  -----          \n"," 0   Id                   24353 non-null  int64          \n"," 1   PostTypeId           24353 non-null  int64          \n"," 2   CreationDate         24353 non-null  datetime64[ns] \n"," 3   Score                24353 non-null  int64          \n"," 4   ViewCount            24353 non-null  int64          \n"," 5   Body                 24353 non-null  object         \n"," 6   OwnerUserId          24238 non-null  object         \n"," 7   LastActivityDate     24353 non-null  datetime64[ns] \n"," 8   Title                24353 non-null  object         \n"," 9   Tags                 24353 non-null  object         \n"," 10  AnswerCount          24353 non-null  int64          \n"," 11  CommentCount         24353 non-null  int64          \n"," 12  FavoriteCount        6708 non-null   object         \n"," 13  ClosedDate           1416 non-null   datetime64[ns] \n"," 14  ContentLicense       24353 non-null  object         \n"," 15  Tags_SpaceDelimited  24353 non-null  object         \n"," 16  Tags_Clean           24353 non-null  object         \n"," 17  TagCount             24353 non-null  int64          \n"," 18  Tag1                 24353 non-null  object         \n"," 19  Tag2                 21064 non-null  object         \n"," 20  Tag3                 15037 non-null  object         \n"," 21  Tag4                 8302 non-null   object         \n"," 22  Tag5                 3687 non-null   object         \n"," 23  Tag1_Freq            24353 non-null  int64          \n"," 24  Tag2_Freq            21064 non-null  float64        \n"," 25  Tag3_Freq            15037 non-null  float64        \n"," 26  Tag4_Freq            8302 non-null   float64        \n"," 27  Tag5_Freq            3687 non-null   float64        \n"," 28  Total_Tag_Freqency   24353 non-null  float64        \n"," 29  Tag1_Renamed         24353 non-null  object         \n"," 30  Tag2_Renamed         24353 non-null  object         \n"," 31  Tag3_Renamed         24353 non-null  object         \n"," 32  Tag4_Renamed         24353 non-null  object         \n"," 33  Tag5_Renamed         24353 non-null  object         \n"," 34  TopTag               24353 non-null  category       \n"," 35  Elapsed_Time         24353 non-null  timedelta64[ns]\n"," 36  Elapsed_Time_Int     24353 non-null  int16          \n"," 37  rank                 24353 non-null  int64          \n"," 38  Tag1_Renamed2        24353 non-null  object         \n"," 39  TopTag_Revised       24353 non-null  int64     \n","'''\n","\n","questions_df = pd.read_pickle('/content/drive/My Drive/Capstone2/Data/questions_df_09252020.pickle')\n","\n","questions_df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 24353 entries, 0 to 24352\n","Data columns (total 40 columns):\n"," #   Column               Non-Null Count  Dtype          \n","---  ------               --------------  -----          \n"," 0   Id                   24353 non-null  int64          \n"," 1   PostTypeId           24353 non-null  int64          \n"," 2   CreationDate         24353 non-null  datetime64[ns] \n"," 3   Score                24353 non-null  int64          \n"," 4   ViewCount            24353 non-null  int64          \n"," 5   Body                 24353 non-null  object         \n"," 6   OwnerUserId          24238 non-null  object         \n"," 7   LastActivityDate     24353 non-null  datetime64[ns] \n"," 8   Title                24353 non-null  object         \n"," 9   Tags                 24353 non-null  object         \n"," 10  AnswerCount          24353 non-null  int64          \n"," 11  CommentCount         24353 non-null  int64          \n"," 12  FavoriteCount        6708 non-null   object         \n"," 13  ClosedDate           1416 non-null   datetime64[ns] \n"," 14  ContentLicense       24353 non-null  object         \n"," 15  Tags_SpaceDelimited  24353 non-null  object         \n"," 16  Tags_Clean           24353 non-null  object         \n"," 17  TagCount             24353 non-null  int64          \n"," 18  Tag1                 24353 non-null  object         \n"," 19  Tag2                 21064 non-null  object         \n"," 20  Tag3                 15037 non-null  object         \n"," 21  Tag4                 8302 non-null   object         \n"," 22  Tag5                 3687 non-null   object         \n"," 23  Tag1_Freq            24353 non-null  int64          \n"," 24  Tag2_Freq            21064 non-null  float64        \n"," 25  Tag3_Freq            15037 non-null  float64        \n"," 26  Tag4_Freq            8302 non-null   float64        \n"," 27  Tag5_Freq            3687 non-null   float64        \n"," 28  Total_Tag_Freqency   24353 non-null  float64        \n"," 29  Tag1_Renamed         24353 non-null  object         \n"," 30  Tag2_Renamed         24353 non-null  object         \n"," 31  Tag3_Renamed         24353 non-null  object         \n"," 32  Tag4_Renamed         24353 non-null  object         \n"," 33  Tag5_Renamed         24353 non-null  object         \n"," 34  TopTag               24353 non-null  category       \n"," 35  Elapsed_Time         24353 non-null  timedelta64[ns]\n"," 36  Elapsed_Time_Int     24353 non-null  int16          \n"," 37  rank                 24353 non-null  int64          \n"," 38  Tag1_Renamed2        24353 non-null  object         \n"," 39  TopTag_Revised       24353 non-null  int64          \n","dtypes: category(1), datetime64[ns](3), float64(5), int16(1), int64(10), object(19), timedelta64[ns](1)\n","memory usage: 7.3+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rYJBGuegELBW","colab":{"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1609371306372,"user_tz":420,"elapsed":358,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"e4197f9d-4792-41d7-e927-03b9ee8d3951"},"source":["'''\n","2. Examine contents\n","'''\n","questions_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>PostTypeId</th>\n","      <th>CreationDate</th>\n","      <th>Score</th>\n","      <th>ViewCount</th>\n","      <th>Body</th>\n","      <th>OwnerUserId</th>\n","      <th>LastActivityDate</th>\n","      <th>Title</th>\n","      <th>Tags</th>\n","      <th>AnswerCount</th>\n","      <th>CommentCount</th>\n","      <th>FavoriteCount</th>\n","      <th>ClosedDate</th>\n","      <th>ContentLicense</th>\n","      <th>Tags_SpaceDelimited</th>\n","      <th>Tags_Clean</th>\n","      <th>TagCount</th>\n","      <th>Tag1</th>\n","      <th>Tag2</th>\n","      <th>Tag3</th>\n","      <th>Tag4</th>\n","      <th>Tag5</th>\n","      <th>Tag1_Freq</th>\n","      <th>Tag2_Freq</th>\n","      <th>Tag3_Freq</th>\n","      <th>Tag4_Freq</th>\n","      <th>Tag5_Freq</th>\n","      <th>Total_Tag_Freqency</th>\n","      <th>Tag1_Renamed</th>\n","      <th>Tag2_Renamed</th>\n","      <th>Tag3_Renamed</th>\n","      <th>Tag4_Renamed</th>\n","      <th>Tag5_Renamed</th>\n","      <th>TopTag</th>\n","      <th>Elapsed_Time</th>\n","      <th>Elapsed_Time_Int</th>\n","      <th>rank</th>\n","      <th>Tag1_Renamed2</th>\n","      <th>TopTag_Revised</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2014-05-13 23:58:30.457</td>\n","      <td>9</td>\n","      <td>708</td>\n","      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n","      <td>5</td>\n","      <td>2014-05-14 00:36:31.077</td>\n","      <td>How can I do simple machine learning without h...</td>\n","      <td>&lt;machine-learning&gt;</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2014-05-14 14:40:25.950</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>machine-learning</td>\n","      <td>[machine-learning]</td>\n","      <td>1</td>\n","      <td>machine-learning</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7766</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7766.0</td>\n","      <td>machine-learning</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>0 days 00:38:00.620000</td>\n","      <td>0</td>\n","      <td>21642</td>\n","      <td>machine-learning</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>2014-05-14 00:11:06.457</td>\n","      <td>4</td>\n","      <td>441</td>\n","      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n","      <td>36</td>\n","      <td>2014-05-16 13:45:00.237</td>\n","      <td>What open-source books (or other materials) pr...</td>\n","      <td>&lt;education&gt;&lt;open-source&gt;</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2014-05-14 08:40:54.950</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>education open-source</td>\n","      <td>[education, open-source]</td>\n","      <td>2</td>\n","      <td>education</td>\n","      <td>open-source</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>33</td>\n","      <td>16.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>49.0</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2 days 13:33:53.780000</td>\n","      <td>2</td>\n","      <td>16792</td>\n","      <td>Other</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>2014-05-14 01:25:59.677</td>\n","      <td>22</td>\n","      <td>1717</td>\n","      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n","      <td>66</td>\n","      <td>2014-06-20 17:36:05.023</td>\n","      <td>Is Data Science the Same as Data Mining?</td>\n","      <td>&lt;data-mining&gt;&lt;definitions&gt;</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>data-mining definitions</td>\n","      <td>[data-mining, definitions]</td>\n","      <td>2</td>\n","      <td>data-mining</td>\n","      <td>definitions</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1005</td>\n","      <td>31.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1036.0</td>\n","      <td>data-mining</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>37 days 16:10:05.346000</td>\n","      <td>37</td>\n","      <td>15799</td>\n","      <td>data-mining</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>2014-05-14 01:41:23.110</td>\n","      <td>2</td>\n","      <td>643</td>\n","      <td>&lt;p&gt;In which situations would one system be pre...</td>\n","      <td>64</td>\n","      <td>2014-05-14 01:41:23.110</td>\n","      <td>What are the advantages and disadvantages of S...</td>\n","      <td>&lt;databases&gt;</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>2014-05-14 07:41:49.437</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>databases</td>\n","      <td>[databases]</td>\n","      <td>1</td>\n","      <td>databases</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>89</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>89.0</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0 days 00:00:00</td>\n","      <td>0</td>\n","      <td>21681</td>\n","      <td>Other</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>2014-05-14 01:57:56.880</td>\n","      <td>17</td>\n","      <td>382</td>\n","      <td>&lt;p&gt;I use &lt;a href=\"http://www.csie.ntu.edu.tw/~...</td>\n","      <td>63</td>\n","      <td>2014-05-17 16:24:14.523</td>\n","      <td>Use liblinear on big data for semantic analysis</td>\n","      <td>&lt;machine-learning&gt;&lt;bigdata&gt;&lt;libsvm&gt;</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>machine-learning bigdata libsvm</td>\n","      <td>[machine-learning, bigdata, libsvm]</td>\n","      <td>3</td>\n","      <td>machine-learning</td>\n","      <td>bigdata</td>\n","      <td>libsvm</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7766</td>\n","      <td>433.0</td>\n","      <td>14.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>8213.0</td>\n","      <td>machine-learning</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>3 days 14:26:17.643000</td>\n","      <td>3</td>\n","      <td>10411</td>\n","      <td>machine-learning</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  PostTypeId  ...     Tag1_Renamed2  TopTag_Revised\n","0   5           1  ...  machine-learning               1\n","1   7           1  ...             Other               0\n","2  14           1  ...       data-mining               1\n","3  15           1  ...             Other               0\n","4  16           1  ...  machine-learning               1\n","\n","[5 rows x 40 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"Yjn9a6nIrgMo"},"source":["# Task 3: Examine 'Body' Feature \n","Examine the feature 'Body' which are the questions to gain statistical insights and identify cleaning tasks needed"]},{"cell_type":"code","metadata":{"id":"fg-jk-8zDsGF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609364207917,"user_tz":420,"elapsed":388,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"9064f894-41e1-405d-c61f-28e8327c7b96"},"source":["'''\n","3. Examine the feature 'Body' which are the questions and identify cleaning tasks needed; \n","first show the Top Tag questions with highest and lowest rank\n","\n","Notice that the lower ranked questions are oftentimes shorter, simpler, and specific to one narrow topic rather than the higher ranked questions \n","which tend to be broader or higher level questions\n","(correlated with TagCount)\n","\n","4 of the top 5 ranked questions are about machine-learning\n","\n","The number of questions asked is variable, as is the length of the question - in some instances there is not really a question but a statement \n","\n","So a count of question marks in the question body and length of the question body are good features to add and analyze\n","\n","Cleaning items include html / xml formatting removal, special character removal, and expanding contractions\n","Also there are many code snippets and hyperlinks that should probably be removed\n","Certain acronyms like LDA, PCA, SGD are important to recognize - in a future step, must use POS tagging to identify these\n","'''\n","toptagQuestions = questions_df.loc[questions_df[\"TopTag_Revised\"] == 1]\n","toptagrank = list(zip(toptagQuestions[\"Id\"],toptagQuestions[\"rank\"],toptagQuestions[\"Tags_SpaceDelimited\"],toptagQuestions[\"Title\"],toptagQuestions[\"Body\"]))\n","toptagrank.sort(key=lambda x: x[1],reverse=False)\n","\n","for id,r,tag,t,b in toptagrank[:10]:\n","    print(\"HighestRank\", \"*\" * 148, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Rank :\",r)\n","    print(\"Question Tags\\t:\",tag) \n","    print(\"Question Title\\t:\",t) \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')\n","for id,r,tag,t,b in toptagrank[-10:]:\n","    print(\"LowestRank\", \"*\" * 143, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Rank :\",r)\n","    print(\"Question Tags\\t:\",tag) \n","    print(\"Question Title\\t:\",t) \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 22\n","Rank : 1\n","Question Tags\t: data-mining clustering octave k-means categorical-data\n","Question Title\t: K-Means clustering for mixed numeric and categorical data\n","Question Body\t:\n","\n","<p>My data set contains a number of numeric attributes and one categorical.</p>  <p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN,\n","CategoricalAttr</code>, </p>  <p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>,\n","<code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>  <p>I'm using default k-means clustering algorithm implementation for Octave <a\n","href=\"https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/\">https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-\n","octave/</a>. It works with numeric data only.</p>  <p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three\n","numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 14899\n","Rank : 2\n","Question Tags\t: machine-learning neural-network deep-learning svm software-recommendation\n","Question Title\t: How to draw Deep learning network architecture diagrams?\n","Question Body\t:\n","\n","<p>I have built my model. Now I want to draw the network architecture diagram for my research paper. Example is shown below:</p>  <p><a\n","href=\"https://i.stack.imgur.com/zyIUI.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/zyIUI.png\" alt=\"enter image description here\"></a></p>  <p><a\n","href=\"https://i.stack.imgur.com/CHuCF.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/CHuCF.png\" alt=\"enter image description here\"></a></p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 410\n","Rank : 3\n","Question Tags\t: machine-learning neural-network deep-learning optimization hyperparameter\n","Question Title\t: Choosing a learning rate\n","Question Body\t:\n","\n","<p>I'm currently working on implementing Stochastic Gradient Descent, <code>SGD</code>, for neural nets using back-propagation, and while I understand its\n","purpose I have some questions about how to choose values for the learning rate.</p>  <ul> <li>Is the learning rate related to the shape of the error gradient,\n","as it dictates the rate of descent?</li> <li>If so, how do you use this information to inform your decision about a value?</li> <li>If it's not what sort of\n","values should I choose, and how should I choose them?</li> <li>It seems like you would want small values to avoid overshooting, but how do you choose one such\n","that you don't get stuck in local minima or take to long to descend?</li> <li>Does it make sense to have a constant learning rate, or should I use some metric\n","to alter its value as I get nearer a minimum in the gradient?</li> </ul>  <p>In short: How do I choose the learning rate for SGD?</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 5226\n","Rank : 4\n","Question Tags\t: machine-learning python scikit-learn random-forest decision-trees\n","Question Title\t: strings as features in decision tree/random forest\n","Question Body\t:\n","\n","<p>I am doing some problems on an application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country\n","name) as features. Now the library, <a href=\"http://scikit-learn.org\" rel=\"noreferrer\">scikit-learn</a> takes only numbers as parameters, but I want to inject\n","the strings as well as they carry a significant amount of knowledge.</p>  <p>How do I handle such a scenario?</p>  <p>I can convert a string to numbers by some\n","mechanism such as hashing in Python. But I would like to know the best practice on how strings are handled in decision tree problems.</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 6547\n","Rank : 5\n","Question Tags\t: machine-learning python data-mining anomaly-detection library\n","Question Title\t: Open source Anomaly Detection in Python\n","Question Body\t:\n","\n","<p><strong>Problem Background:</strong> I am working on a project that involves log files similar to those found in the IT monitoring space (to my best\n","understanding of IT space). These log files are time-series data, organized into hundreds/thousands of rows of various parameters. Each parameter is numeric\n","(float) and there is a non-trivial/non-error value for each time point. My task is to monitor said log files for anomaly detection (spikes, falls, unusual\n","patterns with some parameters being out of sync, strange 1st/2nd/etc. derivative behavior, etc.). </p>  <p>On a similar assignment, I have tried Splunk with\n","Prelert, but I am exploring open-source options at the moment.</p>  <p><strong>Constraints:</strong> I am limiting myself to Python because I know it well, and\n","would like to delay the switch to R and the associated learning curve. Unless there seems to be overwhelming support for R (or other languages/software), I\n","would like to stick to Python for this task.</p>  <p>Also, I am working in a Windows environment for the moment. I would like to continue to sandbox in Windows\n","on small-sized log files but can move to Linux environment if needed. </p>  <p><strong>Resources:</strong> I have checked out the following with dead-ends as\n","results:</p>  <ol> <li><p><a href=\"https://datascience.stackexchange.com/questions/5193/python-or-r-for-implementing-machine-learning-algorithms-for-fraud-\n","detection\">Python or R for implementing machine learning algorithms for fraud detection</a>. Some info here is helpful, but unfortunately, I am struggling to\n","find the right package because:</p></li> <li><p>Twitter's \"AnomalyDetection\" is in R, and I want to stick to Python. Furthermore, the Python port <a\n","href=\"https://github.com/nicolasmiller/pyculiarity\" rel=\"nofollow noreferrer\">pyculiarity</a> seems to cause issues in implementing in Windows environment for\n","me. </p></li> <li><p>Skyline, my next attempt, seems to have been pretty much discontinued (from <a href=\"https://github.com/etsy/skyline/issues/119\"\n","rel=\"nofollow noreferrer\">github issues</a>). I haven't dived deep into this, given how little support there seems to be online.  </p></li> <li><p>scikit-learn\n","I am still exploring, but this seems to be much more manual. The down-in-the-weeds approach is OK by me, but my background in learning tools is weak, so would\n","like something like a black box for the technical aspects like algorithms, similar to Splunk+Prelert.</p></li> </ol>  <p><strong>Problem Definition and\n","Questions:</strong> I am looking for open-source software that can help me with automating the process of anomaly detection from time-series log files in Python\n","via packages or libraries. </p>  <ol start=\"5\"> <li>Do such things exist to assist with my immediate task, or are they imaginary in my mind? </li> <li>Can\n","anyone assist with concrete steps to help me to my goal, including background fundamentals or concepts? </li> <li>Is this the best StackExchange community to\n","ask in, or is Stats, Math, or even Security or Stackoverflow the better options?</li> </ol>  <p><strong>EDIT [2015-07-23]</strong>  Note that the latest update\n","to <a href=\"https://github.com/nicolasmiller/pyculiarity\" rel=\"nofollow noreferrer\">pyculiarity</a> seems to be <a\n","href=\"https://github.com/nicolasmiller/pyculiarity/issues/1\" rel=\"nofollow noreferrer\">fixed</a> for the Windows environment! I have yet to confirm, but should\n","be another useful tool for the community.</p>  <p><strong>EDIT [2016-01-19]</strong> A minor update. I had not time to work on this and research, but I am\n","taking a step back to understand the fundamentals of this problem before continuing to research in specific details. For example, two concrete steps that I am\n","taking are:</p>  <ol> <li><p>Starting with the Wikipedia articles for anomaly detection [<a href=\"https://en.wikipedia.org/wiki/Anomaly_detection\" rel=\"nofollow\n","noreferrer\">https://en.wikipedia.org/wiki/Anomaly_detection</a> ], understanding fully, and then either moving up or down in concept hierarchy of other linked\n","Wikipedia articles, such as [<a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" rel=\"nofollow\n","noreferrer\">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a> ], and then to [<a href=\"https://en.wikipedia.org/wiki/Machine_learning\"\n","rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/Machine_learning</a> ].</p></li> <li><p>Exploring techniques in the great surveys done by Chandola et al\n","2009 \"Anomaly Detection: A Survey\"[<a href=\"http://www-users.cs.umn.edu/~banerjee/papers/09/anomaly.pdf\" rel=\"nofollow noreferrer\">http://www-\n","users.cs.umn.edu/~banerjee/papers/09/anomaly.pdf</a> ] and Hodge et al 2004 \"A Survey of Outlier Detection Methodologies\"[<a\n","href=\"http://eprints.whiterose.ac.uk/767/1/hodgevj4.pdf\" rel=\"nofollow noreferrer\">http://eprints.whiterose.ac.uk/767/1/hodgevj4.pdf</a> ]. </p></li> </ol>\n","<p>Once the concepts are better understood (I hope to play around with toy examples as I go to develop the practical side as well), I hope to understand which\n","open source Python tools are better suited for my problems.</p>  <p><strong>EDIT [2020-02-04]</strong> It has been a few years since I worked on this problem,\n","and am no longer working on this project, so I will not be following or researching this area until further notice. Thank you very much to all for their input.\n","I hope this discussion helps others that need guidance on anomaly detection work.</p>  <p>FWIW, if I had to do the same project now with the same resources (few\n","thousand USD in expenses), I would pursue the deep learning/neural network approach. The ability of the method to automatically learn structure and hierarchy\n","via hidden layers would've been very appealing since we had lots of data and (now) could spend the money on cloud compute. I would still use Python though\n",";).</p>  <p>Cheers!</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 37428\n","Rank : 6\n","Question Tags\t: python visualization jupyter anaconda graphviz\n","Question Title\t: GraphViz not working when imported inside PydotPlus (`GraphViz's executables not found`)\n","Question Body\t:\n","\n","<p>I've been trying to make these packages work for quite some time now but with no success. Basically the error is:</p>  <pre><code>GraphViz's Executables not\n","found </code></pre>  <p><strong>EDIT</strong>: I had not posted a terminal <code>log</code> with the error originally. I'm using <code>Ubuntu</code> now so I\n","won't be able to reproduce the exact same error I got in the past (a year ago, so far away in the past...). However, I've been experiencing a similar --- if not\n","the same --- error in my current setup; even while using a virtual environment with <code>pipenv</code>. The error seems to come from lines that were described\n","in <a href=\"https://datascience.stackexchange.com/a/48563/57429\">@张乾元's answer</a>:</p>  <pre><code>Traceback (most recent call last):   File \"example.py\", line\n","49, in &lt;module&gt;     Image(graph.create_png())   File \"/home/philippe/.local/lib/python3.6/site-packages/pydotplus/graphviz.py\", line 1797, in\n","&lt;lambda&gt;     lambda f=frmt, prog=self.prog: self.create(format=f, prog=prog)   File \"/home/philippe/.local/lib/python3.6/site-\n","packages/pydotplus/graphviz.py\", line 1960, in create     'GraphViz\\'s executables not found') pydotplus.graphviz.InvocationException: GraphViz's executables\n","not found </code></pre>  <p>I've tried to install <code>GraphViz</code> via 2 different ways: via <code>pip install graphviz</code> and through the\n","<code>.msi</code> package (and also tried to install <code>pydot</code>, <code>pydotplus</code> and <code>graphviz</code> in many different orders).</p>  <p>The\n","code I'm trying to run is simply a <code>dot-to-png</code> converter for the <a href=\"https://scikit-\n","learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\" rel=\"nofollow noreferrer\">Iris Dataset</a>.</p>  <pre class=\"lang-py prettyprint-\n","override\"><code>from sklearn.tree import DecisionTreeClassifier import sklearn.datasets as datasets from sklearn.externals.six import StringIO from sklearn.tree\n","import export_graphviz  import pandas as pd import pydotplus  from IPython.display import Image  iris = datasets.load_iris() df = pd.DataFrame(iris.data,\n","columns = iris.feature_names) y = iris.target  dtree = DecisionTreeClassifier() dtree.fit(df,y)  dot_data = StringIO() export_graphviz(     dtree,      out_file\n","= dot_data,     filled = True,      rounded = True,     special_characters = True ) graph_1 = pydotplus.graph_from_dot_data(dot_data.getvalue())\n","Image(graph_1.create_png()) </code></pre>  <p>In <code>Jupyter Notebooks</code> and in <code>Atom</code>, the system seems to be looking for\n","<code>GraphViz</code> inside <code>pydotplus</code>, as it points to <code>~\\Anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py</code>. Shouldn't it be the other\n","way around?</p>  <p>Lastly, I just want to point out that I've already tried adding <code>GraphViz</code>'s path to the system's <code>PATH</code> using\n","<code>C:\\Users\\Philippe\\Anaconda3\\Library\\bin\\graphviz</code>.</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 761\n","Rank : 7\n","Question Tags\t: machine-learning python clustering k-means geospatial\n","Question Title\t: Clustering geo location coordinates (lat,long pairs)\n","Question Body\t:\n","\n","<p>What is the right approach and clustering algorithm for geolocation clustering?</p>  <p>I'm using the following code to cluster geolocation coordinates:</p>\n","<pre><code>import numpy as np import matplotlib.pyplot as plt from scipy.cluster.vq import kmeans2, whiten  coordinates= np.array([            [lat, long],\n","[lat, long],             ...            [lat, long]            ]) x, y = kmeans2(whiten(coordinates), 3, iter = 20)   plt.scatter(coordinates[:,0],\n","coordinates[:,1], c=y); plt.show() </code></pre>  <p>Is it right to use K-means for geolocation clustering, as it uses Euclidean distance, and not <a\n","href=\"https://en.wikipedia.org/wiki/Haversine_formula\" rel=\"noreferrer\">Haversine formula</a> as a distance function?</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 29480\n","Rank : 8\n","Question Tags\t: machine-learning neural-network deep-learning dataset colab\n","Question Title\t: Uploading images folder from my system into Google Colab\n","Question Body\t:\n","\n","<p>I want to train a deep learning model on a dataset containing around 3000 images. Since the dataset is huge, I want to use Google colab since it's GPU\n","supported. How do I upload this full image folder into my notebook and use it?</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 16904\n","Rank : 9\n","Question Tags\t: machine-learning algorithms xgboost ensemble-modeling gbm\n","Question Title\t: GBM vs XGBOOST? Key differences?\n","Question Body\t:\n","\n","<p>I am trying to understand the key differences between GBM and XGBOOST. I tried to google it, but could not find any good answers explaining the differences\n","between the two algorithms and why xgboost almost always performs better than GBM. What makes XGBOOST so fast?</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank **************************************************************************************************************************************************** \n","\n","Question id: 10048\n","Rank : 10\n","Question Tags\t: python neural-network classification clustering keras\n","Question Title\t: What is the best Keras model for multi-class classification?\n","Question Body\t:\n","\n","<p>I am working on research, where need to classify one of three event WINNER=(<code>win</code>, <code>draw</code>, <code>lose</code>)</p>  <pre><code>WINNER\n","LEAGUE  HOME    AWAY    MATCH_HOME  MATCH_DRAW  MATCH_AWAY  MATCH_U2_50 MATCH_O2_50 3         13    550      571          1.86        3.34        4.23\n","1.66     2.11 3         7     322     334           7.55         4.1         1.4       2.17     1.61 </code></pre>  <p>My current model is:</p>  <pre><code>def\n","build_model(input_dim, output_classes):     model = Sequential()     model.add(Dense(input_dim=input_dim, output_dim=12, activation=relu))\n","model.add(Dropout(0.5))     model.add(Dense(output_dim=output_classes, activation='softmax'))     model.compile(loss='categorical_crossentropy',\n","optimizer='adadelta')     return model </code></pre>  <ol> <li>I am not sure that is the correct one for multi-class classification</li> <li>What is the best\n","setup for binary classification?</li> </ol>  <p>EDIT: #2 - Like that?</p>  <pre><code>model.add(Dense(input_dim=input_dim, output_dim=12, activation='sigmoid'))\n","model.add(Dropout(0.5)) model.add(Dense(output_dim=output_classes, activation='softmax')) model.compile(loss='binary_crossentropy', optimizer='adadelta')\n","</code></pre>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 74969\n","Rank : 24339\n","Question Tags\t: time-series\n","Question Title\t: Measure oscillation of time series\n","Question Body\t:\n","\n","<p>Suppose you have a time series [0, 100, 0, 100, ....] and another one [0, 0,...,0, 100,...100]. Same number of data points, same number of 0s and 100s. Their\n","moments , mean, variance higher-order, are identical. I seek some method that measures some notion of stability ,eg the first one would have low stability-high\n","oscillation and the opposite would be true for the second one.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 31757\n","Rank : 24340\n","Question Tags\t: data\n","Question Title\t: Where can I find internet data usage stats?\n","Question Body\t:\n","\n","<p>I need stats on how much data the average person uses each day on the internet. Web browsing, emailing, watching netflix, etc.</p>  <p>Not just \"mobile data\"\n","but all data, including cable or fiber or whatever wired connection is used at home.</p>  <p>If the stats are monthly (or something else) then that is fine, I\n","can extract daily stats from that.</p>  <p>I thought this would have been pretty easy to find but I'm coming up blank!</p>  <p>Can you point me in the right\n","direction?</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 72952\n","Rank : 24341\n","Question Tags\t: reinforcement-learning\n","Question Title\t: Design reinforcement learning model to explore all optimal solutions?\n","Question Body\t:\n","\n","<p>I am working to use <code>DQN</code> and <code>Policy Gradient</code> reinforcement learning models to solve classic maze escaping problems. </p>  <p>So far\n","I have been able to train a model which, after around 100 episodes, quickly explored ONE optimal solution to escape mazes.</p>  <p>However, it is easy to see\n","that for many maze designs, the optimal solutions could be multiple, and I would to take one step further to collect <strong>all optimal and distinguishable\n","solutions</strong>. </p>  <p>However, I tried some searches online and so far, the only material I can find is this <a\n","href=\"https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\" rel=\"nofollow noreferrer\">Learning Diverse Skills</a>. But this seems obstacle to me. I\n","somewhat believe this seems a classic (?) and easier problem that should be addressed in textbook? </p>  <p>Could someone shed lights on this matter? Thank you\n","very much. </p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 75123\n","Rank : 24342\n","Question Tags\t: reinforcement-learning\n","Question Title\t: Defining State - Action Feature for Snake\n","Question Body\t:\n","\n","<p>I was recently having a lot of trouble with a problem I was trying to deal with. I coded a game of snake and tried to use linear function approximation with\n","SARSA(lambda) using the true online update to try and get at least somewhere.</p>  <p>The problem I was having was in defining the features. I used </p>\n","<p>x(S,A) = (xdist to apple//cellsize, ydist to apple//cellsize, vision[0][0],...., vision[i][i]) where vision is a list of binary features concerning squares\n","around the snake's head. </p>  <p>For example if |vision| was 3^2, you would first form a three by three grid with the middle square being the snake's head and\n","then for vision[a][b] you would go to row a column b and return 1 if that square held something that was something that could kill the snake (boundaries\n","included) and 0 if it was safe. </p>  <p>I gave it 40 reward for eating an apple, -10 reward for a boundary collision or collision with itself, -18 reward if it\n","ran out of moves, and -1 reward otherwise. </p>  <p>When I tried to use this however, no value of |vision|, no change in the rewards, and no amount of training\n","could make the snake ever get more than the 1 apple that always starts ahead of it. For this example and in general, is there a good and concrete way to define\n","the state-action feature vector.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 60064\n","Rank : 24345\n","Question Tags\t: r\n","Question Title\t: how to replace loadings in factanal object in r\n","Question Body\t:\n","\n","<p>I am trying to create a diagram but the function for it from package library(semPlot) takes an object. I can use factanal object or many other objects from\n","FA packages.</p>  <p>I chose factanal for simplicity and tried to say:</p>  <pre><code>fa_obj &lt;- factanal(x) replace loadings in fa_obj with my own matrix of\n","loadings A. but I get an error : Error in semPlotModel.default(loadings(object)) :    Object not recognized as SEM model  fa_obj$loadings &lt;-  A </code></pre>\n","<p>My issue probably stems from other data in the object doesn't match my loadings.</p>  <p>Anyway, what is the easiest object I can create to be able to use\n","pat diagram tool?</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 73501\n","Rank : 24346\n","Question Tags\t: reinforcement-learning\n","Question Title\t: Separation line between solvable and insolvable cases in Multi-armed bandit\n","Question Body\t:\n","\n","<p>Consider the multi-armed bandits game with the following rules:<br> We have 10 buckets each initialized as 2. Whenever you play n-th strategy on t-th step\n","you add t to n-th bucket you add t to the content of the n-th bucket. If the result is prime you get +1. If not you get -1. Which learning strategy works the\n","best in this environment if the rules are not known to the player?</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 75120\n","Rank : 24347\n","Question Tags\t: reinforcement-learning\n","Question Title\t: reinforcement learning with both dynamic and static states\n","Question Body\t:\n","\n","<p>I am wondering is there any algorithm accommodate both dynamic and static states?</p>  <p>In my problem, some states change over time depending on the\n","actions taken. Other states are static. I am aware that contextual bandit is appropriate for static states. </p>  <p>But is traditional reinforcement learning\n","algorithm, like Q learning, appropriate for a problem where states contain both dynamic ones and static ones.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 74623\n","Rank : 24349\n","Question Tags\t: pandas\n","Question Title\t: Loading a Single Series from a Pickled DataFrame in Pandas\n","Question Body\t:\n","\n","<p>After saving a Pandas After saving a Pandas DataFrame with <code>df.to_pickle(file_name)</code>, it can be loaded with <code>df =\n","pd.read_pickle(file_name)</code>. But sometimes, you may only want to load the data for one Series at a particular time, and loading the entire DataFrame is\n","inefficient. Is there a way to load just a single Series from a pickled DataFrame?</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 75121\n","Rank : 24351\n","Question Tags\t: python\n","Question Title\t: What Python algorithms for fitting a curve for a 3D elliptic point cloud to find about its \"curvature\"?\n","Question Body\t:\n","\n","<p>What Python algorithms for fitting a curve for a 3D elliptic point cloud to find about its \"curvature\"?</p>  <p>Particularly it's a point cloud of a tree and\n","I want to measure the curvature of its top.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank *********************************************************************************************************************************************** \n","\n","Question id: 73249\n","Rank : 24352\n","Question Tags\t: cnn\n","Question Title\t: Is finite context length really a major concern in CNN model trained for classification using fixed-length 1D time-series data?\n","Question Body\t:\n","\n","<p>The finite context length issue basically happens in the regression problem as per my understanding. However, I am not sure the CNN model specially trained\n","with time-series data for classification problems can have finite context length issues. How can we avoid finite context length issue in the CNN model for time\n","series classification problem?</p>\n","**************************************************************************************************************************************************************** \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YmNqoX1ou3bH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371317227,"user_tz":420,"elapsed":328,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"8e41ef74-3bdf-495c-febe-dd333c3d294c"},"source":["'''\n","3a. Now look at questions with less frequent tags in the same fashion (not on TopTag List)\n","\n","Additional cleaning items - what to do about mispellings - \"bot\" used instead of \"but\", but \"bot\" is an important word in Data science\n","'''\n","btmtagQuestions = questions_df.loc[questions_df[\"TopTag_Revised\"] == 0]\n","\n","btmtagvcrank = list(zip(btmtagQuestions[\"Id\"],btmtagQuestions[\"rank\"],btmtagQuestions[\"Tags_SpaceDelimited\"],btmtagQuestions[\"Title\"],btmtagQuestions[\"Body\"]))\n","btmtagvcrank.sort(key=lambda x: x[1],reverse=False)\n","\n","for id,r,tag,t,b in btmtagvcrank[:10]:\n","    print(\"HighestRank\", \"*\" * 125, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Rank:\",r)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t) \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')\n","for id,r,tag,t,b in btmtagvcrank[-10:]:\n","    print(\"LowestRank\", \"*\" * 126, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Rank:\",r)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["HighestRank ***************************************************************************************************************************** \n","\n","Question id: 29851\n","Rank: 105\n","Question Tags\t: data-cleaning preprocessing word-embeddings encoding embeddings \n","\n","Question Title\t: One Hot Encoding vs Word Embeding - When to choose one or another?\n","Question Body\t:\n","\n","<p>A colleague of mine is having an interesting situation, he has quite a large set of possibilities for a defined categorical feature (+/- 300 different\n","values)</p>  <p>The usual data science approach would be to perform a One-Hot Encoding. However, wouldn't it be a bit extreme to perform some One-Hot Encoding\n","with a dictionary quite large (+/- 300 values) ? Is there any best practice on when to choose Embedding vectors or One-Hot Encoding ?</p>  <hr>  <p>Additional,\n","information: how would you handle the previous case if new values can be added to the dictionary. Re-training seems the only solution, however with One-Hot\n","Encoding, the data dimension will simultaniously increase which may lead to additional troubles, embedding vectors, on the opposite side, can keep the same\n","dimension even if new values appears.</p>  <p>How would you handle such a case ? Embedding vectors clearly seem more appropriate to me, however I would like to\n","validate my opinion and check if there is another solution that could be more apporiate.</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 66350\n","Rank: 162\n","Question Tags\t: machine-learning-model training supervised-learning accuracy overfitting \n","\n","Question Title\t: What would I prefer - an over-fitted model or a less accurate model?\n","Question Body\t:\n","\n","<p>Let's say we have two models trained. And let's say we are looking for good accuracy.  The first has an accuracy of 100% on training set and 84% on test set.\n","Clearly over-fitted. The second has an accuracy of 83% on training set and 83% on test set. </p>  <p>On the one hand, model #1 is over-fitted but on the other\n","hand it still yields better performance on an unseen test set than the good general model in #2. </p>  <p>Which model would you choose to use in production? The\n","First or the Second and why?</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 24760\n","Rank: 226\n","Question Tags\t: algorithms anomaly-detection outlier terminology definitions \n","\n","Question Title\t: What is the difference between outlier detection and anomaly detection?\n","Question Body\t:\n","\n","<p>I would like to know the difference in terms of applications (e.g. which one is credit card fraud detection?) and in terms of used  techniques.</p>\n","<p>Example papers which define the task would be welcome.</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 24370\n","Rank: 254\n","Question Tags\t: similarity information-retrieval ranking cosine-distance similar-documents \n","\n","Question Title\t: Cosine similarity between query and document confusion\n","Question Body\t:\n","\n","<p>I am going through the Manning book for Information retrieval. Currently I am at the part about cosine similarity. One thing is not clear for me.  </p>\n","<p>Let's say that I have the tf idf vectors for the query and a document. I want to compute the cosine similarity between both vectors. </p>  <p>When I compute\n","the magnitude for the document vector, do I sum the squares of all the terms in the vector or just the terms in the query?   </p>  <p>Here is an example : we\n","have user query \"cat food beef\" .<br> Lets say its vector is (0,1,0,1,1).( assume there are only 5 directions  in the vector one for each unique word in the\n","query and the document)<br> We have a document \"Beef is delicious\"<br>  Its vector is (1,1,1,0,0). We want to find the cosine similarity between the query and\n","the document vectors. </p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 27277\n","Rank: 326\n","Question Tags\t: convnet image-classification computer-vision object-detection faster-rcnn \n","\n","Question Title\t: Faster-RCNN how anchor work with slider in RPN layer?\n","Question Body\t:\n","\n","<p>I am trying to understand the whole Faster-RCNN, </p>  <p>From  <a href=\"https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work\"\n","rel=\"noreferrer\">https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work</a></p>  <blockquote>   <p>Then a sliding window is run\n","spatially on these feature maps. The size of sliding window is n×n (here 3×3). For each sliding window, a set of 9 anchors are generated which all have the same\n","center (xa,ya)(xa,ya) but with 3 different aspect ratios and 3 different scales as shown below. Note that all these coordinates are computed with respect to the\n","original image.</p> </blockquote>  <p><a href=\"https://i.stack.imgur.com/PWd2d.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/PWd2d.png\" alt=\"enter\n","image description here\"></a></p>  <p>It is much more clear than other articles for my opion, but still hard to understand how the feature map generate.</p>\n","<p>I saw another flow pics : <a href=\"https://i.stack.imgur.com/ci0Vw.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/ci0Vw.png\" alt=\"enter image\n","description here\"></a> <a href=\"https://i.stack.imgur.com/gpj2I.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/gpj2I.png\" alt=\"enter image\n","description here\"></a></p>  <p><a href=\"https://i.stack.imgur.com/veyVL.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/veyVL.png\" alt=\"enter image\n","description here\"></a></p>  <p>The problem, I write below steps for example:</p>  <ol> <li>If input is 600x1000x3 pic</li> <li>Through VGG16 convnet , layer 13\n","output feature map is 40x60x512</li> <li>Use a 3x3 sliding window, generate 1x1x512 feature map ???</li> </ol>  <p>Here, how 3x3 sliding window use a set of 9\n","anchors ??? </p>  <p>Sorry, I am really new to object detection and image proccessing.</p>  <p>I only have a little understand about the steps, I known\n","<strong>9 anchor shapes</strong>(not the real anchor) are used to generate a lot of anchors(2400*9 in this case).</p>  <p>I can only imagine that use 9 anchor\n","shape to slide the <strong>original image</strong> to get the all IoU . I don't understand how to use 3x3 sliding window in conv feature map here. </p>  <p>I\n","know how anchors be selected,  2400*9 -> ignore cross-boundary -> 6000 -> apply NMS -> 2000 ,  in each minibatch, it randomly choose 512 anchors from 2000.</p>\n","<p>What I can't understand is 3x3 slide with 9 anchor shape . Because from original paper, anchors with is 16, height from 11 to 273 . I don't think it use the\n","13 layer conv output feature map to calculate IoU . Anchor must be apply in original image,  so what is the 3x3 sliding window doing ??</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 13525\n","Rank: 442\n","Question Tags\t: beginner tools career reference-request books \n","\n","Question Title\t: Is there any book for modern optimization in Python?\n","Question Body\t:\n","\n","<p>I was reading <a href=\"http://rads.stackoverflow.com/amzn/click/B00PUM14EY\" rel=\"nofollow\">Modern Optimization with R (Use R!)</a> and wondering if a book\n","like this exists in Python too? To be precise something that covers stochastic gradient descent and other advanced optimization techniques. Many thanks!</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 39825\n","Rank: 539\n","Question Tags\t: cross-validation accuracy performance metric grid-search \n","\n","Question Title\t: Log loss vs accuracy for deciding between different learning rates?\n","Question Body\t:\n","\n","<p>While  model tuning using cross validation and grid search I was plotting the graph of different learning rate against log loss and accuracy separately.</p>\n","<p><strong>Log loss</strong></p>  <p>When I used log loss as score in grid search to identify the best learning rate out of the given range I got the result as\n","follows:</p>  <p>Best: -0.474619 using learning rate: 0.01</p>  <ul> <li>-0.674328 (0.000482) with: learning rate: 0.0001</li> <li>-0.583335 (0.003236) with:\n","learning rate: 0.001</li> <li>-0.474619 (0.004336) with: learning rate: 0.01</li> <li>-0.494540 (0.008705) with: learning rate: 0.1</li> </ul>\n","<p><strong>Accuracy</strong></p>  <p>When I used accuracy as score in grid search to identify the best learning rate out of the given range I got the result as\n","follows:</p>  <p>Best: 0.781958 using learning rate: 0.1</p>  <ul> <li>0.656220 (0.085705) with: learning rate: 0.0001</li> <li>0.715279 (0.010021) with:\n","learning rate: 0.001</li> <li>0.740141 (0.007927) with: learning rate: 0.01</li> <li>0.781958 (0.003770) with: learning rate: 0.1</li> </ul>  <p>In both cases I\n","got different learning rates that I should use to tune my model. When the score is log loss, I got optimum setting for learning rate as 0.01. When score is\n","accuracy, I got optimum setting for learning rate as 0.1.</p>  <p>In such cases, what score should I use for my model?</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 40930\n","Rank: 571\n","Question Tags\t: word-embeddings convolution sequence sequence-to-sequence attention-mechanism \n","\n","Question Title\t: Why does Position Embeddings work?\n","Question Body\t:\n","\n","<p>In the papers <a href=\"https://arxiv.org/pdf/1705.03122.pdf\" rel=\"nofollow noreferrer\">\"Convolutional Sequence to Sequence Learning\"</a> and  <a\n","href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"nofollow noreferrer\">\"Attention Is All You Need\"</a>, positions embeddings are simply added to the input words\n","embeddings to give the model a sense of the order of the input sequence. These position embeddings are generated from a sinusoidal signal depending on the\n","absolute position of the word in the sequence and the dimension. We obtain position embeddings of the same dimension as the word embeddings and we simply sum\n","these two. </p>  <p>I can understand that this helps the model to get a sens of the ordering of the input, but I'm quite disturbed by the fact that adding these\n","two might also erase some of the information contained in the word embeddings. Do you have an explanation on why this might work (or not) ? Is there some\n","literature about it ?    </p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 14172\n","Rank: 600\n","Question Tags\t: numerical mutual-information distribution estimators information-theory \n","\n","Question Title\t: How to estimate the mutual information numerically?\n","Question Body\t:\n","\n","<p>Suppose I have a sample {$z_i$}$_{i\\in[0,N]}$ = {($x_i,y_i$)}$_{i\\in[0,N]}$ which commes from a probability distribution $p_z(z)$. How can I use it to\n","estimate the mutual information between X and Y ?</p>  <p>$MI(X,Y) = \\int_Y \\int_X                  p_z(x,y) \\log{ \\left(\\frac{p_z(x,y)}{p_x(x)\\,p_y(y)}\n","\\right) }$</p>  <p>where $p_x$ and $p_y$ are the marginal distributions of X and Y:</p>  <p>$p_x(x) = \\int_Yp_z(x,y)$</p>  <p>$p_y(y) = \\int_Xp_z(x,y)$.</p>\n","**************************************************************************************************************************************************************** \n","\n","HighestRank ***************************************************************************************************************************** \n","\n","Question id: 21618\n","Rank: 605\n","Question Tags\t: image-classification feature-extraction image-recognition computer-vision amazon-ml \n","\n","Question Title\t: Image Feature Vectors\n","Question Body\t:\n","\n","<p>I have downloaded a dataset from Amazon. <a href=\"http://jmcauley.ucsd.edu/data/amazon/\" rel=\"nofollow noreferrer\">http://jmcauley.ucsd.edu/data/amazon/</a>\n","Dataset involves feature vectors of images. There are around 1.5 M feature vectors.</p>  <p>Dataset consists of 10 characters (the product ID), followed by 4096\n","floats (repeated for every product). </p>  <p>Every product image involves feature vectors with (4096x1) size. Feature vectors involve float numbers.</p>\n","<p>What do these float numbers mean? </p>  <p>What I understood is, there are at total 4096 features, and each index of feature vectors indicate a specific\n","feature. The values in feature vectors indicate the frequency of regarding feature in all specific image.</p>  <p>Is it so? Or, if it is not, what might be the\n","right explanation?</p>  <p>Thanks, </p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 72404\n","Rank: 24333\n","Question Tags\t: autoencoder \n","\n","Question Title\t: Why is the 2 D latent space of the variational auto encoder getting constricted to a regression line \n","\n","Question Body\t:\n","\n","<p>when I am training my variational autoencoder, the 2D latent space is getting constricted to a regression line instead of a Gaussian distribution.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 74744\n","Rank: 24334\n","Question Tags\t: object-detection \n","\n","Question Title\t: Linear Bottlenecks from MobileNetV2 \n","\n","Question Body\t:\n","\n","<p>I am trying to understand the need for the linear bottlenecks in the MobileNetV2 architecture. There are two points that the authors make:</p>  <blockquote>\n","<p>If the manifold of interest remains non-zero volume after ReLU transformation, it corresponds to a linear transformation.</p> </blockquote>  <p>and</p>\n","<blockquote>   <p>ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace\n","of the input space.</p> </blockquote>  <p>First one describes a case where we have multiple positive outputs (after ReLU transformation) and we can say that the\n","positive outputs correspond to a linear transformation of the input (before ReLU).</p>  <p>Second one describes how ReLU collapses majority of the input to zero\n","and in order to learn we have to increase number of inputs.</p>  <p>When we combine the two, we get an answer as to why the linear bottlenecks work better. </p>\n","<p>Are my interpretations correct? Is there another way to interpret this?</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 74671\n","Rank: 24335\n","Question Tags\t: association-rules \n","\n","Question Title\t: Association Rules on big dataset with lots of categorical data \n","\n","Question Body\t:\n","\n","<p>I am trying to analyze, as an exercice, <a href=\"https://www.kaggle.com/rtatman/chocolate-bar-ratings\" rel=\"nofollow noreferrer\">this</a> dataset. Is it ok\n","to use association rules on it? I am trying to use <strong>apriori()</strong> algorithm. Do you have any advice about it? I am still a beginner and reading lots\n","of documentation got me lost instead of helping...</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 72398\n","Rank: 24336\n","Question Tags\t: aws \n","\n","Question Title\t: Real time visualization in AWS \n","\n","Question Body\t:\n","\n","<p>I want to improve my current application. I am using redis using ElastiCache in AWS in order to store some user data from my website.</p>  <p>This solution\n","is not scalable and I want to scale it using Amazon Kinesis Data Firehose for the autoscale streaming output, AWS Lambda to modify the output of my logs.</p>\n","<p>I want to visualise  the data in near real time and here is where my doubt is:</p>  <p>If I use s3 to store my straming data and I use aws athena and a use a\n","web-based applications for visualise this data in near real time, aws is going to charge me every time that the application will update the new log data?</p>\n","<p>A best solution could be use AWS elastic search and do it with Kibana?</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 73876\n","Rank: 24337\n","Question Tags\t: rstudio \n","\n","Question Title\t: Error in solve.default(H, g[!fixed]) : system is computationally singular: reciprocal condition number = 1.32962e-20 \n","\n","Question Body\t:\n","\n","<p><strong><a href=\"https://i.stack.imgur.com/bkElL.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/bkElL.png\" alt=\"strong\n","text\"></a></strong></p>  <p>Hello, first six rows of my dataset are as given below. I am getting the above error. I have taken the dependent variable as \"fm\".\n","Please help.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 60882\n","Rank: 24343\n","Question Tags\t: similarity \n","\n","Question Title\t: How to identify similar datasets based on multiple temporal variables? \n","\n","Question Body\t:\n","\n","<p>I have a dataset that describes movie releases, i.e. day by day number of seats allocated to the movie and number of seats sold at different geographic\n","locations. This dataset looks something like this:</p>  <p><a href=\"https://i.stack.imgur.com/Yk7nK.png\" rel=\"nofollow noreferrer\"><img\n","src=\"https://i.stack.imgur.com/Yk7nK.png\" alt=\"enter image description here\"></a></p>  <p>This example dataset describes three days of a movie pre-release (thus\n","the negative release day), and it says that on the first day when showtimes were published 5000 seats were allocated to the movie and 500 seats were sold on\n","that day. Next day there were still 5000 seats allocated and another 300 were sold, etc.</p>  <p>Release Day is normalised to represent the same period in the\n","movie release window across all movies, i.e. release day 1 will always be Monday of the first isoWeek that contains the showtimes.</p>  <p>My dataset describes\n","couple of thousand movies across different cities. My task is given 1 movie release identify other movie releases that behaved similar at the same point in time\n","in the past.</p>  <p>Ideally, I am looking to do this in PostgreSQL, but it could be done outside of the database too.</p>  <p>What is the best way to find the\n","most similar datasets where multiple variables are shared (country ID, city ID, release day) and other variables have varying scales (seat count, seat sold\n","count)?</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 71552\n","Rank: 24344\n","Question Tags\t: normalization \n","\n","Question Title\t: The need to normalize input data for DNN when using He_initialization \n","\n","Question Body\t:\n","\n","<p>I'm reading Geron's \"Hands-On Machine Learning with Scikit_learn, Keras &amp; Tensorflow\"</p>  <p>I noticed in the problem set after chapter 11, he ran a DNN\n","using ELU activation and He Initialization. But he did not do any input normalization. The input is 0-255 RGB. Is it because He Initialization takes care of the\n","normalization part? Thanks.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 73375\n","Rank: 24348\n","Question Tags\t: time \n","\n","Question Title\t: Getting a constant accuracy for a ping-like command \n","\n","Question Body\t:\n","\n","<p>very simple and naïve question here, I'm trying to mesure some RTTs with a reasonnable accuracy. The Ubuntu ping command provides a pretty good mesure with a\n","10µs accuracy. but only when the total RTT time is under 10ms. </p>  <p>Actually, it seems that the output time is constantly given with 3 digits (e.g. 6.34 ms\n","; 17.3 ms ; 137 ms).</p>  <p>I would like to keep the 10µs accuracy whatever the output value.</p>  <p>Does anyone knows if such an option is available with the\n","command ping, or if there exist another tool which will allow me to get what I want.</p>  <p>Thanks in advance, and have a great day.</p>  <p>PS : I'm not a\n","native english speaker so i may have made some grammatical mistakes, sincere apologies for that.</p>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 71500\n","Rank: 24350\n","Question Tags\t: apache-hadoop \n","\n","Question Title\t: What are common problems around HADOOP storage? \n","\n","Question Body\t:\n","\n","<p>I've been asked to lead a program to understand why our Hadoop storage is constantly near capacity.  What questions should I ask?</p>  <ol> <li>Data\n","age,</li> <li>Data size?</li> <li>Housekeeping schedule?</li> <li>How do we identify the different types of compression used by different applications?</li>\n","<li>How can we identify where the duplicate data sources are?</li> <li>Are jobs designated for edge nodes only on edge nodes?</li> </ol>\n","**************************************************************************************************************************************************************** \n","\n","LowestRank ****************************************************************************************************************************** \n","\n","Question id: 72522\n","Rank: 24353\n","Question Tags\t: vae \n","\n","Question Title\t: What is achieved by converting the latent space to normal distribution in VAE? \n","\n","Question Body\t:\n","\n","<blockquote>   <p>Instead of forwarding the latent values to the decoder directly, VAEs use them to calculate a mean and a standard deviation. The input to the\n","decoder is then sampled from the corresponding normal distribution</p> </blockquote>  <p>How this helps?</p>\n","**************************************************************************************************************************************************************** \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S4mZNhi4sceG"},"source":["# Task 4 Get counts of certain special characters from body\n","Get counts of certain special characters to add to features prior to cleaning: 4a. Questions marks 4b. Text bolding 4c. Number of paragraphs 4d. Code examples"]},{"cell_type":"code","metadata":{"id":"T09QWAnzzGFg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371338024,"user_tz":420,"elapsed":359,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"83cf86bf-89e7-4022-a8b4-046a91eaa66d"},"source":["'''\n","4. The following insights have been gained in reviewing some of the questions:\n","there may be a correlation between number of questions asked (\"?\"), bolding of text <strong>, number of paragraphs <p>, and code examples <code> \n","in the body of the text; let's get a count of these prior to cleaning and removing them and keep them as features\n","\n","4a. Starting with count of questions - mean is 1.5 - some have no questions at all; max is 58 wow; looking at it the high question count comes from\n","an error log that was provided as part of the question, so this one will probably show up with a high character and word count in the body, as well \n","as a hit for a long code example\n","'''\n","questions_df[\"NumQuestions\"] = questions_df[\"Body\"].map(lambda x: str.count(x, '?'))\n","#questions_df.head()\n","#questions_df[\"NumQuestions\"].describe()\n","maxquestion = questions_df.loc[questions_df[\"NumQuestions\"] == 58]\n","\n","maxquestionlist = list(zip(maxquestion[\"Id\"],maxquestion[\"Tags_SpaceDelimited\"],maxquestion[\"Title\"],maxquestion[\"Body\"]))\n","                            \n","for id,tag,t,b in maxquestionlist[0:]:\n","    print(\"Question id:\",id)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Question id: 74666\n","Question Tags\t: tensorflow gpu generative-models \n","\n","Question Title\t: Is it possible to train stylegan2 with a custom dataset using a graphics card that only has 6GB of VRAM (GeForce GTX 1660)? \n","\n","Question Body\t:\n","\n","<p>I'm attempting to train <a href=\"https://github.com/NVlabs/stylegan2\" rel=\"nofollow noreferrer\">stylegan2</a> using a custom dataset, but no matter what\n","settings I use I see the same error:</p>  <pre><code>2020-05-22 11:15:05.261933: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection:\n","deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message\n","frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch\n","sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature. 2020-05-22 11:15:05.339186: I\n","tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 3.52G (3781073152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n","</code></pre>  <p>I'm assuming this means I need more GPU memory, but I've read that you can lower memory use in exchange for longer training periods. I did\n","have to downgrade from tensorflow2 to 1.15 to use this project so there could be some underlying configuration issue, but I am able to generate images from the\n","pretrained models without any issues.</p>  <p>This is how I'm running the training process:</p>  <p><code>python run_training.py --num-gpus=1 --data-\n","dir=datasets --config=config-e --dataset=customdata --mirror-augment=true</code></p>  <p>I've tried using the other config-x options, and adjusting the settings\n","in both <code>run_training.py</code> and <code>training/training_loop.py</code> although more specifically I'm just trying different values for\n","<code>sched.minibatch_size_base</code> and <code>sched.minibatch_gpu_base</code>. Checking the results folder does tell me that the settings I've changed in\n","<code>run_training.py</code> are actually used during the training process. </p>  <p>Here's the complete log from <code>run_training.py</code> if it's\n","useful:</p>  <pre><code>Local submit - run_dir: results\\00021-stylegan2-customdata-1gpu-config-e dnnlib: Running training.training_loop.training_loop() on\n","localhost... 2020-05-22 13:02:45.261043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\n","2020-05-22 13:02:51.127997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll 2020-05-22\n","13:02:51.169757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:  name: GeForce GTX 1660 major: 7 minor: 5\n","memoryClockRate(GHz): 1.785 pciBusID: 0000:01:00.0 2020-05-22 13:02:51.176966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully\n","opened dynamic library cudart64_100.dll 2020-05-22 13:02:51.187788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic\n","library cublas64_100.dll 2020-05-22 13:02:51.197589: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library\n","cufft64_100.dll 2020-05-22 13:02:51.205389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\n","2020-05-22 13:02:51.216122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll 2020-05-22\n","13:02:51.225483: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll 2020-05-22\n","13:02:51.244887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll 2020-05-22 13:02:51.253430: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0 2020-05-22 13:02:51.966561: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-05-22 13:02:51.971731: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 2020-05-22 13:02:51.974966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\n","2020-05-22 13:02:51.979741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0\n","with 4630 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1660, pci bus id: 0000:01:00.0, compute capability: 7.5) Streaming data using\n","training.dataset.TFRecordDataset... self.tfrecord_dir: datasets\\customdata Dataset shape = [3, 64, 64] Dynamic range = [0, 255]    Label size    = 0\n","Constructing networks... Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... 2020-05-22 13:03:25.588173: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:  name: GeForce GTX 1660 major: 7 minor: 5 memoryClockRate(GHz): 1.785\n","pciBusID: 0000:01:00.0 2020-05-22 13:03:25.596152: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library\n","cudart64_100.dll 2020-05-22 13:03:25.600627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library\n","cublas64_100.dll 2020-05-22 13:03:25.605487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\n","2020-05-22 13:03:25.610555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll   2020-05-22\n","13:03:25.618346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll 2020-05-22\n","13:03:25.622514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll 2020-05-22\n","13:03:25.626790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll      2020-05-22\n","13:03:25.632722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0 2020-05-22 13:03:25.638261: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-05-22 13:03:25.642363: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0  2020-05-22 13:03:25.645684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\n","2020-05-22 13:03:25.649560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 4630 MB memory) -&gt;\n","physical GPU (device: 0, name: GeForce GTX 1660, pci bus id: 0000:01:00.0, compute capability: 7.5) Loading... Done. Setting up TensorFlow plugin\n","\"upfirdn_2d.cu\": Preprocessing... 2020-05-22 13:03:50.302225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:  name:\n","GeForce GTX 1660 major: 7 minor: 5 memoryClockRate(GHz): 1.785 pciBusID: 0000:01:00.0 2020-05-22 13:03:50.310782: I\n","tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll 2020-05-22 13:03:50.316161: I\n","tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll 2020-05-22 13:03:50.395110: I\n","tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll 2020-05-22 13:03:50.463435: I\n","tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll 2020-05-22 13:03:50.468677: I\n","tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll 2020-05-22 13:03:50.527377: I\n","tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll 2020-05-22 13:03:50.531735: I\n","tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll 2020-05-22 13:03:50.537159: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0 2020-05-22 13:03:50.615931: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-05-22 13:03:50.679408: I\n","tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0  2020-05-22 13:03:50.682438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\n","2020-05-22 13:03:50.686257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 4630 MB memory) -&gt;\n","physical GPU (device: 0, name: GeForce GTX 1660, pci bus id: 0000:01:00.0, compute capability: 7.5) Loading... Done.  G                           Params\n","OutputShape       WeightShape      ---                         ---       ---               --- latents_in                  -         (?, 512)          -\n","labels_in                   -         (?, 0)            - lod                         -         ()                - dlatent_avg                 -         (512,)\n","- G_mapping/latents_in        -         (?, 512)          - G_mapping/labels_in         -         (?, 0)            - G_mapping/Normalize         -         (?,\n","512)          - G_mapping/Dense0            262656    (?, 512)          (512, 512) G_mapping/Dense1            262656    (?, 512)          (512, 512)\n","G_mapping/Dense2            262656    (?, 512)          (512, 512) G_mapping/Dense3            262656    (?, 512)          (512, 512) G_mapping/Dense4\n","262656    (?, 512)          (512, 512) G_mapping/Dense5            262656    (?, 512)          (512, 512) G_mapping/Dense6            262656    (?, 512)\n","(512, 512) G_mapping/Dense7            262656    (?, 512)          (512, 512) G_mapping/Broadcast         -         (?, 10, 512)      - G_mapping/dlatents_out\n","-         (?, 10, 512)      - Truncation/Lerp             -         (?, 10, 512)      - G_synthesis/dlatents_in     -         (?, 10, 512)      -\n","G_synthesis/4x4/Const       8192      (?, 512, 4, 4)    (1, 512, 4, 4) G_synthesis/4x4/Conv        2622465   (?, 512, 4, 4)    (3, 3, 512, 512)\n","G_synthesis/4x4/ToRGB       264195    (?, 3, 4, 4)      (1, 1, 512, 3) G_synthesis/8x8/Conv0_up    2622465   (?, 512, 8, 8)    (3, 3, 512, 512)\n","G_synthesis/8x8/Conv1       2622465   (?, 512, 8, 8)    (3, 3, 512, 512) G_synthesis/8x8/Upsample    -         (?, 3, 8, 8)      - G_synthesis/8x8/ToRGB\n","264195    (?, 3, 8, 8)      (1, 1, 512, 3) G_synthesis/16x16/Conv0_up  2622465   (?, 512, 16, 16)  (3, 3, 512, 512) G_synthesis/16x16/Conv1     2622465   (?,\n","512, 16, 16)  (3, 3, 512, 512) G_synthesis/16x16/Upsample  -         (?, 3, 16, 16)    - G_synthesis/16x16/ToRGB     264195    (?, 3, 16, 16)    (1, 1, 512, 3)\n","G_synthesis/32x32/Conv0_up  2622465   (?, 512, 32, 32)  (3, 3, 512, 512) G_synthesis/32x32/Conv1     2622465   (?, 512, 32, 32)  (3, 3, 512, 512)\n","G_synthesis/32x32/Upsample  -         (?, 3, 32, 32)    - G_synthesis/32x32/ToRGB     264195    (?, 3, 32, 32)    (1, 1, 512, 3) G_synthesis/64x64/Conv0_up\n","1442561   (?, 256, 64, 64)  (3, 3, 512, 256) G_synthesis/64x64/Conv1     721409    (?, 256, 64, 64)  (3, 3, 256, 256) G_synthesis/64x64/Upsample  -         (?,\n","3, 64, 64)    - G_synthesis/64x64/ToRGB     132099    (?, 3, 64, 64)    (1, 1, 256, 3) G_synthesis/images_out      -         (?, 3, 64, 64)    -\n","G_synthesis/noise0          -         (1, 1, 4, 4)      - G_synthesis/noise1          -         (1, 1, 8, 8)      - G_synthesis/noise2          -         (1, 1,\n","8, 8)      - G_synthesis/noise3          -         (1, 1, 16, 16)    - G_synthesis/noise4          -         (1, 1, 16, 16)    - G_synthesis/noise5          -\n","(1, 1, 32, 32)    - G_synthesis/noise6          -         (1, 1, 32, 32)    - G_synthesis/noise7          -         (1, 1, 64, 64)    - G_synthesis/noise8\n","-         (1, 1, 64, 64)    - images_out                  -         (?, 3, 64, 64)    - ---                         ---       ---               --- Total\n","23819544   D                    Params    OutputShape       WeightShape ---                  ---       ---               --- images_in            -         (?,\n","3, 64, 64)    - labels_in            -         (?, 0)            - 64x64/FromRGB        1024      (?, 256, 64, 64)  (1, 1, 3, 256) 64x64/Conv0          590080\n","(?, 256, 64, 64)  (3, 3, 256, 256) 64x64/Conv1_down     1180160   (?, 512, 32, 32)  (3, 3, 256, 512) 64x64/Skip           131072    (?, 512, 32, 32)  (1, 1,\n","256, 512) 32x32/Conv0          2359808   (?, 512, 32, 32)  (3, 3, 512, 512) 32x32/Conv1_down     2359808   (?, 512, 16, 16)  (3, 3, 512, 512) 32x32/Skip\n","262144    (?, 512, 16, 16)  (1, 1, 512, 512) 16x16/Conv0          2359808   (?, 512, 16, 16)  (3, 3, 512, 512) 16x16/Conv1_down     2359808   (?, 512, 8, 8)\n","(3, 3, 512, 512) 16x16/Skip           262144    (?, 512, 8, 8)    (1, 1, 512, 512) 8x8/Conv0            2359808   (?, 512, 8, 8)    (3, 3, 512, 512)\n","8x8/Conv1_down       2359808   (?, 512, 4, 4)    (3, 3, 512, 512) 8x8/Skip             262144    (?, 512, 4, 4)    (1, 1, 512, 512) 4x4/MinibatchStddev  -\n","(?, 513, 4, 4)    - 4x4/Conv             2364416   (?, 512, 4, 4)    (3, 3, 513, 512) 4x4/Dense0           4194816   (?, 512)          (8192, 512) Output\n","513       (?, 1)            (512, 1) scores_out           -         (?, 1)            - ---                  ---       ---               --- Total\n","23407361  2020-05-22 13:03:58.578847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\n","2020-05-22 13:03:58.961664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll 2020-05-22\n","13:04:00.763442: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows Relying on driver to perform ptx\n","compilation. This message will be only logged once. 2020-05-22 13:04:01.548775: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection:\n","deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message\n","frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch\n","sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.    2020-05-22 13:04:01.651217: I\n","tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory Building\n","TensorFlow graph... </code></pre>  <p>Here's the contents of <code>submit_config.txt</code> written to the results folder for the above job:</p>  <pre><code>{\n","'datasets': [],     'host_name': 'localhost',     'local': &lt;dnnlib.submission.internal.local.TargetOptions object at 0x0000027CF0D20D48&gt;,     'num_gpus':\n","1,     'nvprof': False,     'platform_extras': &lt;dnnlib.submission.submit.PlatformExtras object at 0x0000027CF0D20E08&gt;,     'print_info': False,\n","'run_desc': 'stylegan2-customdata-1gpu-config-e',     'run_dir': 'results\\\\00021-stylegan2-customdata-1gpu-config-e',     'run_dir_extra_files': [],\n","'run_dir_ignore': ['__pycache__', '*.pyproj', '*.sln', '*.suo', '.cache', '.idea', '.vs', '.vscode', '_cudacache'],     'run_dir_root': 'results',\n","'run_func_kwargs': {   'D_args': {'fmap_base': 8192, 'func_name': 'training.networks_stylegan2.D_stylegan2'},                            'D_loss_args':\n","{'func_name': 'training.loss.D_logistic_r1', 'gamma': 100},                            'D_opt_args': {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08},\n","'G_args': {'fmap_base': 8192, 'func_name': 'training.networks_stylegan2.G_main'},                            'G_loss_args': {'func_name':\n","'training.loss.G_logistic_ns_pathreg'},                            'G_opt_args': {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08},\n","'data_dir': 'datasets',                            'dataset_args': {'tfrecord_dir': 'customdata'},                            'grid_args': {'layout': 'random',\n","'size': '8k'},                            'image_snapshot_ticks': 10,                            'metric_arg_list': [{'func_name':\n","'metrics.frechet_inception_distance.FID', 'minibatch_per_gpu': 8, 'name': 'fid50k', 'num_images': 50000}],                            'mirror_augment': True,\n","'network_snapshot_ticks': 10,                            'sched_args': {'D_lrate_base': 0.002, 'G_lrate_base': 0.002, 'minibatch_gpu_base': 1,\n","'minibatch_size_base': 8},                            'tf_config': {'rnd.np_random_seed': 1000},                            'total_kimg': 25000},\n","'run_func_name': 'training.training_loop.training_loop',     'run_id': 21,     'run_name': '00021-stylegan2-customdata-1gpu-config-e',     'submit_target':\n","&lt;SubmitTarget.LOCAL: 1&gt;,     'task_name': 'itsame-00021-stylegan2-customdata-1gpu-config-e',     'user_name': 'itsame'}  </code></pre>  <p>I've trained\n","other models with the same hardware, but I'm guessing stylegan2 requires a bit more space to work. Thanks for reading!</p>  <p>EDIT: </p>  <p>I've added some\n","code to <code>tfutil.py</code> and now I have a different error! According to the web, I may need to downgrade CUDA.</p>  <pre><code>gpu_options =\n","tf.GPUOptions(per_process_gpu_memory_fraction=0.333, allow_growth=True)     graph_options = tf.GraphOptions(place_pruned_graph=True)     config_proto =\n","tf.ConfigProto(gpu_options=gpu_options, graph_options=graph_options) </code></pre>  <p>error is now:</p>\n","<pre><code>tensorflow.python.framework.errors_impl.InternalError: cudaErrorInvalidConfiguration          [[node\n","GPU0/G_loss/PathReg/G/G_synthesis/8x8/Upsample/UpFirDn2D (defined at C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n","</code></pre>  <p>EDIT 5/23/2020:</p>  <p>The above error seemed to go away on its own after reducing the batch size and using a much lower gpu memory fraction.\n","I'm seeing this error now:</p>  <pre><code>tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[3,3,512,512]\n","and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc      [[node TrainG/Apply0/grad_acc_var_38/Assign (defined at\n","C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]] Hint: If you want to see a list of allocated tensors when OOM happens, add\n","report_tensor_allocations_upon_oom to RunOptions for current allocation info. </code></pre>  <p>I'm going to try and reduce the tensor size to 256x256. I have\n","no idea how to do that or what it means, but most of what I've read about this error seems to suggest that.</p>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zZiubXqW6Itx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371351007,"user_tz":420,"elapsed":297,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"b7fda88e-c693-48b5-a779-4ecf27fcd451"},"source":["'''\n","4b. Next lets get a count of bolded text <strong> \n","mean is .4 which means that most questions do not have bold text; max is 42 wow; \n","\n","looking at this one question poser has accentuated distinctions in his variable name examples and highlighted his main question at the end;\n","does use of these bold text items help to predict tags at all? we shall see\n","'''\n","questions_df[\"BodyBoldCount\"] = questions_df[\"Body\"].map(lambda x: str.count(x, '<strong>'))\n","#questions_df.head()\n","#questions_df[\"BodyBoldCount\"].describe()\n","boldtextquestions = questions_df.loc[questions_df[\"BodyBoldCount\"] == 42]\n","\n","boldtextquestionslist = list(zip(boldtextquestions[\"Id\"],boldtextquestions[\"Tags_SpaceDelimited\"],boldtextquestions[\"Title\"],boldtextquestions[\"Body\"]))\n","                            \n","for id,tag,t,b in boldtextquestionslist[0:]:\n","    print(\"Question id:\",id)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Question id: 5367\n","Question Tags\t: machine-learning predictive-modeling ranking \n","\n","Question Title\t: Best way to format data for supervised machine learning ranking predictions \n","\n","Question Body\t:\n","\n","<p>I'm fairly new to machine learning, but I'm doing my best to learn as much as possible.</p>  <p>I am curious about how predicting athlete performance\n","(runners in particular) in a race of a specific starting lineup. For instance, if RunnerA, RunnerB, RunnerC, and RunnerD are all racing a 400 meter race, I want\n","to best predict whether <strong>RunnerA</strong> will beat <strong>RunnerB</strong> based on past race result information (which I have at my disposal).\n","However, I have many cases where <strong>RunnerA</strong> has never raced against <strong>RunnerB</strong>; yet I do have data showing <strong>RunnerA</strong>\n","has beat <strong>RunnerC</strong> in the past, and <strong>RunnerC</strong> has beat <strong>RunnerB</strong> in the past. This logic extends deeper as well.\n","So, it would seem that <strong>RunnerA</strong> <em>should</em> beat <strong>RunnerB</strong>, given this information. My real concern is when it gets more\n","complicated than this as I add more features (multiple runners, different distances, etc), and so I'm turing to ML algorithms to help my predictions.</p>\n","<p>However, I am having difficulty figuring out how to include this in my row data that I can train (after all, correctly formatting data is 99% of proper\n","machine learning), and I am hoping that someone here might have thought along the same lines in the past and might be able to shed some light. </p>  <hr>\n","<p><strong>Example:</strong></p>  <p>I am currently trying to include <strong>RunnerX</strong>-<strong>RunnerY</strong> past race data by counting all the races\n","that <strong>RunnerX</strong> and <strong>RunnerY</strong> have run together and normalizing them on a scale from <code>-1</code> to <code>1</code>;\n","<code>-1</code> indicating <strong>RunnerX</strong> lost all past races against RunnerY; and <code>+1</code> indicating that RunnerX has  won all past races\n","against RunnerY; and <code>+1</code> indicating. And <code>0</code> indicating an equal number of wins and losses (or no past races against each other).</p>\n","<p>For instance, if <strong>RunnerA</strong> is racing <strong>RunnerB</strong>, and <strong>RunnerA</strong> has beat <strong>RunnerB</strong> in the past,\n","then I want the algorithm to know that (denoted by a <code>+1</code> on the <strong>RunnerB</strong> column of row <strong>RunnerA</strong>); same for vice\n","versa. Taking it another step further, If <strong>RunnerA</strong> is racing <strong>RunnerC</strong> (but the two have never raced each other in the past), and\n","<strong>RunnerA</strong> has beat <strong>RunnerD</strong> in a past race, and <strong>RunnerD</strong> has beat <strong>RunnerC</strong> in a past race, then I\n","want the algorithm to learn that <strong>RunnerA</strong> should beat <strong>RunnerC</strong>. I say <em>beat</em> here, but I mean an \"average beat\" for any\n","<strong>RunnerX</strong>-<strong>RunnerY</strong> combinations when data for more than 1 past race is available.</p>  <p>I have set my data up as:</p>\n","<pre><code>name     track   surface  distance  age    RunnerA   RunnerB   RunnerC   RunnerD RunnerA  Home    2        400       11     0         1         0\n","1 RunnerC  Away    2        400       12     0         0         0         -1 RunnerD  Home    2        400       10     0         0         1         0\n","</code></pre>  <p>which shows that RunnerA has beat <strong>RunnerB</strong> and <strong>RunnerD</strong> in the past. RunnerC has lost to\n","<strong>RunnerD</strong>. And <strong>RunnerD</strong> has beat <strong>RunnerC</strong>.</p>  <hr>  <p><strong>The problem:</strong></p>  <p>The problem is\n","that I don't really think this is a correct display of the information for an ML algorithm.</p>  <p><em>From what I understand, ML data should be row\n","independent. And this data isn't because row 1 (<strong>RunnerA</strong>) has beat <strong>RunnerD</strong>, yet the data indicating <strong>RunnerD</strong>\n","has beat <strong>RunnerC</strong> is in row 3.</em></p>  <p>Does anyone have any ideas how I might be able to incorporate this past win-percentage-for-runner-\n","pair-combination data??? I'm totally stuck here. I've read a lot about some algorithms that estimate the win loss by simply totaling win statistics, but those\n","don't say anything about the actual probability of a particular runner to beat another particular runner.</p>  <p>Any pointers would be super helpful.</p>\n","<p>Thanks!!!</p>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S6IqFKer7-JM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371367005,"user_tz":420,"elapsed":259,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"1a5268ab-f8f6-49ff-beb4-8ce21ad1e0c1"},"source":["'''\n","4c. Next lets get a count of number of paragraphs in each question body \n","mean is 4, some have no paragraphs whatsoever; most have 2-6 paragraphs, and max is 57 wow; \n","\n","let's look at examples of zero and the 57 one; \n","those without any paragraphs are limited to either code or lines without paragraphs; \n","in the one with 57 the question poser has formatted his code examples\n","into paragraphs, thus accounting for the high count.\n","'''\n","questions_df[\"ParagraphCount\"] = questions_df[\"Body\"].map(lambda x: str.count(x, '<p>'))\n","#questions_df.head()\n","#questions_df[\"ParagraphCount\"].describe()\n","lst = [0,57]\n","paragraphquestions = questions_df.loc[questions_df[\"ParagraphCount\"].isin(lst)].sort_values(by = \"ParagraphCount\", ascending=False).head()\n","#paragraphquestions.head()\n","\n","paragraphcountlist = list(zip(paragraphquestions[\"Id\"],paragraphquestions[\"Tags_SpaceDelimited\"],paragraphquestions[\"Title\"],paragraphquestions[\"Body\"]))\n","                            \n","for id,tag,t,b in paragraphcountlist[0:]:\n","    print(\"Examples of Paragraph Counts\", \"*\" * 131, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Examples of Paragraph Counts *********************************************************************************************************************************** \n","\n","Question id: 41795\n","Question Tags\t: cnn \n","\n","Question Title\t: Value of loss and accuracy does not change over Epochs \n","\n","Question Body\t:\n","\n","<p>I am trying to work on a CNN model for churn. Here is my code. NO matter what optimizer I choose, change the learning rate, learning decay, loss function\n","etc, losses and accuracy do not change over epoch. I am feeding following array as input to model, which are encoded ( label encoded and then CSC) x_train.shape\n","=  (27999, 1, 500, 10) y_train.shape =  (27999,) x_test.shape =  (57540, 1, 500, 10) y_test.shape =  (57540,)</p>  <p>Original input is a CSV file of shape\n","(28770155, 11)</p>  <p>Code is as follows:</p>  <h2>label Encoder</h2>  <p>lab=LabelEncoder()</p>  <p>lab1=LabelEncoder()</p>\n","<p>train_label=train_table.apply(lab.fit_transform)</p>  <p>test_label=test_table.apply(lab1.fit_transform)</p>  <p>one_hot_array = csc_matrix(train_label,\n","dtype=np.int8).toarray()</p>  <p>one_hot_test = csc_matrix(test_label, dtype=np.int8).toarray()</p>  <h1>Fit now of rows as image</h1>  <p>rows_per_image =\n","500</p>  <p>cols_per_image = oh_cols-1</p>  <p>cols_per_image_test = ot_cols-1</p>  <p>num_colors_per_pixel = 1</p>  <p>num_of_images = oh_rows //\n","rows_per_image</p>  <p>num_of_images_test = ot_rows // rows_per_image</p>  <p>image_shape = (num_colors_per_pixel, rows_per_image, cols_per_image)</p>\n","<p>image_shape_test = (num_colors_per_pixel, rows_per_image, cols_per_image_test)</p>  <h1>Drop tail rows that wont fit into a 500 row image</h1>\n","<p>truncated_one_hot_array = one_hot_array[:rows_per_image * num_of_images, :]</p>  <p>truncated_one_hot_test = one_hot_test[ :rows_per_image *\n","num_of_images_test, :]</p>  <p># x_train is the image array</p>  <p>x_train = truncated_one_hot_array[:, :-1].reshape(num_of_images, image_shape[0],\n","#num_colors_per_pixel, image_shape[1], #rows_per_image, image_shape[2]  #cols_per_image 187 )</p>  <p>x_test = truncated_one_hot_test[:, :-1].reshape(\n","num_of_images_test,        image_shape_test[0], #num_colors_per_pixel,         image_shape_test[1], #rows_per_image,         image_shape_test[2]\n","#cols_per_image )</p>  <p>...Some code for Y train and Y test .....</p>  <h1>Model :</h1>  <p>model = Sequential()</p>  <p>K.set_image_dim_ordering('th')  # th\n","= (channels, rows, cols) ; tf = (rows, cols, channels)</p>  <p>model.add(Conv2D(20, (3, 3), input_shape = image_shape, activation= 'relu', padding=\"same\" ))</p>\n","<p>model.add(MaxPooling2D(pool_size=(2, 2)))</p>  <p>model.add(Conv2D(20, (3, 3), activation='relu'))</p>  <p>model.add(Dropout(0.2))</p>\n","<p>model.add(Flatten())</p>  <p>model.add(Dense(32, activation= 'relu' ))</p>  <p>model.add(Dense(25, activation= 'relu' ))</p>  <p>model.add(Dense(1,\n","activation= 'sigmoid' ))</p>  <p>opt = SGD(lr =0.01,decay = 1e-6,momentum = 0.9)</p>  <p>model.compile(loss= 'binary_crossentropy' , optimizer=opt,\n","metrics=[ 'accuracy' ])</p>  <p>print ('model.input_shape  ', model.input_shape)</p>  <p>model.summary()</p>  <p>model.fit(x_train, y_train,\n","validation_data =(x_test,y_test),            epochs=10, # 20:1            batch_size=8) #160 #change training epochs</p>  <p>model.input_shape   (None, 1, 500,\n","10)</p>  <hr>  <h1>Layer (type)                 Output Shape              Param #</h1>  <p>conv2d_1 (Conv2D)            (None, 20, 500, 10)       200</p>  <hr>\n","<p>max_pooling2d_1 (MaxPooling2 (None, 20, 250, 5)        0</p>  <hr>  <p>conv2d_2 (Conv2D)            (None, 20, 248, 3)        3620</p>  <hr>  <p>dropout_1\n","(Dropout)          (None, 20, 248, 3)        0</p>  <hr>  <p>flatten_1 (Flatten)          (None, 14880)             0</p>  <hr>  <p>dense_1 (Dense)\n","(None, 32)                476192</p>  <hr>  <p>dense_2 (Dense)              (None, 25)                825</p>  <hr>  <h1>dense_3 (Dense)              (None, 1)\n","26</h1>  <p>Total params: 480,863 Trainable params: 480,863 Non-trainable params: 0</p>  <hr>  <p>WARNING:tensorflow:Variable *= will be deprecated. Use\n","variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.</p>  <p>Train on 27999 samples, validate\n","on 57540 samples</p>  <p>Epoch 1/10 27999/27999 [==============================] - 37s 1ms/step - loss: 2.5676 - acc: 0.8407 - val_loss: 2.2765 - val_acc:\n","0.8588</p>  <p>Epoch 2/10 27999/27999 [==============================] - 36s 1ms/step - loss: 2.5652 - acc: 0.8409 - val_loss: 2.2765 - val_acc: 0.8588</p>\n","<p>Epoch 3/10 27999/27999 [==============================] - 36s 1ms/step - loss: 2.5652 - acc: 0.8409 - val_loss: 2.2765 - val_acc: 0.8588</p>  <p>Epoch 4/10\n","27999/27999 [==============================] - 36s 1ms/step - loss: 2.5652 - acc: 0.8409 - val_loss: 2.2765 - val_acc: 0.8588</p>  <p>Epoch 5/10 27999/27999\n","[==============================] - 36s 1ms/step - loss: 2.5652 - acc: 0.8409 - val_loss: 2.2765 - val_acc: 0.8588</p>  <p>Epoch 6/10 27999/27999\n","[==============================] - 36s 1ms/step - loss: 2.5652 - acc: 0.8409 - val_loss: 2.2765 - val_acc: 0.8588</p>  <p>Epoch 7/10.......</p>  <p>Kindly help.\n","Tried all possible scenarios.</p>\n","**************************************************************************************************************************************************************** \n","\n","Examples of Paragraph Counts *********************************************************************************************************************************** \n","\n","Question id: 22283\n","Question Tags\t: tensorflow inception \n","\n","Question Title\t: TensorFlow Inception V3 \n","\n","Question Body\t:\n","\n","<ol> <li>What are bottleneck values and how are they generated?</li> <li>How does the next-to-last layer of Inception use these bottlenecks to generate the\n","accuracy?</li> <li>How is the final accuracy calculated?</li> </ol>\n","**************************************************************************************************************************************************************** \n","\n","Examples of Paragraph Counts *********************************************************************************************************************************** \n","\n","Question id: 61227\n","Question Tags\t: python data-science-model \n","\n","Question Title\t: Substituting nan values with mean code \n","\n","Question Body\t:\n","\n","<pre><code>for x in num_cols:     imp = SimpleImputer(missing_values=np.nan, strategy='mean')     imp.fit(np.array(ds[x]).reshape(-1,1))     ds[x] =\n","imp.transform(np.array(ds[x]).reshape(-1,1)) </code></pre>\n","**************************************************************************************************************************************************************** \n","\n","Examples of Paragraph Counts *********************************************************************************************************************************** \n","\n","Question id: 69820\n","Question Tags\t: python \n","\n","Question Title\t: python///TypeError: unsupported operand type(s) for -: 'list' and 'list' ,i write the code and i get this massege \n","\n","Question Body\t:\n","\n","<pre class=\"lang-py prettyprint-override\"><code>N = int(input('number of traverse=')) x = [] y = [] for r in range(N):     x.append([float(input('enter x='))])\n","for l in range (N) :     y.append([float(input('enter y='))])  N=N-1 b=int(N) area = x[0] * (y[1] - y[b]) + x[b] * (y[0] - y[b-1])  for k in range(1, N):\n","area = area + x[k] * (y[k+1] - y[k - 1])     i=i+1 area = .5 *abs(area)**strong text** <span class=\"math-container\">```</span> </code></pre>\n","**************************************************************************************************************************************************************** \n","\n","Examples of Paragraph Counts *********************************************************************************************************************************** \n","\n","Question id: 69567\n","Question Tags\t: machine-learning neural-network tensorflow data-science-model \n","\n","Question Title\t: What is neural structure learning in tensorflow? \n","\n","Question Body\t:\n","\n","<ul> <li>What is neural structure learning?</li> <li>what is the difference between neural network vs neural structure learning?</li> </ul>\n","**************************************************************************************************************************************************************** \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZCHukCZ8-vUj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371386465,"user_tz":420,"elapsed":355,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"ba86a567-45d8-4ed5-9e1b-c40d5b735418"},"source":["'''\n","4d. Next lets get a count of number of code examples in the question \n","\n","mean is 1 code example per question, max is 36 wow; let's look at the 36 ones - there are 2; \n","\n","yup lots of code examples in those two, but in the first one it looks like the code should have been\n","formatted differently, since it is a powerbi question....; as we would expect most of the time, \n","where code examples are given tags are specific to a programming language; this would be something to explore\n","'''\n","questions_df[\"CodeCount\"] = questions_df[\"Body\"].map(lambda x: str.count(x, '<code>'))\n","#questions_df.head()\n","questions_df[\"CodeCount\"].describe()\n","\n","codeinquestions = questions_df.loc[questions_df[\"CodeCount\"] == 36]\n","#codeinquestions.head()\n","codeinquestionslist = list(zip(codeinquestions[\"Id\"],codeinquestions[\"Tags_SpaceDelimited\"],codeinquestions[\"Title\"],codeinquestions[\"Body\"]))\n","                            \n","for id,tag,t,b in codeinquestionslist[0:]:\n","    print(\"Examples of High Code Counts\", \"*\" * 131, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Examples of High Code Counts *********************************************************************************************************************************** \n","\n","Question id: 54035\n","Question Tags\t: powerbi \n","\n","Question Title\t: (SOLVED) Power BI, Page level filter not working with many to one relation \n","\n","Question Body\t:\n","\n","<p>EDIT: Solution found.</p>  <p>Turns out the relations were set to <code>Cross filter direction: Single</code> on all relations (and the one from <code>Month\n","table.Month</code> to <code>A.Month</code> was not set to active).</p>  <p>Setting <code>Cross filter direction: Both</code> fixed the problem.</p>  <hr>  <p>I\n","have four tables <code>A</code>, <code>B</code>, <code>Month table</code> and <code>Quarter table</code></p>  <p><code>A</code> has a column\n","<code>A.Month</code>, and many other columns with irrelevant data</p>  <p><code>B</code> has a column <code>B.Quarter</code>, and many other columns with\n","irrelevant data</p>  <p><code>Month table</code> has columns <code>Month table.Month</code> and <code>Month table.Quarter</code></p>  <p><code>Quarter\n","table</code> has a column <code>Quarter table.Quarter</code></p>  <hr>  <p>One to many relations are shown as this: <code>\"&lt;\"</code></p>  <p><code>Quarter\n","table.Quarter &lt; Month table.Quarter</code> as there are multiple (3) months in one quarter</p>  <p><code>Month table.Month &lt; A.Month</code> as there are\n","several rows of data for each month</p>  <p><code>Quarter table.Quarter &lt; B.Quarter</code> as there are several rows of data for each quarter</p>  <hr>\n","<p>Here is a visualisation of the relations</p>  <p><a href=\"https://i.stack.imgur.com/Euz3A.png\" rel=\"nofollow noreferrer\"><img\n","src=\"https://i.stack.imgur.com/Euz3A.png\" alt=\"enter image description here\"></a></p>  <hr>  <p>On my dashboard, I have a visual for <code>A</code>, and a\n","visual for <code>B</code></p>  <p>What I'm trying to do is to apply a page level filter with <code>Month table.Month</code>, thinking that therefore,\n","<code>A</code> should only show data for whatever month/months are selected in the filter, and <code>B</code> should show the data for the corresponding\n","quarter/quarters.</p>  <hr>  <ul> <li><p>Filter to January</p>  <p>Expectation:</p>  <ul> <li><code>A</code> shows data for January</li> <li><code>B</code>\n","shows data for Q1</li> </ul>  <p>Reality:</p>  <ul> <li><code>A</code> shows data for all months</li> <li><code>B</code> shows data for all quarters</li>\n","</ul></li> <li><p>Switch page level filter from <code>Month table.Month</code> to <code>Quarter table.Quarter</code> and set filter to Q1</p>\n","<p>Expectation:</p>  <ul> <li><code>A</code> shows data for January, February and Mars</li> <li><code>B</code> shows data for Q1</li> </ul>  <p>Reality:</p>\n","<ul> <li><code>A</code> shows data for all months</li> <li><code>B</code> shows data for Q1</li> </ul></li> </ul>  <hr>  <p>What am I missing?</p>\n","**************************************************************************************************************************************************************** \n","\n","Examples of High Code Counts *********************************************************************************************************************************** \n","\n","Question id: 55690\n","Question Tags\t: python dataset bigdata numpy \n","\n","Question Title\t: How to do numpy matmul broadcasting between two numpy tensors? \n","\n","Question Body\t:\n","\n","<p>I have the Pauli matrices which are (2x2) and complex</p>  <pre class=\"lang-py prettyprint-override\"><code>II = np.identity(2, dtype=complex) X =\n","np.array([[0, 1], [1, 0]], dtype=complex) Y = np.array([[0, -1j], [1j, 0]], dtype=complex) Z = np.array([[1, 0], [0, -1]], dtype=complex) </code></pre>  <p>and\n","a <code>depolarizing_error</code> function which takes in a normally distributed random number <code>param</code>, generated by\n","<code>np.random.normal(noise_mean, noise_sd)</code></p>  <pre class=\"lang-py prettyprint-override\"><code>def depolarizing_error(param):     XYZ =\n","np.sqrt(param/3)*np.array([X, Y, Z])     return np.array([np.sqrt(1-param)*II, XYZ[0], XYZ[1], XYZ[2]]) </code></pre>  <p>Now if I feed in a single number for\n","<code>param</code> of let's say <code>a</code>, my function should return an output of <code>np.array([np.sqrt(1-a)*II, a*X, a*Y, a*Z])</code> where\n","<code>a</code> is a <code>float</code> and <code>*</code> denotes the element-wise multiplication between <code>a</code> and the entries of the (2x2) matrices\n","<code>II, X, Y, Z</code>. Now for vectorization purposes, I wish to feed in an array of <code>param</code> i.e.</p>  <pre class=\"lang-py prettyprint-\n","override\"><code>param = np.array([a, b, c, ..., n])   Eqn(1) </code></pre>  <p>again with all <code>a, b, c, ..., n</code> generated independently by\n","<code>np.random.normal(noise_mean, noise_sd)</code> (I think it's doable with <code>np.random.normal(noise_mean, noise_sd, n)</code> or something) such that my\n","function now returns:</p>  <pre class=\"lang-py prettyprint-override\"><code>np.array([[np.sqrt(1-a)*II, a*X, a*Y, a*Z],           [np.sqrt(1-b)*II, b*X, b*Y,\n","b*Z],           ................................,           [np.sqrt(1-n)*II, n*X, n*Y, n*Z]]) </code></pre>  <p>I thought feeding in something like\n","<code>np.random.normal(noise_mean, noise_sd, n)</code> as <code>param</code>, giving output as <code>np.array([a, b, c,...,n])</code> would sort itself out and\n","return what I want above. but my <code>XYZ = np.sqrt(param/3)*np.array([X, Y, Z])</code> ended up doing element-wise dot product instead of element-wise\n","multiplication. I tried using param as <code>np.array([a, b])</code> and ended up with </p>  <pre><code>np.array([np.dot(np.sqrt(1-[a, b]), II),\n","np.dot(np.sqrt([a, b]/3),  X),           np.dot(np.sqrt([a, b]/3),  Y),           np.dot(np.sqrt([a, b]/3),  Z)]) </code></pre>  <p>instead. So far I've tried\n","something like</p>  <pre><code>def depolarizing_error(param):     XYZ = np.sqrt(param/3)@np.array([X, Y, Z])     return np.array([np.sqrt(1-param)*II, XYZ[0],\n","XYZ[1], XYZ[2]]) </code></pre>  <p>thinking that the matmul @ will just broadcast it conveniently for me but then I got really bogged down by the\n","dimensions.</p>  <p>Now my motivation for wanting to do all this is because I have another matrix that's given by:</p>  <pre><code>def random_angles(sd,\n","seq_length):     return np.random.normal(0, sd, (seq_length,3))  def unitary_error(params):     e_1 =\n","np.exp(-1j*(params[:,0]+params[:,2])/2)*np.cos(params[:,1]/2)     e_2 = np.exp(-1j*(params[:,0]-params[:,2])/2)*np.sin(params[:,1]/2)     return np.array([[e_1,\n","e_2], [-e_2.conj(), e_1.conj()]],                      dtype=complex).transpose(2,0,1) </code></pre>  <p>where here the size of <code>seq_length</code> is\n","equivalent to the number of entries in Eqn(1) <code>param</code>, denoting <code>N = seq_length = |param|</code> say. Here my <code>unitary_error</code>\n","function should give me an output of </p>  <pre><code>np.array([V_1, V_2, ..., V_N]) </code></pre>  <p>such that I'll be able to use <code>np.matmul</code> as\n","an attempt to implement vectorization like this</p>  <pre><code>np.array([V_1, V_2, ..., V_N])@np.array([[np.sqrt(1-a)*II, a*X, a*Y, a*Z],\n","[np.sqrt(1-b)*II, b*X, b*Y, b*Z],                                          ................................,\n","[np.sqrt(1-n)*II, n*X, n*Y, n*Z]])@np.array([V_1, V_2, ..., V_N]) </code></pre>  <p>to finally give</p>  <pre><code>np.array([[V_1@np.sqrt(1-a)*II@V_1,\n","V_1@a*X@V_1, V_1@a*Y@V_1, V_1@a*Z@V_1],           [V_2@np.sqrt(1-b)*II@V_2, V_2@b*X@V_2, V_2@b*Y@V_2, V_2@b*Z@V_2],           ................................,\n","[V_N@np.sqrt(1-n)*II@V_N, V_N@n*X@V_N, V_N@n*Y@V_N, V_N@n*Z@V_N]]) </code></pre>  <p>where here <code>@</code> denotes the element-wise dot-product</p>\n","**************************************************************************************************************************************************************** \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xWtnMmHEstiX"},"source":["# Task 5 Clean Body Text - Mispellings will Come out in the Wash\n",">5a. Remove code snippets\n",">5b. Remove html formatting\n",">5c. Expand contractions\n",">5d. Language detection to make sure everything is in English\n",">5e. Remove special characters\n",">5f. Simple Lemmatization\n",">5g. Named Entity Recognition with Spacy\n",">5h. POS tagging\n",">5i. Convert to lowercase\n",">5j. Remove stop words"]},{"cell_type":"code","metadata":{"id":"KNHk2aVpZeyC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371422089,"user_tz":420,"elapsed":13679,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"72950f18-757d-4b96-b7e9-7c8b56ca9636"},"source":["'''\n","5. Now we are ready to clean; from the examples above we see the following cleaning tasks are required:\n","5a. Remove code snippets using Beautiful Soup\n","'''\n","questions_df[\"Soup\"] = [BeautifulSoup(text,'lxml') for text in questions_df[\"Body\"]]\n","\n","def remove_code(soup):\n","  for tag in soup.find_all('code'):\n","    tag.replaceWith('')\n","  return soup\n","    \n","questions_df['NoCode'] = questions_df[\"Soup\"].apply(remove_code)\n","\n","# QC of result with question identified above\n","\n","codeinquestions = questions_df.loc[questions_df[\"Id\"] == 55690]\n","\n","codeinquestionslist = list(zip(codeinquestions[\"Id\"],codeinquestions[\"Tags_SpaceDelimited\"],codeinquestions[\"Title\"],codeinquestions[\"Body\"],codeinquestions[\"NoCode\"]))\n","                            \n","for id,tag,t,b,nc in codeinquestionslist[0:]:\n","    print(\"Examples of High Code Counts\", \"*\" * 131, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')\n","    print(\"Question Body WO Code\\t:\",nc, '\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Examples of High Code Counts *********************************************************************************************************************************** \n","\n","Question id: 55690\n","Question Tags\t: python dataset bigdata numpy \n","\n","Question Title\t: How to do numpy matmul broadcasting between two numpy tensors? \n","\n","Question Body\t:\n","\n","<p>I have the Pauli matrices which are (2x2) and complex</p>  <pre class=\"lang-py prettyprint-override\"><code>II = np.identity(2, dtype=complex) X =\n","np.array([[0, 1], [1, 0]], dtype=complex) Y = np.array([[0, -1j], [1j, 0]], dtype=complex) Z = np.array([[1, 0], [0, -1]], dtype=complex) </code></pre>  <p>and\n","a <code>depolarizing_error</code> function which takes in a normally distributed random number <code>param</code>, generated by\n","<code>np.random.normal(noise_mean, noise_sd)</code></p>  <pre class=\"lang-py prettyprint-override\"><code>def depolarizing_error(param):     XYZ =\n","np.sqrt(param/3)*np.array([X, Y, Z])     return np.array([np.sqrt(1-param)*II, XYZ[0], XYZ[1], XYZ[2]]) </code></pre>  <p>Now if I feed in a single number for\n","<code>param</code> of let's say <code>a</code>, my function should return an output of <code>np.array([np.sqrt(1-a)*II, a*X, a*Y, a*Z])</code> where\n","<code>a</code> is a <code>float</code> and <code>*</code> denotes the element-wise multiplication between <code>a</code> and the entries of the (2x2) matrices\n","<code>II, X, Y, Z</code>. Now for vectorization purposes, I wish to feed in an array of <code>param</code> i.e.</p>  <pre class=\"lang-py prettyprint-\n","override\"><code>param = np.array([a, b, c, ..., n])   Eqn(1) </code></pre>  <p>again with all <code>a, b, c, ..., n</code> generated independently by\n","<code>np.random.normal(noise_mean, noise_sd)</code> (I think it's doable with <code>np.random.normal(noise_mean, noise_sd, n)</code> or something) such that my\n","function now returns:</p>  <pre class=\"lang-py prettyprint-override\"><code>np.array([[np.sqrt(1-a)*II, a*X, a*Y, a*Z],           [np.sqrt(1-b)*II, b*X, b*Y,\n","b*Z],           ................................,           [np.sqrt(1-n)*II, n*X, n*Y, n*Z]]) </code></pre>  <p>I thought feeding in something like\n","<code>np.random.normal(noise_mean, noise_sd, n)</code> as <code>param</code>, giving output as <code>np.array([a, b, c,...,n])</code> would sort itself out and\n","return what I want above. but my <code>XYZ = np.sqrt(param/3)*np.array([X, Y, Z])</code> ended up doing element-wise dot product instead of element-wise\n","multiplication. I tried using param as <code>np.array([a, b])</code> and ended up with </p>  <pre><code>np.array([np.dot(np.sqrt(1-[a, b]), II),\n","np.dot(np.sqrt([a, b]/3),  X),           np.dot(np.sqrt([a, b]/3),  Y),           np.dot(np.sqrt([a, b]/3),  Z)]) </code></pre>  <p>instead. So far I've tried\n","something like</p>  <pre><code>def depolarizing_error(param):     XYZ = np.sqrt(param/3)@np.array([X, Y, Z])     return np.array([np.sqrt(1-param)*II, XYZ[0],\n","XYZ[1], XYZ[2]]) </code></pre>  <p>thinking that the matmul @ will just broadcast it conveniently for me but then I got really bogged down by the\n","dimensions.</p>  <p>Now my motivation for wanting to do all this is because I have another matrix that's given by:</p>  <pre><code>def random_angles(sd,\n","seq_length):     return np.random.normal(0, sd, (seq_length,3))  def unitary_error(params):     e_1 =\n","np.exp(-1j*(params[:,0]+params[:,2])/2)*np.cos(params[:,1]/2)     e_2 = np.exp(-1j*(params[:,0]-params[:,2])/2)*np.sin(params[:,1]/2)     return np.array([[e_1,\n","e_2], [-e_2.conj(), e_1.conj()]],                      dtype=complex).transpose(2,0,1) </code></pre>  <p>where here the size of <code>seq_length</code> is\n","equivalent to the number of entries in Eqn(1) <code>param</code>, denoting <code>N = seq_length = |param|</code> say. Here my <code>unitary_error</code>\n","function should give me an output of </p>  <pre><code>np.array([V_1, V_2, ..., V_N]) </code></pre>  <p>such that I'll be able to use <code>np.matmul</code> as\n","an attempt to implement vectorization like this</p>  <pre><code>np.array([V_1, V_2, ..., V_N])@np.array([[np.sqrt(1-a)*II, a*X, a*Y, a*Z],\n","[np.sqrt(1-b)*II, b*X, b*Y, b*Z],                                          ................................,\n","[np.sqrt(1-n)*II, n*X, n*Y, n*Z]])@np.array([V_1, V_2, ..., V_N]) </code></pre>  <p>to finally give</p>  <pre><code>np.array([[V_1@np.sqrt(1-a)*II@V_1,\n","V_1@a*X@V_1, V_1@a*Y@V_1, V_1@a*Z@V_1],           [V_2@np.sqrt(1-b)*II@V_2, V_2@b*X@V_2, V_2@b*Y@V_2, V_2@b*Z@V_2],           ................................,\n","[V_N@np.sqrt(1-n)*II@V_N, V_N@n*X@V_N, V_N@n*Y@V_N, V_N@n*Z@V_N]]) </code></pre>  <p>where here <code>@</code> denotes the element-wise dot-product</p>\n","**************************************************************************************************************************************************************** \n","\n","Question Body WO Code\t: <html><body><p>I have the Pauli matrices which are (2x2) and complex</p>\n","<pre class=\"lang-py prettyprint-override\"></pre>\n","<p>and a  function which takes in a normally distributed random number , generated by </p>\n","<pre class=\"lang-py prettyprint-override\"></pre>\n","<p>Now if I feed in a single number for  of let's say , my function should return an output of  where  is a  and  denotes the element-wise multiplication between  and the entries of the (2x2) matrices .\n","Now for vectorization purposes, I wish to feed in an array of  i.e.</p>\n","<pre class=\"lang-py prettyprint-override\"></pre>\n","<p>again with all  generated independently by  (I think it's doable with  or something)\n","such that my function now returns:</p>\n","<pre class=\"lang-py prettyprint-override\"></pre>\n","<p>I thought feeding in something like  as , giving output as  would sort itself out and return what I want above. but my  ended up doing element-wise dot product instead of element-wise multiplication. I tried using param as \n","and ended up with </p>\n","<pre></pre>\n","<p>instead. So far I've tried something like</p>\n","<pre></pre>\n","<p>thinking that the matmul @ will just broadcast it conveniently for me but then I got really bogged down by the dimensions.</p>\n","<p>Now my motivation for wanting to do all this is because I have another matrix that's given by:</p>\n","<pre></pre>\n","<p>where here the size of  is equivalent to the number of entries in Eqn(1) , denoting  say. Here my  function should give me an output of </p>\n","<pre></pre>\n","<p>such that I'll be able to use  as an attempt to implement vectorization like this</p>\n","<pre></pre>\n","<p>to finally give</p>\n","<pre></pre>\n","<p>where here  denotes the element-wise dot-product</p>\n","</body></html> \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Bu4iF4QA3TZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371455677,"user_tz":420,"elapsed":11139,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"1adb7ad1-3a1f-4ae6-af61-c49dd3f0d797"},"source":["'''\n",">5b. Remove html formatting\n","\n","Starting with stripping html formatting - using Beautiful Soup -  results look good!\n","'''\n","# Must first convert results from above back to text string\n","\n","def convert_string(soup):\n","  for text in soup.find_all():\n","    return str(text)\n","\n","questions_df[\"NoCodeString\"] = questions_df[\"NoCode\"].apply(convert_string)\n","\n","#Then get text\n","questions_df[\"BodyText\"] = [BeautifulSoup(text,'lxml').get_text() for text in questions_df[\"NoCodeString\"]]\n","\n","codeinquestions = questions_df.loc[questions_df[\"Id\"] == 55690]\n","\n","codeinquestionslist = list(zip(codeinquestions[\"Id\"],codeinquestions[\"Tags_SpaceDelimited\"],codeinquestions[\"Title\"],codeinquestions[\"Body\"],codeinquestions[\"BodyText\"]))\n","                            \n","for id,tag,t,b,bt in codeinquestionslist[0:]:\n","    print(\"Examples of High Code Counts\", \"*\" * 131, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Question Tags\\t:\",tag,'\\n') \n","    print(\"Question Title\\t:\",t,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(b, 160)) \n","    print(\"*\" * 160,'\\n')\n","    print(\"Question Body WO Code\\t:\" '\\n')\n","    print(textwrap.fill(bt, 160)) \n","    print(\"*\" * 160,'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Examples of High Code Counts *********************************************************************************************************************************** \n","\n","Question id: 55690\n","Question Tags\t: python dataset bigdata numpy \n","\n","Question Title\t: How to do numpy matmul broadcasting between two numpy tensors? \n","\n","Question Body\t:\n","\n","<p>I have the Pauli matrices which are (2x2) and complex</p>  <pre class=\"lang-py prettyprint-override\"><code>II = np.identity(2, dtype=complex) X =\n","np.array([[0, 1], [1, 0]], dtype=complex) Y = np.array([[0, -1j], [1j, 0]], dtype=complex) Z = np.array([[1, 0], [0, -1]], dtype=complex) </code></pre>  <p>and\n","a <code>depolarizing_error</code> function which takes in a normally distributed random number <code>param</code>, generated by\n","<code>np.random.normal(noise_mean, noise_sd)</code></p>  <pre class=\"lang-py prettyprint-override\"><code>def depolarizing_error(param):     XYZ =\n","np.sqrt(param/3)*np.array([X, Y, Z])     return np.array([np.sqrt(1-param)*II, XYZ[0], XYZ[1], XYZ[2]]) </code></pre>  <p>Now if I feed in a single number for\n","<code>param</code> of let's say <code>a</code>, my function should return an output of <code>np.array([np.sqrt(1-a)*II, a*X, a*Y, a*Z])</code> where\n","<code>a</code> is a <code>float</code> and <code>*</code> denotes the element-wise multiplication between <code>a</code> and the entries of the (2x2) matrices\n","<code>II, X, Y, Z</code>. Now for vectorization purposes, I wish to feed in an array of <code>param</code> i.e.</p>  <pre class=\"lang-py prettyprint-\n","override\"><code>param = np.array([a, b, c, ..., n])   Eqn(1) </code></pre>  <p>again with all <code>a, b, c, ..., n</code> generated independently by\n","<code>np.random.normal(noise_mean, noise_sd)</code> (I think it's doable with <code>np.random.normal(noise_mean, noise_sd, n)</code> or something) such that my\n","function now returns:</p>  <pre class=\"lang-py prettyprint-override\"><code>np.array([[np.sqrt(1-a)*II, a*X, a*Y, a*Z],           [np.sqrt(1-b)*II, b*X, b*Y,\n","b*Z],           ................................,           [np.sqrt(1-n)*II, n*X, n*Y, n*Z]]) </code></pre>  <p>I thought feeding in something like\n","<code>np.random.normal(noise_mean, noise_sd, n)</code> as <code>param</code>, giving output as <code>np.array([a, b, c,...,n])</code> would sort itself out and\n","return what I want above. but my <code>XYZ = np.sqrt(param/3)*np.array([X, Y, Z])</code> ended up doing element-wise dot product instead of element-wise\n","multiplication. I tried using param as <code>np.array([a, b])</code> and ended up with </p>  <pre><code>np.array([np.dot(np.sqrt(1-[a, b]), II),\n","np.dot(np.sqrt([a, b]/3),  X),           np.dot(np.sqrt([a, b]/3),  Y),           np.dot(np.sqrt([a, b]/3),  Z)]) </code></pre>  <p>instead. So far I've tried\n","something like</p>  <pre><code>def depolarizing_error(param):     XYZ = np.sqrt(param/3)@np.array([X, Y, Z])     return np.array([np.sqrt(1-param)*II, XYZ[0],\n","XYZ[1], XYZ[2]]) </code></pre>  <p>thinking that the matmul @ will just broadcast it conveniently for me but then I got really bogged down by the\n","dimensions.</p>  <p>Now my motivation for wanting to do all this is because I have another matrix that's given by:</p>  <pre><code>def random_angles(sd,\n","seq_length):     return np.random.normal(0, sd, (seq_length,3))  def unitary_error(params):     e_1 =\n","np.exp(-1j*(params[:,0]+params[:,2])/2)*np.cos(params[:,1]/2)     e_2 = np.exp(-1j*(params[:,0]-params[:,2])/2)*np.sin(params[:,1]/2)     return np.array([[e_1,\n","e_2], [-e_2.conj(), e_1.conj()]],                      dtype=complex).transpose(2,0,1) </code></pre>  <p>where here the size of <code>seq_length</code> is\n","equivalent to the number of entries in Eqn(1) <code>param</code>, denoting <code>N = seq_length = |param|</code> say. Here my <code>unitary_error</code>\n","function should give me an output of </p>  <pre><code>np.array([V_1, V_2, ..., V_N]) </code></pre>  <p>such that I'll be able to use <code>np.matmul</code> as\n","an attempt to implement vectorization like this</p>  <pre><code>np.array([V_1, V_2, ..., V_N])@np.array([[np.sqrt(1-a)*II, a*X, a*Y, a*Z],\n","[np.sqrt(1-b)*II, b*X, b*Y, b*Z],                                          ................................,\n","[np.sqrt(1-n)*II, n*X, n*Y, n*Z]])@np.array([V_1, V_2, ..., V_N]) </code></pre>  <p>to finally give</p>  <pre><code>np.array([[V_1@np.sqrt(1-a)*II@V_1,\n","V_1@a*X@V_1, V_1@a*Y@V_1, V_1@a*Z@V_1],           [V_2@np.sqrt(1-b)*II@V_2, V_2@b*X@V_2, V_2@b*Y@V_2, V_2@b*Z@V_2],           ................................,\n","[V_N@np.sqrt(1-n)*II@V_N, V_N@n*X@V_N, V_N@n*Y@V_N, V_N@n*Z@V_N]]) </code></pre>  <p>where here <code>@</code> denotes the element-wise dot-product</p>\n","**************************************************************************************************************************************************************** \n","\n","Question Body WO Code\t:\n","\n","I have the Pauli matrices which are (2x2) and complex  and a  function which takes in a normally distributed random number , generated by   Now if I feed in a\n","single number for  of let's say , my function should return an output of  where  is a  and  denotes the element-wise multiplication between  and the entries of\n","the (2x2) matrices . Now for vectorization purposes, I wish to feed in an array of  i.e.  again with all  generated independently by  (I think it's doable with\n","or something) such that my function now returns:  I thought feeding in something like  as , giving output as  would sort itself out and return what I want\n","above. but my  ended up doing element-wise dot product instead of element-wise multiplication. I tried using param as  and ended up with   instead. So far I've\n","tried something like  thinking that the matmul @ will just broadcast it conveniently for me but then I got really bogged down by the dimensions. Now my\n","motivation for wanting to do all this is because I have another matrix that's given by:  where here the size of  is equivalent to the number of entries in\n","Eqn(1) , denoting  say. Here my  function should give me an output of   such that I'll be able to use  as an attempt to implement vectorization like this  to\n","finally give  where here  denotes the element-wise dot-product\n","**************************************************************************************************************************************************************** \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2im0o55SJYNi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371476028,"user_tz":420,"elapsed":8596,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"50510c49-6364-4f71-e24f-5f779f7f8e2c"},"source":["'''\n","5c. Expand contractions using contractions module and associated dictionary; process is to split on words then rejoin after contractions are expanded\n","'''\n","questions_df['bodytext_expanded'] = questions_df['BodyText'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n","questions_df['bodytext_expanded'] = questions_df['bodytext_expanded'].str.join(\" \")\n","\n","print(\"Before: \", textwrap.fill(questions_df['BodyText'][1], 160))\n","print('\\n')\n","print(\"After: \", textwrap.fill(questions_df['bodytext_expanded'][1], 160))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before:  As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an\n","applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular\n","pieces or papers.\n","\n","\n","After:  As a researcher and instructor, I am looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an\n","applied perspective. To be clear, I am especially interested in a thorough overview that provides material suitable for a college-level course, not particular\n","pieces or papers.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v-niU_ob_wB7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609372323552,"user_tz":420,"elapsed":3211,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"f69f8a7a-251d-45ee-8972-059fa2865922"},"source":["'''\n","5d. Check to see if all questions are in English; Using Facebook's fasttext library and their prebuilt model\n","\n","A few rows come back as a different language than english (fr,ja,kn), a review of these rows indicates that the\n","questions are reduced to almost nothing with the removal of the code snippets producing erroneous results;\n","I have found that the removal of the code snippets produces a more consistent result of english whereas before code removal\n","other languages were detected where string examples within the question body text were in an alternate language.  See the examples below in\n","the first idlst\n","Good catch Facebook! \n","Bottomline is we do not need to exclude any rows due to a different language being used.\n","'''\n","pretrained_model = '/content/drive/My Drive/Capstone2/Data/lid.176.bin'\n","model = fasttext.load_model(pretrained_model)\n","langs = []\n","for sent in questions_df['bodytext_expanded']:\n","    lang = model.predict(sent)[0]\n","    langs.append(str(lang)[11:13])\n","questions_df['lang'] = langs\n","\n","lang_df = questions_df[['Id','bodytext_expanded','lang']].groupby(by='lang').count()\n","#print(lang_df.head(12))\n","\n","questions_df[questions_df['lang'] != 'en']\n","\n","#idlst = [224,53950,55166,75109]\n","idlst = [32040,61424,66366]\n","\n","#lang_sc_questions = questions_df.loc[questions_df[\"Id\"].isin(idlst)]\n","#lang_sc_questions.head()\n","#langsc_quest_lst = list(zip(lang_sc_questions[\"Id\"],lang_sc_questions[\"lang\"],lang_sc_questions[\"Body\"],lang_sc_questions[\"BodyText\"],lang_sc_questions['bodytext_expanded']))\n","          \n","#for id,l,b,btext,bexp in langsc_quest_lst[0:]:\n","#    print(\"Examples of Non-English Language\", \"*\" * 127, '\\n')\n","#    print(\"Question id\\t:\",id)\n","#    print(\"Language\\t:\",l)\n","#    print(\"Question Body1\\t:\" '\\n')\n","#    print(textwrap.fill(b, 160)) \n","#    print(\"*\" * 160,'\\n')\n","#    print(\"Question Body2\\t:\" '\\n')\n","#    print(textwrap.fill(btext, 160)) \n","#    print(\"*\" * 160,'\\n')\n","#    print(\"Question Body3\\t:\" '\\n')\n","#    print(textwrap.fill(bexp, 160)) \n","#    print(\"*\" * 160,'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4BluDD0tJpWb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609371836334,"user_tz":420,"elapsed":1134,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"cabce626-4d4d-4c9d-d5f3-93a99ff6fdd8"},"source":["'''\n","5e. Remove special characters and punctuation; first let's take a look at what special characters we have in the body text;\n","then remove all that we need to; \n","'''\n","nopunct = string.ascii_letters+string.digits+string.whitespace\n","\n","def stripalphanum(InputString):\n","    return \" \".join([ch for ch in InputString if ch not in (nopunct)])\n","      \n","questions_df['punct_only'] = questions_df['bodytext_expanded'].apply(stripalphanum)\n","\n","print(\"Full Text: \", textwrap.fill(questions_df['bodytext_expanded'][2], 160))\n","print('\\n')\n","print(\"Special Characters: \", questions_df['punct_only'][2])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Full Text:  I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed. My particular question\n","is in regards to Data Mining. I took a graduate class in Data Mining a few years back. What are the differences between Data Science and Data Mining and in\n","particular what more would I need to look at to become proficient in Data Mining?\n","\n","\n","Special Characters:  . . . ?\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"olT1Swxt9lnA","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1609372230729,"user_tz":420,"elapsed":410,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"752ec01b-1bbe-4098-bd43-8eed627b3e84"},"source":["'''\n","5e. Task continued - analyzing and removing special characters\n","Let's get a list of all the Special Characters and Their Frequency Counts and save to a new dataframe\n","\n","The most common special characters are typical and expected, but also due to the different language examples identified above in a \n","handful of the questions, we see accented letters, chinese and arabic characters too.\n","\n","Also note that there are Greek letters, theta, alpha, and beta that are pertinent to data science and that should be left in place also.\n","I am choosing to remove all of the common characters and leave the exceptions noted in these comments.\n","'''\n","all_specchar = [item for sublist in questions_df['punct_only'].values for item in sublist]\n","my_set = set(all_specchar)\n","unique_specchar = list(my_set)\n","\n","specchar_freq = collections.Counter(all_specchar)\n","\n","kk=[list(specchar_freq.keys()),list(specchar_freq.values())]\n","\n","specchar_freq_df = pd.DataFrame(np.array(kk).T, columns=['SpecChar','SpecChar_Freq'])\n","specchar_freq_df['SpecChar_Freq'] = pd.to_numeric(specchar_freq_df['SpecChar_Freq'])\n","\n","# Using this code to make the output clear\n","print(\"There are a total of {} special characters this dataset. \\n\".format(len(all_specchar)))\n","\n","print(\"There are {} unique special characters in this dataset. \\n\".format(len(unique_specchar)))\n","\n","print(\"Here is a list of the top 50 special characters sorted by frequency: \\n\")\n","specchar_freq_df['SpecChar_Freq'] = pd.to_numeric(specchar_freq_df['SpecChar_Freq'])\n","specchar_freq_df.sort_values(by='SpecChar_Freq', ascending = False).head(60)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are a total of 1263953 special characters this dataset. \n","\n","There are 329 unique special characters in this dataset. \n","\n","Here is a list of the top 50 special characters sorted by frequency: \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SpecChar</th>\n","      <th>SpecChar_Freq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td></td>\n","      <td>619827</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>.</td>\n","      <td>149521</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>,</td>\n","      <td>115965</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>)</td>\n","      <td>42474</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>(</td>\n","      <td>40843</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-</td>\n","      <td>39317</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>?</td>\n","      <td>35830</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>:</td>\n","      <td>31775</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>$</td>\n","      <td>27289</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>_</td>\n","      <td>21342</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>/</td>\n","      <td>18317</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>\\</td>\n","      <td>18115</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"</td>\n","      <td>16065</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>'</td>\n","      <td>13040</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>}</td>\n","      <td>12804</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>{</td>\n","      <td>12803</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>=</td>\n","      <td>10760</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>^</td>\n","      <td>4066</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>+</td>\n","      <td>3984</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>%</td>\n","      <td>3909</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>[</td>\n","      <td>3721</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>]</td>\n","      <td>3689</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>!</td>\n","      <td>2690</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>|</td>\n","      <td>2606</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>&amp;</td>\n","      <td>1883</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>&gt;</td>\n","      <td>1816</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>;</td>\n","      <td>1816</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>*</td>\n","      <td>1760</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>~</td>\n","      <td>1183</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>#</td>\n","      <td>748</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>&lt;</td>\n","      <td>592</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>@</td>\n","      <td>312</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>’</td>\n","      <td>303</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>“</td>\n","      <td>281</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>”</td>\n","      <td>273</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>–</td>\n","      <td>172</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>‘</td>\n","      <td>158</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>−</td>\n","      <td>132</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>…</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>`</td>\n","      <td>92</td>\n","    </tr>\n","    <tr>\n","      <th>89</th>\n","      <td>×</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>104</th>\n","      <td>λ</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>—</td>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>´</td>\n","      <td>49</td>\n","    </tr>\n","    <tr>\n","      <th>177</th>\n","      <td>我</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>•</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>é</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>178</th>\n","      <td>想</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>∈</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>β</td>\n","      <td>36</td>\n","    </tr>\n","    <tr>\n","      <th>107</th>\n","      <td>θ</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>​</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>α</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>110</th>\n","      <td>σ</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>108</th>\n","      <td>·</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>μ</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>124</th>\n","      <td>²</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>ï</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>°</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>€</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    SpecChar  SpecChar_Freq\n","1                    619827\n","5          .         149521\n","0          ,         115965\n","7          )          42474\n","6          (          40843\n","3          -          39317\n","4          ?          35830\n","9          :          31775\n","31         $          27289\n","19         _          21342\n","8          /          18317\n","24         \\          18115\n","2          \"          16065\n","12         '          13040\n","26         }          12804\n","25         {          12803\n","27         =          10760\n","36         ^           4066\n","17         +           3984\n","21         %           3909\n","29         [           3721\n","30         ]           3689\n","14         !           2690\n","38         |           2606\n","13         &           1883\n","34         >           1816\n","10         ;           1816\n","28         *           1760\n","23         ~           1183\n","18         #            748\n","32         <            592\n","22         @            312\n","15         ’            303\n","11         “            281\n","33         ”            273\n","42         –            172\n","44         ‘            158\n","56         −            132\n","63         …            110\n","41         `             92\n","89         ×             72\n","104        λ             60\n","16         —             51\n","35         ´             49\n","177        我             45\n","37         •             42\n","73         é             42\n","178        想             41\n","20         ∈             40\n","64         β             36\n","107        θ             35\n","78         ​             34\n","79         α             32\n","110        σ             29\n","108        ·             28\n","92         μ             26\n","124        ²             23\n","83         ï             23\n","57         °             21\n","45         €             20"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"HGKPu4zHakWK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609372332693,"user_tz":420,"elapsed":303,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"21c98969-b269-4068-c734-e5ecdef840b7"},"source":["'''\n","5e. Let's convert the accented characters prior to removing the other special characters\n","\n","Interestingly, the Chinese characters have been removed through this process too;\n","I guess that is ok.\n","'''\n","def remove_accented_chars(text):\n","    noaccent_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return noaccent_text\n","\n","questions_df['bodytext_noaccents'] = questions_df['bodytext_expanded'].apply(remove_accented_chars)\n","\n","lang_sc_questions_new = questions_df.loc[questions_df[\"Id\"].isin(idlst)]\n","\n","langsc_quest_lst_new = list(zip(lang_sc_questions_new[\"Id\"],lang_sc_questions_new[\"lang\"],lang_sc_questions_new[\"bodytext_expanded\"],lang_sc_questions_new[\"bodytext_noaccents\"]))\n","          \n","for id,l,btext,na in langsc_quest_lst_new[0:]:\n","    print(\"Examples of Non-English Language\", \"*\" * 127, '\\n')\n","    print(\"Question id:\",id)\n","    print(\"Question Language\\t:\",l,'\\n') \n","    print(\"Question Body\\t:\" '\\n')\n","    print(textwrap.fill(btext, 160)) \n","    print(\"Question Body With Accents Removed\\t:\" '\\n')\n","    print(textwrap.fill(na, 160)) \n","    print(\"*\" * 160,'\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Examples of Non-English Language ******************************************************************************************************************************* \n","\n","Question id: 32040\n","Question Language\t: fr \n","\n","Question Body\t:\n","\n","R code : OUTPUT :\n","Question Body With Accents Removed\t:\n","\n","R code : OUTPUT :\n","**************************************************************************************************************************************************************** \n","\n","Examples of Non-English Language ******************************************************************************************************************************* \n","\n","Question id: 61424\n","Question Language\t: ja \n","\n","Question Body\t:\n","\n","ERROR:\n","Question Body With Accents Removed\t:\n","\n","ERROR:\n","**************************************************************************************************************************************************************** \n","\n","Examples of Non-English Language ******************************************************************************************************************************* \n","\n","Question id: 66366\n","Question Language\t: kn \n","\n","Question Body\t:\n","\n","}\n","Question Body With Accents Removed\t:\n","\n","}\n","**************************************************************************************************************************************************************** \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yUjrrIhVRR2U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609372349694,"user_tz":420,"elapsed":1437,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"71496413-d82a-4661-dffc-462e67d54d72"},"source":["'''\n","5e. Choosing to remove only the most common special characters, leaving remaining foreign language and greek characters in place \n","'''\n","punct = '!\"#$%&\\'()*+,./:;-<=>?@[\\\\]^_`{|}~'\n","\n","def strip_spec_char(InputString):\n","    return \"\".join([ch for ch in InputString if ch not in (punct)])\n","    \n","questions_df['bodytext_nospch'] = questions_df['bodytext_noaccents'].apply(strip_spec_char)\n","#questions_df.head()\n","\n","print(\"Before: \", textwrap.fill(questions_df['bodytext_noaccents'][3], 160))\n","print('\\n')\n","print(\"After: \", textwrap.fill(questions_df['bodytext_nospch'][3], 160))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before:  In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-\n","relational databases?\n","\n","\n","After:  In which situations would one system be preferred over the other What are the relative advantages and disadvantages of relational databases versus nonrelational\n","databases\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RnA4GuTE4LaN","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1609372819402,"user_tz":420,"elapsed":86796,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"e92825d3-4ed1-482a-8a0c-e1f2cba6f511"},"source":["'''\n","5f Simple Lemmatizing \n","'''\n","lm = WordNetLemmatizer()\n","\n","def lem_vrbs(text):\n","  return ' '.join([lm.lemmatize(w,'v') for w in word_tokenize(text)])\n","\n","questions_df['temp'] = questions_df['bodytext_nospch'].apply(lem_vrbs)\n","\n","def lem_nouns(text):\n","  return ' '.join([lm.lemmatize(w,'n') for w in word_tokenize(text)])\n","\n","questions_df['temp2'] = questions_df['temp'].apply(lem_nouns)\n","\n","def lem_adj(text):\n","  return ' '.join([lm.lemmatize(w,'a') for w in word_tokenize(text)])\n","\n","questions_df['temp3'] = questions_df['temp2'].apply(lem_adj)\n","\n","def lem_adv(text):\n","  return ' '.join([lm.lemmatize(w,'r') for w in word_tokenize(text)])\n","\n","questions_df['BodyText_Lemma'] = questions_df['temp3'].apply(lem_adv)\n","\n","print(\"Before Special Character Removal: \")\n","print(textwrap.fill(questions_df['bodytext_noaccents'][8], 160))\n","print('\\n')\n","print(\"Before Lemmatization: \")\n","print(textwrap.fill(questions_df['bodytext_nospch'][8], 160))\n","print('\\n')\n","print(\"After: \") \n","print(textwrap.fill(questions_df['BodyText_Lemma'][8], 160))\n","\n","questions_df.drop(axis = 1, labels = ['temp','temp2','temp3'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before Special Character Removal: \n","I have a bunch of customer profiles stored in a elasticsearch cluster. These profiles are now used for creation of target groups for our email subscriptions.\n","Target groups are now formed manually using elasticsearch faceted search capabilities (like get all male customers of age 23 with one car and 3 children). How\n","could I search for interesting groups automatically - using data science, machine learning, clustering or something else? r programming language seems to be a\n","good tool for this task, but I can not form a methodology of such group search. One solution is to somehow find the largest clusters of customers and use them\n","as target groups, so the question is: How can I automatically choose largest clusters of similar customers (similar by parameters that I do not know at this\n","moment)? For example: my program will connect to elasticsearch, offload customer data to CSV and using R language script will find that large portion of\n","customers are male with no children and another large portion of customers have a car and their eye color is brown.\n","\n","\n","Before Lemmatization: \n","I have a bunch of customer profiles stored in a elasticsearch cluster These profiles are now used for creation of target groups for our email subscriptions\n","Target groups are now formed manually using elasticsearch faceted search capabilities like get all male customers of age 23 with one car and 3 children How\n","could I search for interesting groups automatically  using data science machine learning clustering or something else r programming language seems to be a good\n","tool for this task but I can not form a methodology of such group search One solution is to somehow find the largest clusters of customers and use them as\n","target groups so the question is How can I automatically choose largest clusters of similar customers similar by parameters that I do not know at this moment\n","For example my program will connect to elasticsearch offload customer data to CSV and using R language script will find that large portion of customers are male\n","with no children and another large portion of customers have a car and their eye color is brown\n","\n","\n","After: \n","I have a bunch of customer profile store in a elasticsearch cluster These profile be now use for creation of target group for our email subscription Target\n","group be now form manually use elasticsearch faceted search capability like get all male customer of age 23 with one car and 3 child How could I search for\n","interest group automatically use data science machine learn cluster or something else r program language seem to be a good tool for this task but I can not form\n","a methodology of such group search One solution be to somehow find the large cluster of customer and use them a target group so the question be How can I\n","automatically choose large cluster of similar customer similar by parameter that I do not know at this moment For example my program will connect to\n","elasticsearch offload customer data to CSV and use R language script will find that large portion of customer be male with no child and another large portion of\n","customer have a car and their eye color be brown\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>PostTypeId</th>\n","      <th>CreationDate</th>\n","      <th>Score</th>\n","      <th>ViewCount</th>\n","      <th>Body</th>\n","      <th>OwnerUserId</th>\n","      <th>LastActivityDate</th>\n","      <th>Title</th>\n","      <th>Tags</th>\n","      <th>AnswerCount</th>\n","      <th>CommentCount</th>\n","      <th>FavoriteCount</th>\n","      <th>ClosedDate</th>\n","      <th>ContentLicense</th>\n","      <th>Tags_SpaceDelimited</th>\n","      <th>Tags_Clean</th>\n","      <th>TagCount</th>\n","      <th>Tag1</th>\n","      <th>Tag2</th>\n","      <th>Tag3</th>\n","      <th>Tag4</th>\n","      <th>Tag5</th>\n","      <th>Tag1_Freq</th>\n","      <th>Tag2_Freq</th>\n","      <th>Tag3_Freq</th>\n","      <th>Tag4_Freq</th>\n","      <th>Tag5_Freq</th>\n","      <th>Total_Tag_Freqency</th>\n","      <th>Tag1_Renamed</th>\n","      <th>Tag2_Renamed</th>\n","      <th>Tag3_Renamed</th>\n","      <th>Tag4_Renamed</th>\n","      <th>Tag5_Renamed</th>\n","      <th>TopTag</th>\n","      <th>Elapsed_Time</th>\n","      <th>Elapsed_Time_Int</th>\n","      <th>rank</th>\n","      <th>Tag1_Renamed2</th>\n","      <th>TopTag_Revised</th>\n","      <th>NumQuestions</th>\n","      <th>BodyBoldCount</th>\n","      <th>ParagraphCount</th>\n","      <th>CodeCount</th>\n","      <th>Soup</th>\n","      <th>NoCode</th>\n","      <th>NoCodeString</th>\n","      <th>BodyText</th>\n","      <th>bodytext_expanded</th>\n","      <th>lang</th>\n","      <th>punct_only</th>\n","      <th>bodytext_noaccents</th>\n","      <th>bodytext_nospch</th>\n","      <th>BodyText_Lemma</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2014-05-13 23:58:30.457</td>\n","      <td>9</td>\n","      <td>708</td>\n","      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n","      <td>5</td>\n","      <td>2014-05-14 00:36:31.077</td>\n","      <td>How can I do simple machine learning without h...</td>\n","      <td>&lt;machine-learning&gt;</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2014-05-14 14:40:25.950</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>machine-learning</td>\n","      <td>[machine-learning]</td>\n","      <td>1</td>\n","      <td>machine-learning</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7766</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7766.0</td>\n","      <td>machine-learning</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>0 days 00:38:00.620000</td>\n","      <td>0</td>\n","      <td>21642</td>\n","      <td>machine-learning</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;I've always been interested in machine l...</td>\n","      <td>[[[&lt;p&gt;I've always been interested in machine l...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;I've always been interested in ...</td>\n","      <td>I've always been interested in machine learnin...</td>\n","      <td>I have always been interested in machine learn...</td>\n","      <td>en</td>\n","      <td>, \" \" - - ? , \" \" , , , , , , . , , ?</td>\n","      <td>I have always been interested in machine learn...</td>\n","      <td>I have always been interested in machine learn...</td>\n","      <td>I have always be interest in machine learn but...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>2014-05-14 00:11:06.457</td>\n","      <td>4</td>\n","      <td>441</td>\n","      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n","      <td>36</td>\n","      <td>2014-05-16 13:45:00.237</td>\n","      <td>What open-source books (or other materials) pr...</td>\n","      <td>&lt;education&gt;&lt;open-source&gt;</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2014-05-14 08:40:54.950</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>education open-source</td>\n","      <td>[education, open-source]</td>\n","      <td>2</td>\n","      <td>education</td>\n","      <td>open-source</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>33</td>\n","      <td>16.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>49.0</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2 days 13:33:53.780000</td>\n","      <td>2</td>\n","      <td>16792</td>\n","      <td>Other</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;As a researcher and instructor, I'm look...</td>\n","      <td>[[[&lt;p&gt;As a researcher and instructor, I'm look...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;As a researcher and instructor,...</td>\n","      <td>As a researcher and instructor, I'm looking fo...</td>\n","      <td>As a researcher and instructor, I am looking f...</td>\n","      <td>en</td>\n","      <td>, - ( ) . , - , .</td>\n","      <td>As a researcher and instructor, I am looking f...</td>\n","      <td>As a researcher and instructor I am looking fo...</td>\n","      <td>As a researcher and instructor I be look for o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>2014-05-14 01:25:59.677</td>\n","      <td>22</td>\n","      <td>1717</td>\n","      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n","      <td>66</td>\n","      <td>2014-06-20 17:36:05.023</td>\n","      <td>Is Data Science the Same as Data Mining?</td>\n","      <td>&lt;data-mining&gt;&lt;definitions&gt;</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>data-mining definitions</td>\n","      <td>[data-mining, definitions]</td>\n","      <td>2</td>\n","      <td>data-mining</td>\n","      <td>definitions</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1005</td>\n","      <td>31.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1036.0</td>\n","      <td>data-mining</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>37 days 16:10:05.346000</td>\n","      <td>37</td>\n","      <td>15799</td>\n","      <td>data-mining</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;I am sure data science as will be discus...</td>\n","      <td>[[[&lt;p&gt;I am sure data science as will be discus...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;I am sure data science as will ...</td>\n","      <td>I am sure data science as will be discussed in...</td>\n","      <td>I am sure data science as will be discussed in...</td>\n","      <td>en</td>\n","      <td>. . . ?</td>\n","      <td>I am sure data science as will be discussed in...</td>\n","      <td>I am sure data science as will be discussed in...</td>\n","      <td>I be sure data science a will be discus in thi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>2014-05-14 01:41:23.110</td>\n","      <td>2</td>\n","      <td>643</td>\n","      <td>&lt;p&gt;In which situations would one system be pre...</td>\n","      <td>64</td>\n","      <td>2014-05-14 01:41:23.110</td>\n","      <td>What are the advantages and disadvantages of S...</td>\n","      <td>&lt;databases&gt;</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>2014-05-14 07:41:49.437</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>databases</td>\n","      <td>[databases]</td>\n","      <td>1</td>\n","      <td>databases</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>89</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>89.0</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0 days 00:00:00</td>\n","      <td>0</td>\n","      <td>21681</td>\n","      <td>Other</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;In which situations would one system be ...</td>\n","      <td>[[[&lt;p&gt;In which situations would one system be ...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;In which situations would one s...</td>\n","      <td>In which situations would one system be prefer...</td>\n","      <td>In which situations would one system be prefer...</td>\n","      <td>en</td>\n","      <td>? - ?</td>\n","      <td>In which situations would one system be prefer...</td>\n","      <td>In which situations would one system be prefer...</td>\n","      <td>In which situation would one system be prefer ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>2014-05-14 01:57:56.880</td>\n","      <td>17</td>\n","      <td>382</td>\n","      <td>&lt;p&gt;I use &lt;a href=\"http://www.csie.ntu.edu.tw/~...</td>\n","      <td>63</td>\n","      <td>2014-05-17 16:24:14.523</td>\n","      <td>Use liblinear on big data for semantic analysis</td>\n","      <td>&lt;machine-learning&gt;&lt;bigdata&gt;&lt;libsvm&gt;</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>machine-learning bigdata libsvm</td>\n","      <td>[machine-learning, bigdata, libsvm]</td>\n","      <td>3</td>\n","      <td>machine-learning</td>\n","      <td>bigdata</td>\n","      <td>libsvm</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7766</td>\n","      <td>433.0</td>\n","      <td>14.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>8213.0</td>\n","      <td>machine-learning</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>3 days 14:26:17.643000</td>\n","      <td>3</td>\n","      <td>10411</td>\n","      <td>machine-learning</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;I use &lt;a href=\"http://www.csie.ntu.edu.t...</td>\n","      <td>[[[&lt;p&gt;I use &lt;a href=\"http://www.csie.ntu.edu.t...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;I use &lt;a href=\"http://www.csie....</td>\n","      <td>I use Libsvm to train data and predict classif...</td>\n","      <td>I use Libsvm to train data and predict classif...</td>\n","      <td>en</td>\n","      <td>. - , - . , , . . ? ?</td>\n","      <td>I use Libsvm to train data and predict classif...</td>\n","      <td>I use Libsvm to train data and predict classif...</td>\n","      <td>I use Libsvm to train data and predict classif...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24348</th>\n","      <td>75142</td>\n","      <td>1</td>\n","      <td>2020-05-30 22:38:28.097</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>&lt;p&gt;I keep hearing that machine learning is jus...</td>\n","      <td>96422</td>\n","      <td>2020-05-30 22:38:28.097</td>\n","      <td>Can all known ML algorithms be written as a se...</td>\n","      <td>&lt;linear-algebra&gt;&lt;matrix&gt;</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 4.0</td>\n","      <td>linear-algebra matrix</td>\n","      <td>[linear-algebra, matrix]</td>\n","      <td>2</td>\n","      <td>linear-algebra</td>\n","      <td>matrix</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>51</td>\n","      <td>42.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>93.0</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0 days 00:00:00</td>\n","      <td>0</td>\n","      <td>20908</td>\n","      <td>Other</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;I keep hearing that machine learning is ...</td>\n","      <td>[[[&lt;p&gt;I keep hearing that machine learning is ...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;I keep hearing that machine lea...</td>\n","      <td>I keep hearing that machine learning is just l...</td>\n","      <td>I keep hearing that machine learning is just l...</td>\n","      <td>en</td>\n","      <td>. ( ? ) , , - , , , , , , ? , . . , , , ? ? ? ?</td>\n","      <td>I keep hearing that machine learning is just l...</td>\n","      <td>I keep hearing that machine learning is just l...</td>\n","      <td>I keep hear that machine learn be just linear ...</td>\n","    </tr>\n","    <tr>\n","      <th>24349</th>\n","      <td>75143</td>\n","      <td>1</td>\n","      <td>2020-05-30 22:41:01.187</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>&lt;p&gt;I am currently a few months into computer v...</td>\n","      <td>97950</td>\n","      <td>2020-05-30 22:41:01.187</td>\n","      <td>What Laptop should I use?</td>\n","      <td>&lt;gpu&gt;&lt;hardware&gt;</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 4.0</td>\n","      <td>gpu hardware</td>\n","      <td>[gpu, hardware]</td>\n","      <td>2</td>\n","      <td>gpu</td>\n","      <td>hardware</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>116</td>\n","      <td>22.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>138.0</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0 days 00:00:00</td>\n","      <td>0</td>\n","      <td>21049</td>\n","      <td>Other</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;I am currently a few months into compute...</td>\n","      <td>[[[&lt;p&gt;I am currently a few months into compute...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;I am currently a few months int...</td>\n","      <td>I am currently a few months into computer visi...</td>\n","      <td>I am currently a few months into computer visi...</td>\n","      <td>en</td>\n","      <td>/ . , . . . ? , ? . . : , € \" . . . . . . . . !</td>\n","      <td>I am currently a few months into computer visi...</td>\n","      <td>I am currently a few months into computer visi...</td>\n","      <td>I be currently a few month into computer visio...</td>\n","    </tr>\n","    <tr>\n","      <th>24350</th>\n","      <td>75144</td>\n","      <td>1</td>\n","      <td>2020-05-30 23:50:57.707</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>&lt;p&gt;Here's the code in question. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a ...</td>\n","      <td>54721</td>\n","      <td>2020-05-30 23:50:57.707</td>\n","      <td>Why does the BERT NSP head linear layer have t...</td>\n","      <td>&lt;machine-learning&gt;&lt;bert&gt;&lt;transformer&gt;</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 4.0</td>\n","      <td>machine-learning bert transformer</td>\n","      <td>[machine-learning, bert, transformer]</td>\n","      <td>3</td>\n","      <td>machine-learning</td>\n","      <td>bert</td>\n","      <td>transformer</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7766</td>\n","      <td>112.0</td>\n","      <td>76.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7954.0</td>\n","      <td>machine-learning</td>\n","      <td>Other</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>0 days 00:00:00</td>\n","      <td>0</td>\n","      <td>15034</td>\n","      <td>machine-learning</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>[[[&lt;p&gt;Here's the code in question. &lt;/p&gt;, \\n, &lt;...</td>\n","      <td>[[[&lt;p&gt;Here's the code in question. &lt;/p&gt;, \\n, &lt;...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;Here's the code in question. &lt;/...</td>\n","      <td>Here's the code in question. \\nhttps://github....</td>\n","      <td>here is the code in question. https://github.c...</td>\n","      <td>en</td>\n","      <td>. : / / . / / / / / / / _ . # ? ?</td>\n","      <td>here is the code in question. https://github.c...</td>\n","      <td>here is the code in question httpsgithubcomhug...</td>\n","      <td>here be the code in question httpsgithubcomhug...</td>\n","    </tr>\n","    <tr>\n","      <th>24351</th>\n","      <td>75145</td>\n","      <td>1</td>\n","      <td>2020-05-31 00:18:50.067</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>&lt;p&gt;Suppose there is a website and the decision...</td>\n","      <td>39982</td>\n","      <td>2020-05-31 00:35:16.450</td>\n","      <td>Should I update action value functions when th...</td>\n","      <td>&lt;reinforcement-learning&gt;&lt;markov-process&gt;</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 4.0</td>\n","      <td>reinforcement-learning markov-process</td>\n","      <td>[reinforcement-learning, markov-process]</td>\n","      <td>2</td>\n","      <td>reinforcement-learning</td>\n","      <td>markov-process</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>473</td>\n","      <td>56.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>529.0</td>\n","      <td>reinforcement-learning</td>\n","      <td>Other</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>0 days 00:16:26.383000</td>\n","      <td>0</td>\n","      <td>21044</td>\n","      <td>reinforcement-learning</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>[[[&lt;p&gt;Suppose there is a website and the decis...</td>\n","      <td>[[[&lt;p&gt;Suppose there is a website and the decis...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;Suppose there is a website and ...</td>\n","      <td>Suppose there is a website and the decision-ma...</td>\n","      <td>Suppose there is a website and the decision-ma...</td>\n","      <td>en</td>\n","      <td>- . ( , , ) . , . $ $ ? , $ _ { + } = $ - ? , ...</td>\n","      <td>Suppose there is a website and the decision-ma...</td>\n","      <td>Suppose there is a website and the decisionmak...</td>\n","      <td>Suppose there be a website and the decisionmak...</td>\n","    </tr>\n","    <tr>\n","      <th>24352</th>\n","      <td>75147</td>\n","      <td>1</td>\n","      <td>2020-05-31 01:52:17.643</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>&lt;p&gt;Im trying to do image classification using ...</td>\n","      <td>98215</td>\n","      <td>2020-05-31 01:59:43.053</td>\n","      <td>Don't know how to preprocess my dataset for im...</td>\n","      <td>&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;cnn&gt;&lt;alex-net&gt;</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","      <td>CC BY-SA 4.0</td>\n","      <td>python deep-learning keras cnn alex-net</td>\n","      <td>[python, deep-learning, keras, cnn, alex-net]</td>\n","      <td>5</td>\n","      <td>python</td>\n","      <td>deep-learning</td>\n","      <td>keras</td>\n","      <td>cnn</td>\n","      <td>alex-net</td>\n","      <td>4500</td>\n","      <td>3162.0</td>\n","      <td>2010.0</td>\n","      <td>952.0</td>\n","      <td>10.0</td>\n","      <td>10634.0</td>\n","      <td>python</td>\n","      <td>deep-learning</td>\n","      <td>keras</td>\n","      <td>cnn</td>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>0 days 00:07:25.410000</td>\n","      <td>0</td>\n","      <td>3683</td>\n","      <td>python</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>[[[&lt;p&gt;Im trying to do image classification usi...</td>\n","      <td>[[[&lt;p&gt;Im trying to do image classification usi...</td>\n","      <td>&lt;html&gt;&lt;body&gt;&lt;p&gt;Im trying to do image classific...</td>\n","      <td>Im trying to do image classification using CNN...</td>\n","      <td>I am trying to do image classification using C...</td>\n","      <td>en</td>\n","      <td>. . : / / . . . / . . ( ) : , : : ! .</td>\n","      <td>I am trying to do image classification using C...</td>\n","      <td>I am trying to do image classification using C...</td>\n","      <td>I be try to do image classification use CNN Th...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>24353 rows × 54 columns</p>\n","</div>"],"text/plain":["          Id  ...                                     BodyText_Lemma\n","0          5  ...  I have always be interest in machine learn but...\n","1          7  ...  As a researcher and instructor I be look for o...\n","2         14  ...  I be sure data science a will be discus in thi...\n","3         15  ...  In which situation would one system be prefer ...\n","4         16  ...  I use Libsvm to train data and predict classif...\n","...      ...  ...                                                ...\n","24348  75142  ...  I keep hear that machine learn be just linear ...\n","24349  75143  ...  I be currently a few month into computer visio...\n","24350  75144  ...  here be the code in question httpsgithubcomhug...\n","24351  75145  ...  Suppose there be a website and the decisionmak...\n","24352  75147  ...  I be try to do image classification use CNN Th...\n","\n","[24353 rows x 54 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"vAmDTlDdk_5P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609375994986,"user_tz":420,"elapsed":576977,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"976e5d07-f7c2-4dce-b942-180b9bb6613c"},"source":["'''\n","5g. Perform Named Entity Recognition using Spacy - warning - this process takes a long time to run\n","not very happy with results of the NER labels for data science terms; will not include as a feature in the analysis\n"," '''   \n","# Loading in corpus\n","\n","nlp = en_core_web_sm.load() \n","\n","# Testing process on one sentence\n","#doc = nlp('I have one hundred sentences that I would like to study for sentiment analysis. The language is Italian. Could you please provide a small example on how I could approach this problem?')\n","#print([(X.text, X.label_) for X in doc.ents])\n","\n","# Extracting NERs and labels to new column in pandas dataframe \n","\n","#def spacy_named_entity_wlabels(c):    \n","#  ner = nlp(c)\n","#  ner2 = [[w.text,w.label_] for w in ner.ents]\n","#  return ner2 \n","\n","#questions_df['NER'] = questions_df['BodyText_Lemma'].apply(spacy_named_entity_wlabels)\n","\n","#print(\"Before: \", textwrap.fill(questions_df['BodyText_Lemma'][4], 160))\n","#print('\\n')\n","#print(\"After: \", questions_df['NER'][4])\n","\n","# Getting separate column with just the NER text results\n","\n","#def spacy_named_entity(c):    \n","#  ner = nlp(c)\n","#  ner_text = [[w.text] for w in ner.ents]\n","#  return ner_text\n","\n","#questions_df['NER_text'] = questions_df['BodyText_Lemma'].apply(spacy_named_entity)\n","# questions_df.head()\n","\n","#print(\"Before: \", textwrap.fill(questions_df['BodyText_Lemma'][4], 160))\n","#print('\\n')\n","#print(\"After: \", questions_df['NER_text'][4])\n","\n","# Getting separate column with just the NER label results\n","\n","def spacy_label(c):    \n","  ner = nlp(c)\n","  ner_label = [[w.label_] for w in ner.ents]\n","  return ner_label\n","\n","questions_df['NER_label'] = questions_df['BodyText_Lemma'].apply(spacy_label)\n","#questions_df.head()\n","\n","print(\"Before: \", textwrap.fill(questions_df['BodyText_Lemma'][4], 160))\n","print('\\n')\n","print(\"After: \", questions_df['NER_label'][4])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before:  I use Libsvm to train data and predict classification on semantic analysis problem But it have a performance issue on largescale data because semantic analysis\n","concern ndimension problem Last year Liblinear be release and it can solve performance bottleneck But it cost too much memory Is MapReduce the only way to solve\n","semantic analysis problem on big data Or be there any other method that can improve memory bottleneck on Liblinear\n","\n","\n","After:  [['GPE'], ['DATE'], ['EVENT'], ['PERSON']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VX0MbHQ2CPQN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609376176594,"user_tz":420,"elapsed":242,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"bdabc655-cbb3-4520-cf92-a95cd1c1e592"},"source":["'''\n","5g. NER continued, further analysis\n","'''\n","questions_df.loc[questions_df['Id'] == 55166]\n","print(\"Before: \", textwrap.fill(questions_df['BodyText_Lemma'][16223], 160))\n","print('\\n')\n","print(\"After: \", questions_df['NER'][16223])\n","questions_df.loc[questions_df['Id'] == 75109]\n","print(\"Before: \", textwrap.fill(questions_df['BodyText_Lemma'][24332], 160))\n","print('\\n')\n","print(\"After: \", questions_df['NER'][24332])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before:  How Do I interpret this summary output in R Coefficients\n","\n","\n","After:  []\n","Before:  I have one hundred sentence that I would like to study for sentiment analysis The language be Italian Could you please provide a small example on how I could\n","approach this problem For example use sentence like Many thank\n","\n","\n","After:  [['one hundred', 'CARDINAL'], ['Italian', 'NORP']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XojuykrWfMbA"},"source":["'''\n","5g. NER contd; build dataframe with the text, label pair lists and tag to review and analyze further\n","And filter out cardinal labels and others that that are not needed -  future work?\n","''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdVroSLuC3QM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609377150629,"user_tz":420,"elapsed":134048,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"4df381b8-ea8b-4e90-b2d8-cf221b482fdd"},"source":["'''\n","5h. Perform POS tagging only as a way to pull out important nouns\n","'''\n","\n","def pos_text(text):\n","  is_noun = lambda pos: pos[:2] == 'NN'\n","  text_tok = nltk.word_tokenize(text)\n","  return [word for (word, pos) in nltk.pos_tag(text_tok) if is_noun(pos)]\n","\n","#test = pos_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")\n","#print(test)\n","questions_df['bodytext_nouns'] = questions_df['BodyText_Lemma'].apply(pos_text)\n","\n","print(\"Before: \", textwrap.fill(questions_df['BodyText_Lemma'][4], 160))\n","print('\\n')\n","print(\"After: \", questions_df['bodytext_nouns'][4])\n","#for word,word_class in questions_df['bodytext_pos'][4]:\n","#   print(word + \",\" + word_class)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before:  I use Libsvm to train data and predict classification on semantic analysis problem But it have a performance issue on largescale data because semantic analysis\n","concern ndimension problem Last year Liblinear be release and it can solve performance bottleneck But it cost too much memory Is MapReduce the only way to solve\n","semantic analysis problem on big data Or be there any other method that can improve memory bottleneck on Liblinear\n","\n","\n","After:  ['data', 'classification', 'analysis', 'problem', 'performance', 'issue', 'data', 'analysis', 'concern', 'ndimension', 'problem', 'year', 'release', 'performance', 'bottleneck', 'memory', 'MapReduce', 'way', 'analysis', 'problem', 'data', 'method', 'memory', 'bottleneck', 'Liblinear']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MY9eUq27BXfP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609377199049,"user_tz":420,"elapsed":286,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"08d3d2b8-f30e-45e4-a722-13846e9f74e7"},"source":["'''\n","5i. Convert to lowercase; do this for bodytext nouns and NER also(?)\n","'''\n","questions_df['bodytext_lc'] = questions_df['BodyText_Lemma'].str.lower()\n","\n","print(\"Before: \", textwrap.fill(questions_df['BodyText_Lemma'][6], 160))\n","print('\\n')\n","print(\"After: \", textwrap.fill(questions_df['bodytext_lc'][6], 160))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before:  We create a social network application for eLearning purpose it be an experimental project that we be research on in our lab It have be use in some case study\n","for a while and the data in our relational DBMS SQL Server 2008 be get big it be a few gigabyte now and the table be highly connect to each other The\n","performance be still fine but when should we consider other option Is it the matter of performance\n","\n","\n","After:  we create a social network application for elearning purpose it be an experimental project that we be research on in our lab it have be use in some case study\n","for a while and the data in our relational dbms sql server 2008 be get big it be a few gigabyte now and the table be highly connect to each other the\n","performance be still fine but when should we consider other option is it the matter of performance\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7nzUy_AqBSCq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609377477816,"user_tz":420,"elapsed":717,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"79ddbaf2-6531-4497-a37e-59fe7f3c11e5"},"source":["'''\n","5j. Remove stop words - using nltk corpus - let's take a look at those english stopwords first\n","I am removing re since there is a module re that is important; also \"r\" in case it is in the stopwords, since this is a language\n","'''\n","#print(stopwords.words('english'))\n","stop_words = set(stopwords.words('english'))\n","exclude_words = set(('re', \"r\", 'q'))\n","new_stop_words = stop_words - exclude_words\n","\n","questions_df['BodyText_NoStopwords'] = questions_df['bodytext_lc'].apply(lambda x: ' '.join([word for word in x.split() if word not in (new_stop_words)]))\n","print(\"Before LC Conversion: \")\n","print(textwrap.fill(questions_df['BodyText_Lemma'][6], 160))\n","print('\\n')\n","print(\"Before StopWord Removal: \")\n","print(textwrap.fill(questions_df['bodytext_lc'][6], 160))\n","print('\\n')\n","print(\"After: \", textwrap.fill(questions_df['BodyText_NoStopwords'][6], 160))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Before LC Conversion: \n","We create a social network application for eLearning purpose it be an experimental project that we be research on in our lab It have be use in some case study\n","for a while and the data in our relational DBMS SQL Server 2008 be get big it be a few gigabyte now and the table be highly connect to each other The\n","performance be still fine but when should we consider other option Is it the matter of performance\n","\n","\n","Before StopWord Removal: \n","we create a social network application for elearning purpose it be an experimental project that we be research on in our lab it have be use in some case study\n","for a while and the data in our relational dbms sql server 2008 be get big it be a few gigabyte now and the table be highly connect to each other the\n","performance be still fine but when should we consider other option is it the matter of performance\n","\n","\n","After:  create social network application elearning purpose experimental project research lab use case study data relational dbms sql server 2008 get big gigabyte table\n","highly connect performance still fine consider option matter performance\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G2MMu60avNre"},"source":["# Task 6 Export Results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHuTYw03YMUA","executionInfo":{"status":"ok","timestamp":1609377579038,"user_tz":420,"elapsed":359,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"d619fb1a-c1f0-4c4d-a14b-f7e49014f76f"},"source":["questions_df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 24353 entries, 0 to 24352\n","Data columns (total 63 columns):\n"," #   Column                Non-Null Count  Dtype          \n","---  ------                --------------  -----          \n"," 0   Id                    24353 non-null  int64          \n"," 1   PostTypeId            24353 non-null  int64          \n"," 2   CreationDate          24353 non-null  datetime64[ns] \n"," 3   Score                 24353 non-null  int64          \n"," 4   ViewCount             24353 non-null  int64          \n"," 5   Body                  24353 non-null  object         \n"," 6   OwnerUserId           24238 non-null  object         \n"," 7   LastActivityDate      24353 non-null  datetime64[ns] \n"," 8   Title                 24353 non-null  object         \n"," 9   Tags                  24353 non-null  object         \n"," 10  AnswerCount           24353 non-null  int64          \n"," 11  CommentCount          24353 non-null  int64          \n"," 12  FavoriteCount         6708 non-null   object         \n"," 13  ClosedDate            1416 non-null   datetime64[ns] \n"," 14  ContentLicense        24353 non-null  object         \n"," 15  Tags_SpaceDelimited   24353 non-null  object         \n"," 16  Tags_Clean            24353 non-null  object         \n"," 17  TagCount              24353 non-null  int64          \n"," 18  Tag1                  24353 non-null  object         \n"," 19  Tag2                  21064 non-null  object         \n"," 20  Tag3                  15037 non-null  object         \n"," 21  Tag4                  8302 non-null   object         \n"," 22  Tag5                  3687 non-null   object         \n"," 23  Tag1_Freq             24353 non-null  int64          \n"," 24  Tag2_Freq             21064 non-null  float64        \n"," 25  Tag3_Freq             15037 non-null  float64        \n"," 26  Tag4_Freq             8302 non-null   float64        \n"," 27  Tag5_Freq             3687 non-null   float64        \n"," 28  Total_Tag_Freqency    24353 non-null  float64        \n"," 29  Tag1_Renamed          24353 non-null  object         \n"," 30  Tag2_Renamed          24353 non-null  object         \n"," 31  Tag3_Renamed          24353 non-null  object         \n"," 32  Tag4_Renamed          24353 non-null  object         \n"," 33  Tag5_Renamed          24353 non-null  object         \n"," 34  TopTag                24353 non-null  category       \n"," 35  Elapsed_Time          24353 non-null  timedelta64[ns]\n"," 36  Elapsed_Time_Int      24353 non-null  int16          \n"," 37  rank                  24353 non-null  int64          \n"," 38  Tag1_Renamed2         24353 non-null  object         \n"," 39  TopTag_Revised        24353 non-null  int64          \n"," 40  NumQuestions          24353 non-null  int64          \n"," 41  BodyBoldCount         24353 non-null  int64          \n"," 42  ParagraphCount        24353 non-null  int64          \n"," 43  CodeCount             24353 non-null  int64          \n"," 44  Soup                  24353 non-null  object         \n"," 45  NoCode                24353 non-null  object         \n"," 46  NoCodeString          24353 non-null  object         \n"," 47  BodyText              24353 non-null  object         \n"," 48  bodytext_expanded     24353 non-null  object         \n"," 49  lang                  24353 non-null  object         \n"," 50  punct_only            24353 non-null  object         \n"," 51  bodytext_noaccents    24353 non-null  object         \n"," 52  bodytext_nospch       24353 non-null  object         \n"," 53  temp                  24353 non-null  object         \n"," 54  temp2                 24353 non-null  object         \n"," 55  temp3                 24353 non-null  object         \n"," 56  BodyText_Lemma        24353 non-null  object         \n"," 57  NER                   24353 non-null  object         \n"," 58  NER_text              24353 non-null  object         \n"," 59  NER_label             24353 non-null  object         \n"," 60  bodytext_nouns        24353 non-null  object         \n"," 61  bodytext_lc           24353 non-null  object         \n"," 62  BodyText_NoStopwords  24353 non-null  object         \n","dtypes: category(1), datetime64[ns](3), float64(5), int16(1), int64(14), object(38), timedelta64[ns](1)\n","memory usage: 12.2+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zsJb0nxS_viB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609377643893,"user_tz":420,"elapsed":8716,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"115602b4-79f5-446a-a060-1d350f533c97"},"source":["'''\n","6. We have cleaned the body text and added features - export for further analysis.\n","Now write out for safekeeping\n","'''\n","import sys\n","#print(sys.getrecursionlimit())\n","sys.setrecursionlimit(2000)\n","print(sys.getrecursionlimit())\n","pickle_out = open(\"/content/drive/My Drive/Capstone2/Data/questions_df_ner_results_10282020.pickle\",\"wb\")\n","pickle.dump(questions_df, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n","pickle_out.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2000\n"],"name":"stdout"}]}]}