{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CP2_08_BernoulliNaiveBayes.ipynb","provenance":[{"file_id":"1sGdM-54Pbq6EOWq30kiQ1AgQOi11Hx3X","timestamp":1617507823317},{"file_id":"11Wcj6NclP2KX26XYfFgEudr0S0bFEp9A","timestamp":1617497432616},{"file_id":"1t9YTFMUahjeBWsyCculou_Pynkh2InG6","timestamp":1602256804263},{"file_id":"12PjhOUiDi-N0FXtlN7AFXyyGAiYi7AWj","timestamp":1601078329171},{"file_id":"1U1u9m5zkvqaI5GElpVlY4tU7zTNShmq9","timestamp":1599859230341},{"file_id":"1JbxiYiHD6fO2SZ9yLDoyqe9IkvHwJxp8","timestamp":1599777923777}],"collapsed_sections":["hMm7TNpqTCwM","JKLAl0zlukPE","-iPLi5m886bu","fCmKiWgw9GiF","qkAIQnqGU1qd","2xDBbNOou7zF","MzSC5VNF2p89"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vldA80BR2gik","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621712935619,"user_tz":360,"elapsed":24499,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"5d13f9f9-8254-49cb-d11c-18153522f211"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BMCdDFsXr5rR"},"source":["# PseudoCode and Task List"]},{"cell_type":"markdown","metadata":{"id":"kxaQxoftDTp3"},"source":["1. Load the pickled pandas dataframe from 04 notebook and check file contents\n","2. Prep the dataset for analysis\n",">2a. Factorize Tags column to a numeric column\n",">2b. Split into dev, cv, and test sets\n","3. Second of 3 different models (Bernoulli Naive Bayes) run each with the 3 diff optimized vectorizors - evaluate with AUC scores on the val set of each transformation - \n",">3.1 Count Vectorization 3.2 TFIDF 3.3 Doc2Vec\n","4. TFIDF / Count Vect give about the same auc scores on the cv set as random forest but all models are less overfitted and model runs much faster.\n"]},{"cell_type":"markdown","metadata":{"id":"gJ3VoPrnsIkz"},"source":["# Tasks 1 Load file and examine contents"]},{"cell_type":"code","metadata":{"id":"vt8U0qUv28dY"},"source":["'''\n","1a Import all modules that are needed\n","'''\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import collections\n","import re\n","import nltk\n","import itertools\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import LeaveOneOut \n","from sklearn import metrics\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.parsing.preprocessing import preprocess_documents\n","from gensim.parsing.preprocessing import preprocess_string\n","from prettytable import PrettyTable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIOI0tDD3Ke7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621712957016,"user_tz":360,"elapsed":2241,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"8886f945-9619-4f48-9b45-3cac578c826e"},"source":["'''\n","1b Load file \n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 24330 entries, 0 to 24352\n","Data columns (total 3 columns):\n"," #   Column          Non-Null Count  Dtype \n","---  ------          --------------  ----- \n"," 0   Id              24330 non-null  int64 \n"," 1   Tag             24330 non-null  object\n"," 2   BodyText_Clean  24330 non-null  object\n","dtypes: int64(1), object(2)\n","memory usage: 760.3+ KB\n","'''\n","\n","questions_df_clean = pd.read_pickle('/content/drive/My Drive/Capstone2/Data/questions_df_clean_11052020.pickle')\n","#questions_df_clean = pd.read_pickle('/content/drive/MyDrive/Data Science/Laura_CP2/Copy of questions_df_clean_11052020.pickle')\n","questions_df_clean.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 24330 entries, 0 to 24352\n","Data columns (total 3 columns):\n"," #   Column          Non-Null Count  Dtype \n","---  ------          --------------  ----- \n"," 0   Id              24330 non-null  int64 \n"," 1   Tag             24330 non-null  object\n"," 2   BodyText_Clean  24330 non-null  object\n","dtypes: int64(1), object(2)\n","memory usage: 760.3+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rYJBGuegELBW","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"ok","timestamp":1621712960078,"user_tz":360,"elapsed":236,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"febdd6ef-f9d6-4174-c4a3-52195f464e8c"},"source":["'''\n","1c. Examine contents\n","'''\n","questions_df_clean.head()\n","#questions_df_clean.loc[questions_df_clean['BodyText_Clean'].isnull()]\n","#questions_df_clean.loc[questions_df_clean['Tag'].isnull()]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Tag</th>\n","      <th>BodyText_Clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>machine-learning</td>\n","      <td>always interest machine learn figure one thing...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>Other</td>\n","      <td>researcher instructor look opensource book sim...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>14</td>\n","      <td>data-mining</td>\n","      <td>sure data science discus forum several synonym...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15</td>\n","      <td>Other</td>\n","      <td>situation would one system prefer relative adv...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16</td>\n","      <td>machine-learning</td>\n","      <td>use libsvm train data predict classification s...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id               Tag                                     BodyText_Clean\n","0   5  machine-learning  always interest machine learn figure one thing...\n","1   7             Other  researcher instructor look opensource book sim...\n","2  14       data-mining  sure data science discus forum several synonym...\n","3  15             Other  situation would one system prefer relative adv...\n","4  16  machine-learning  use libsvm train data predict classification s..."]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Uxa_Yv16OpYL"},"source":["# Task 2 - Prep for modelling\n",">2a. Factorize Tags column to numeric column\n",">2b. Split into dev, cv, and test sets\n",">2c. Verify the distribution of tags within the splits"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEig4gYOOx-n","executionInfo":{"status":"ok","timestamp":1621712963854,"user_tz":360,"elapsed":231,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"3d8d9509-4af8-4533-c7eb-8dd971471ae9"},"source":["''' \n","2a. Factorize Tags Column to numeric\n","Converting tag column (our target variable) to a numeric column for modelling; \n","originally converted to separate columns for each tag\n","that process returned 1(yes) or 0(no) for each tag name in the original Tag column\n","multiple y target variables are represented by these multi-labelled columns\n","returning a list of all these multi-label target columns\n","That initial process did not work well using the below code\n","tag_names = questions_df_clean['Tag'].unique().tolist()\n","#print(tag_names)\n","tag_dummy = pd.get_dummies(questions_df_clean['Tag'], prefix = 'Tag')\n","quest_df_dummies = pd.concat([questions_df_clean, tag_dummy], axis = 1)\n","quest_df_dummies.drop(columns='Tag', inplace=True)\n","quest_df_dummies.info()\n","Using factorize code instead suggested by Ajith\n","'''\n","questions_df_factorized = questions_df_clean.copy()\n","#Creating the dependent variable class\n","factor = pd.factorize(questions_df_factorized['Tag'])\n","questions_df_factorized.Tag = factor[0]\n","definitions = factor[1]\n","print(questions_df_factorized.head())\n","print(questions_df_clean.head())\n","print(definitions)\n","print(factor)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   Id  Tag                                     BodyText_Clean\n","0   5    0  always interest machine learn figure one thing...\n","1   7    1  researcher instructor look opensource book sim...\n","2  14    2  sure data science discus forum several synonym...\n","3  15    1  situation would one system prefer relative adv...\n","4  16    0  use libsvm train data predict classification s...\n","   Id               Tag                                     BodyText_Clean\n","0   5  machine-learning  always interest machine learn figure one thing...\n","1   7             Other  researcher instructor look opensource book sim...\n","2  14       data-mining  sure data science discus forum several synonym...\n","3  15             Other  situation would one system prefer relative adv...\n","4  16  machine-learning  use libsvm train data predict classification s...\n","Index(['machine-learning', 'Other', 'data-mining', 'bigdata', 'r',\n","       'statistics', 'clustering', 'recommender-system', 'nlp',\n","       'feature-selection', 'neural-network', 'python', 'classification',\n","       'visualization', 'apache-spark', 'dataset', 'text-mining', 'pandas',\n","       'time-series', 'regression', 'random-forest', 'predictive-modeling',\n","       'scikit-learn', 'deep-learning', 'decision-trees', 'linear-regression',\n","       'data', 'xgboost', 'tensorflow', 'reinforcement-learning', 'orange',\n","       'keras', 'cnn', 'lstm'],\n","      dtype='object')\n","(array([ 0,  1,  2, ...,  0, 29, 11]), Index(['machine-learning', 'Other', 'data-mining', 'bigdata', 'r',\n","       'statistics', 'clustering', 'recommender-system', 'nlp',\n","       'feature-selection', 'neural-network', 'python', 'classification',\n","       'visualization', 'apache-spark', 'dataset', 'text-mining', 'pandas',\n","       'time-series', 'regression', 'random-forest', 'predictive-modeling',\n","       'scikit-learn', 'deep-learning', 'decision-trees', 'linear-regression',\n","       'data', 'xgboost', 'tensorflow', 'reinforcement-learning', 'orange',\n","       'keras', 'cnn', 'lstm'],\n","      dtype='object'))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YVJfJnIRcQKz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621712968816,"user_tz":360,"elapsed":215,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"3a63ed3e-8913-4342-8dcd-1e1af46502db"},"source":["'''\n","2b. Split into train (70%) / test (30%). Use the train data and further split into train/val split (similar ratio). \n","Leaving the test split to the end.\n","Splitting into target (y) and predictor (X) variable sets and then into \n","test and train sets and using stratification, given that the tag distribution is imbalanced\n","Experimented with various means to deal with stratification and multi-label classification\n","and decided the standard scikit learn module code works better\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","mlb = MultiLabelBinarizer()\n","X=df[list('ABCD')]\n","Y=pd.DataFrame(mlb.fit_transform(df[['sex','weight']].values), columns=mlb.classes_, index=df.index)\n","\n","!pip install scikit-multilearn\n","from skmultilearn.model_selection import iterative_stratification\n","X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = 0.30)\n","\n","'''\n","# Splitting X and y variables\n","X=questions_df_factorized[list(questions_df_factorized.columns)[2]]\n","y=questions_df_factorized[list(questions_df_factorized.columns)[1]]\n","print(X.shape)\n","print(y.shape)\n","# Splitting into train, test\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, stratify=y,random_state=42)\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)\n","# Further splitting train into dev and validation\n","X_dev, X_cv, y_dev, y_cv = train_test_split(X_train,y_train,test_size = 0.30,stratify=y_train,random_state=42)\n","print(X_dev.shape, y_dev.shape)\n","print(X_cv.shape, y_cv.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(24330,)\n","(24330,)\n","(17031,) (17031,)\n","(7299,) (7299,)\n","(11921,) (11921,)\n","(5110,) (5110,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_7T0Gmd0yBDT","executionInfo":{"status":"ok","timestamp":1621712972258,"user_tz":360,"elapsed":218,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"db600b50-bb0c-40f8-e93f-b6eff82d7dce"},"source":["print(type(y_dev))\n","y_dev.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.series.Series'>\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"NKZDPGl7DdLb","executionInfo":{"status":"ok","timestamp":1621712973988,"user_tz":360,"elapsed":226,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"648a07e7-8724-4c54-fab6-2fd77e3f904e"},"source":["X_dev = pd.DataFrame(X_dev)\n","X_cv = pd.DataFrame(X_cv)\n","X_test = pd.DataFrame(X_test)\n","X_train = pd.DataFrame(X_train)\n","X_dev.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BodyText_Clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8286</th>\n","      <td>seems like thing httpswwwsciencedirectcomscien...</td>\n","    </tr>\n","    <tr>\n","      <th>23415</th>\n","      <td>build 2hidden layer mlp use keras use scikit l...</td>\n","    </tr>\n","    <tr>\n","      <th>23360</th>\n","      <td>write fast rcnn run problem back propagation g...</td>\n","    </tr>\n","    <tr>\n","      <th>1698</th>\n","      <td>hear multilayer perceptron approximate functio...</td>\n","    </tr>\n","    <tr>\n","      <th>24085</th>\n","      <td>try correlation analysis dataset data cleanse ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          BodyText_Clean\n","8286   seems like thing httpswwwsciencedirectcomscien...\n","23415  build 2hidden layer mlp use keras use scikit l...\n","23360  write fast rcnn run problem back propagation g...\n","1698   hear multilayer perceptron approximate functio...\n","24085  try correlation analysis dataset data cleanse ..."]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"AvYWS9eBDQpV"},"source":["X_dev['BodyText_Clean'] = X_dev['BodyText_Clean'].apply(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))\n","X_cv['BodyText_Clean'] = X_cv['BodyText_Clean'].apply(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))\n","X_test['BodyText_Clean'] = X_test['BodyText_Clean'].apply(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))\n","X_train['BodyText_Clean'] = X_train['BodyText_Clean'].apply(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-iPLi5m886bu"},"source":["# Task 3 - Build second of 3 different models (Bernoulli Naive Bayes) and run with the 3 diff transformations -  measure the accuracy on the val set for each \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"vzQwLpY0sZ5l","executionInfo":{"status":"ok","timestamp":1621713917743,"user_tz":360,"elapsed":1839,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"cddc5cae-3bfc-4958-a435-1756c167cc16"},"source":["'''\n","3.1a. Running count vectorizer on dev and cv sets with optimal params\n","'''\n","cnt_vect = CountVectorizer(min_df=.005, max_df=0.99, ngram_range=(1,1))\n","X_dev_cntvect_df = pd.DataFrame(cnt_vect.fit_transform(X_dev.BodyText_Clean).toarray(), index=X_dev.index, columns=cnt_vect.get_feature_names())\n","print(X_dev_cntvect_df.shape)\n","X_dev_cntvect_df.head()\n","X_cv_cntvect = cnt_vect.transform(X_cv.BodyText_Clean)\n","\n","# Convert cv sets using the same transformation\n","\n","X_cv_cntvect_df = pd.DataFrame(cnt_vect.transform(X_cv.BodyText_Clean).toarray(), index = X_cv.index, columns = cnt_vect.get_feature_names())\n","X_cv_cntvect_df.head()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(11921, 1231)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1d</th>\n","      <th>2d</th>\n","      <th>2nd</th>\n","      <th>3d</th>\n","      <th>able</th>\n","      <th>absolute</th>\n","      <th>accept</th>\n","      <th>access</th>\n","      <th>accomplish</th>\n","      <th>accord</th>\n","      <th>according</th>\n","      <th>account</th>\n","      <th>accuracy</th>\n","      <th>accurate</th>\n","      <th>achieve</th>\n","      <th>across</th>\n","      <th>action</th>\n","      <th>activation</th>\n","      <th>activity</th>\n","      <th>actual</th>\n","      <th>actually</th>\n","      <th>adam</th>\n","      <th>add</th>\n","      <th>addition</th>\n","      <th>additional</th>\n","      <th>address</th>\n","      <th>adjust</th>\n","      <th>advance</th>\n","      <th>advantage</th>\n","      <th>advice</th>\n","      <th>advise</th>\n","      <th>affect</th>\n","      <th>age</th>\n","      <th>agent</th>\n","      <th>aggregate</th>\n","      <th>ai</th>\n","      <th>aim</th>\n","      <th>al</th>\n","      <th>algorithm</th>\n","      <th>allow</th>\n","      <th>...</th>\n","      <th>visualization</th>\n","      <th>visualize</th>\n","      <th>want</th>\n","      <th>way</th>\n","      <th>web</th>\n","      <th>website</th>\n","      <th>week</th>\n","      <th>weight</th>\n","      <th>weird</th>\n","      <th>welcome</th>\n","      <th>well</th>\n","      <th>whereas</th>\n","      <th>whether</th>\n","      <th>whole</th>\n","      <th>whose</th>\n","      <th>width</th>\n","      <th>wikipedia</th>\n","      <th>win</th>\n","      <th>window</th>\n","      <th>wish</th>\n","      <th>within</th>\n","      <th>without</th>\n","      <th>wonder</th>\n","      <th>word</th>\n","      <th>word2vec</th>\n","      <th>work</th>\n","      <th>world</th>\n","      <th>worth</th>\n","      <th>would</th>\n","      <th>write</th>\n","      <th>wrong</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>xgboost</th>\n","      <th>xi</th>\n","      <th>year</th>\n","      <th>yes</th>\n","      <th>yet</th>\n","      <th>yield</th>\n","      <th>zero</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>20034</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22608</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12975</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>614</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22615</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1231 columns</p>\n","</div>"],"text/plain":["       1d  2d  2nd  3d  able  absolute  ...  xi  year  yes  yet  yield  zero\n","20034   0   0    0   0     0         0  ...   0     0    0    0      0     0\n","22608   0   0    0   0     0         0  ...   0     0    0    0      0     0\n","12975   0   0    0   0     2         0  ...   0     0    0    0      0     0\n","614     0   0    0   0     0         0  ...   0     0    0    0      0     0\n","22615   0   0    0   0     0         0  ...   0     0    0    0      0     0\n","\n","[5 rows x 1231 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9_3pYd-vgD3","executionInfo":{"status":"ok","timestamp":1621713935234,"user_tz":360,"elapsed":4427,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"501a66ad-3c35-4ffb-e04f-20fd6f304e86"},"source":["'''\n","3.1b. Running Bernoulli NB model\n","'''\n","bnb_mod_cv_fit = BernoulliNB()\n","bnb_mod_cv_fit.fit(X_dev_cntvect_df,y_dev)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Nx7peeYs2iD","executionInfo":{"status":"ok","timestamp":1621713943992,"user_tz":360,"elapsed":2216,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"2f2045c3-3569-4482-bcdc-a7b2403ad73e"},"source":["'''\n","3.1c Print scores - AUC score for both dev and cv sets; cv score is about the same as random forest but and less overfitting\n","'''\n","y_cntvect_prob_dev_bnb = bnb_mod_cv_fit.predict_proba(X_dev_cntvect_df)\n","roc_auc_cntvect_dev_bnb = roc_auc_score(y_dev, y_cntvect_prob_dev_bnb, multi_class=\"ovo\",\n","                                  average=\"macro\")\n","\n","print(\"CntVect RF Train AUC Score:\", round(roc_auc_cntvect_dev_bnb,4))\n","\n","y_cntvect_prob_val_bnb = bnb_mod_cv_fit.predict_proba(X_cv_cntvect_df)\n","roc_auc_cntvect_val_bnb = roc_auc_score(y_cv, y_cntvect_prob_val_bnb, multi_class=\"ovo\",\n","                                  average=\"macro\")\n","print(\"CntVect RF Val AUC Score:  \", round(roc_auc_cntvect_val_bnb,4))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["CntVect RF Train AUC Score: 0.9356\n","CntVect RF Val AUC Score:   0.8323\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"MXoFQ3hXtcEk","executionInfo":{"status":"ok","timestamp":1621713987420,"user_tz":360,"elapsed":1763,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"e4fcc342-4602-432b-c562-66664065b354"},"source":["'''\n","3.2a Run tfidf with optimal params\n","'''\n","tfidf_vect = TfidfVectorizer(min_df=.001, max_df=0.999, ngram_range=(1,1))\n","X_dev_tfidf_df = pd.DataFrame(tfidf_vect.fit_transform(X_dev.BodyText_Clean).toarray(), index=X_dev.index, columns=tfidf_vect.get_feature_names())\n","print(X_dev_tfidf_df.shape)\n","X_dev_tfidf_df.head()\n","X_cv_tfidf = tfidf_vect.transform(X_cv.BodyText_Clean)\n","\n","# Convert cv sets using the same transformation\n","\n","X_cv_tfidf_df = pd.DataFrame(tfidf_vect.transform(X_cv.BodyText_Clean).toarray(), index = X_cv.index, columns = tfidf_vect.get_feature_names())\n","X_cv_tfidf_df.head()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["(11921, 3144)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>100k</th>\n","      <th>10fold</th>\n","      <th>10k</th>\n","      <th>1d</th>\n","      <th>1m</th>\n","      <th>1st</th>\n","      <th>1x1</th>\n","      <th>20k</th>\n","      <th>2d</th>\n","      <th>2nd</th>\n","      <th>2x2</th>\n","      <th>30k</th>\n","      <th>3d</th>\n","      <th>3rd</th>\n","      <th>3x3</th>\n","      <th>4th</th>\n","      <th>500k</th>\n","      <th>50k</th>\n","      <th>5fold</th>\n","      <th>5k</th>\n","      <th>5th</th>\n","      <th>5x5</th>\n","      <th>8gb</th>\n","      <th>a1</th>\n","      <th>a2</th>\n","      <th>ab</th>\n","      <th>abbreviation</th>\n","      <th>abc</th>\n","      <th>ability</th>\n","      <th>able</th>\n","      <th>abnormal</th>\n","      <th>absolute</th>\n","      <th>absolutely</th>\n","      <th>abstract</th>\n","      <th>academic</th>\n","      <th>acc</th>\n","      <th>accelerate</th>\n","      <th>acceleration</th>\n","      <th>accelerometer</th>\n","      <th>accept</th>\n","      <th>...</th>\n","      <th>wt</th>\n","      <th>x0</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>x3</th>\n","      <th>x4</th>\n","      <th>xaxis</th>\n","      <th>xgboost</th>\n","      <th>xi</th>\n","      <th>xml</th>\n","      <th>xn</th>\n","      <th>xor</th>\n","      <th>xt</th>\n","      <th>xtest</th>\n","      <th>xtrain</th>\n","      <th>xy</th>\n","      <th>xyz</th>\n","      <th>y0</th>\n","      <th>y1</th>\n","      <th>y2</th>\n","      <th>yaxis</th>\n","      <th>year</th>\n","      <th>yellow</th>\n","      <th>yes</th>\n","      <th>yesno</th>\n","      <th>yet</th>\n","      <th>yi</th>\n","      <th>yield</th>\n","      <th>yolo</th>\n","      <th>york</th>\n","      <th>youtube</th>\n","      <th>ypred</th>\n","      <th>yt</th>\n","      <th>ytest</th>\n","      <th>ytrain</th>\n","      <th>ytrue</th>\n","      <th>zero</th>\n","      <th>zip</th>\n","      <th>zoom</th>\n","      <th>zscore</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>20034</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>22608</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>12975</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.155439</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.151038</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>614</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>22615</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 3144 columns</p>\n","</div>"],"text/plain":["       100k  10fold  10k   1d   1m  ...  ytrue  zero       zip  zoom  zscore\n","20034   0.0     0.0  0.0  0.0  0.0  ...    0.0   0.0  0.000000   0.0     0.0\n","22608   0.0     0.0  0.0  0.0  0.0  ...    0.0   0.0  0.000000   0.0     0.0\n","12975   0.0     0.0  0.0  0.0  0.0  ...    0.0   0.0  0.151038   0.0     0.0\n","614     0.0     0.0  0.0  0.0  0.0  ...    0.0   0.0  0.000000   0.0     0.0\n","22615   0.0     0.0  0.0  0.0  0.0  ...    0.0   0.0  0.000000   0.0     0.0\n","\n","[5 rows x 3144 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fln49R0svb9J","executionInfo":{"status":"ok","timestamp":1621714001349,"user_tz":360,"elapsed":649,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"5d0f2519-dad2-4f18-b0aa-29780a2616f2"},"source":["'''\n","3.2b. Running Bernoulli NB model\n","'''\n","bnb_mod_tfidf_fit = BernoulliNB()\n","bnb_mod_tfidf_fit.fit(X_dev_tfidf_df,y_dev)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-UQ6c3rxnpS","executionInfo":{"status":"ok","timestamp":1621714006120,"user_tz":360,"elapsed":1997,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"d52db62f-ff1c-48f9-987b-669157bdeb1f"},"source":["'''\n","3.2c. Print scores - AUC score for both dev and cv sets; cv score is about the same as random forest but and less overfitting\n","'''\n","y_tfidf_prob_dev_bnb = bnb_mod_tfidf_fit.predict_proba(X_dev_tfidf_df)\n","roc_auc_tfidf_dev_bnb = roc_auc_score(y_dev, y_tfidf_prob_dev_bnb, multi_class=\"ovo\",\n","                                  average=\"macro\")\n","\n","print(\"TFIDF BNB Train AUC Score:\", round(roc_auc_tfidf_dev_bnb,4))\n","\n","y_tfidf_prob_val_bnb = bnb_mod_tfidf_fit.predict_proba(X_cv_tfidf)\n","roc_auc_tfidf_val_bnb = roc_auc_score(y_cv, y_tfidf_prob_val_bnb, multi_class=\"ovo\",\n","                                  average=\"macro\")\n","print(\"TFIDF BNB Val AUC Score:  \", round(roc_auc_tfidf_val_bnb,4))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["TFIDF BNB Train AUC Score: 0.9056\n","TFIDF BNB Val AUC Score:   0.7832\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsQOCZ8d7h_O","executionInfo":{"status":"ok","timestamp":1621714417140,"user_tz":360,"elapsed":66650,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"7131acf2-df83-4446-ec47-b2cd48357c0e"},"source":["'''\n","3.3a. Loading hypertuned doc2vec model from previous notebook and re-creating the tagged docs, then evaluate\n","'''\n","fnl_d2v_model = Doc2Vec.load(\"/content/drive/My Drive/Capstone2/Data/final_d2v_500.model\")\n","\n","# Build separate dataframes with the dev, cv, and test \n","reversefactor = dict(zip(range(34),definitions))\n","\n","y_dev_rf = np.vectorize(reversefactor.get)(y_dev)\n","y_cv_rf = np.vectorize(reversefactor.get)(y_cv)\n","y_test_rf = np.vectorize(reversefactor.get)(y_test)\n","\n","d2v_dev_df = pd.DataFrame({'y': y_dev_rf, 'X': X_dev.BodyText_Clean})\n","d2v_cv_df = pd.DataFrame({'y': y_cv_rf, 'X': X_cv.BodyText_Clean})\n","d2v_test_df = pd.DataFrame({'y': y_test_rf, 'X': X_test.BodyText_Clean})\n","#d2v_dev_df.head()\n","\n","# Ensure they're all string datatype\n","d2v_dev_df['X'] = d2v_dev_df['X'].astype(str)\n","d2v_cv_df['X'] = d2v_cv_df['X'].astype(str)\n","d2v_test_df['X'] = d2v_test_df['X'].astype(str)\n","d2v_dev_df.head()\n","\n","class TaggedDocumentIterator(object):\n","    def __init__(self, doc_list, labels_list):\n","        self.labels_list = labels_list\n","        self.doc_list = doc_list\n","    def __iter__(self):\n","        for idx, doc in enumerate(self.doc_list):\n","            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])\n"," \n","docLabels_dev = list(d2v_dev_df['y'])\n","data_dev = list(d2v_dev_df['X'])\n","tagged_docs_dev = TaggedDocumentIterator(data_dev, docLabels_dev)\n","\n","docLabels_cv = list(d2v_cv_df['y'])\n","data_cv = list(d2v_cv_df['X'])\n","tagged_docs_cv = TaggedDocumentIterator(data_cv, docLabels_cv)\n","\n","docLabels_test = list(d2v_test_df['y'])\n","data_test = list(d2v_test_df['X'])\n","tagged_docs_test = TaggedDocumentIterator(data_test, docLabels_test)\n","\n","type(tagged_docs_dev)\n","d2v_dev_df.info()\n","\n","fnl_dev_targets, fnl_dev_regressors = zip(*[(doc.tags[0], fnl_d2v_model.infer_vector(doc.words, steps=20)) for doc in tagged_docs_dev])\n","fnl_cv_targets, fnl_cv_regressors = zip(*[(doc.tags[0], fnl_d2v_model.infer_vector(doc.words, steps=20)) for doc in tagged_docs_cv])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 11921 entries, 8286 to 3207\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   y       11921 non-null  object\n"," 1   X       11921 non-null  object\n","dtypes: object(2)\n","memory usage: 279.4+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7CO14MnTzBSU","executionInfo":{"status":"ok","timestamp":1621714423381,"user_tz":360,"elapsed":612,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}}},"source":["'''\n","3.3b. Run Running Bernoulli NB model to evaluate the success of doc2vec transform on cv set\n","'''\n","bnb_mod_d2v_fit = BernoulliNB()\n","bnb_mod_d2v_fit.fit(fnl_dev_regressors, fnl_dev_targets)\n","\n","fnl_cv_targets_pred_d2v = bnb_mod_d2v_fit.predict(fnl_cv_regressors)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dySjQ0KzV7d","executionInfo":{"status":"ok","timestamp":1621714428188,"user_tz":360,"elapsed":1744,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"6f4d4020-686b-4689-a09f-41791b0a4333"},"source":["#Print scores - not as high a score as RF model but less overfitting\n","\n","fnl_dev_targets_prob_d2v_bnb = bnb_mod_d2v_fit.predict_proba(fnl_dev_regressors)\n","roc_auc_d2v_dev_bnb = roc_auc_score(fnl_dev_targets,fnl_dev_targets_prob_d2v_bnb, multi_class=\"ovo\",\n","                                  average=\"macro\")\n","\n","print(\"Doc2Vec BNB Train AUC Score:\", round(roc_auc_d2v_dev_bnb,4))\n","\n","fnl_cv_targets_prob_d2v_bnb = bnb_mod_d2v_fit.predict_proba(fnl_cv_regressors)\n","#print(fnl_cv_targets_prob_d2v)\n","roc_auc_d2v_val_bnb = roc_auc_score(fnl_cv_targets, fnl_cv_targets_prob_d2v_bnb, multi_class=\"ovo\", average=\"macro\")\n","\n","print(\"Doc2Vec BNB Val AUC Score:  \", round(roc_auc_d2v_val_bnb,4))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Doc2Vec BNB Train AUC Score: 0.9682\n","Doc2Vec BNB Val AUC Score:   0.8546\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2qRwIp9PcTMC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621714529863,"user_tz":360,"elapsed":214,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"160b6eb1-9384-4ef1-959b-72a2e9701ed0"},"source":["'''\n","Code for creating a nice table for model comparison results\n","'''\n","x = PrettyTable()\n","x.field_names = [\"Model\",\"Transformation\",'AUC Train Score' ,'AUC Val Score']\n","\n","x.add_row([\"Bernoulli Naive Bayes\", \"Doc2Vec\", round(roc_auc_d2v_dev_bnb,4), round(roc_auc_d2v_val_bnb,4)])\n","x.add_row([\"Bernoulli Naive Bayes\", \"Count Vectorizer\",round(roc_auc_cntvect_dev_bnb,4),round(roc_auc_cntvect_val_bnb,4)])\n","x.add_row([\"Bernoulli Naive Bayes\", \"Tf-idf\",round(roc_auc_tfidf_dev_bnb,4),round(roc_auc_tfidf_val_bnb,4)])\n","\n","print(x)\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["+-----------------------+------------------+-----------------+---------------+\n","|         Model         |  Transformation  | AUC Train Score | AUC Val Score |\n","+-----------------------+------------------+-----------------+---------------+\n","| Bernoulli Naive Bayes |     Doc2Vec      |      0.9682     |     0.8546    |\n","| Bernoulli Naive Bayes | Count Vectorizer |      0.9356     |     0.8323    |\n","| Bernoulli Naive Bayes |      Tf-idf      |      0.9056     |     0.7832    |\n","+-----------------------+------------------+-----------------+---------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sve5bWH_9R5k"},"source":["# Task 4 Hypertune to reduce overfitting"]},{"cell_type":"code","metadata":{"id":"qYr6TU9c8qmn","executionInfo":{"status":"ok","timestamp":1621718986374,"user_tz":360,"elapsed":206,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}}},"source":["'''\n","Overfitting and underfitting\n","1. When Laplace smoothing alpha is set to 0- then it overfits the model- high variance model.\n","2. When alpha set to a very high value- then a data point which occurs a very few times would also have a high accuracy \n","resulting in underfittng — biased model.\n","4.1 Grid Search 5 fold CV to see if we can reduce overfitting\n","parameters tested will be \n","alpha = [1.0,2.0,3.0,4.0,5.0]\n","'''\n","# Create the parameter grid \n","param_grid = {\n","    'alpha': [1.0,2.0,3.0,4.0,5.0]\n","}\n","# Create a based model\n","bnb = BernoulliNB()\n","# Instantiate the grid search model\n","grid_search = GridSearchCV(estimator = bnb, param_grid = param_grid, scoring= 'roc_auc_ovo', cv = 5, verbose = 2)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L7LlmLZR8zVo","executionInfo":{"status":"ok","timestamp":1621719021141,"user_tz":360,"elapsed":19324,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"b0ea630e-2566-43b7-ca99-cfd48c7c78db"},"source":["# 4.2 Fit the grid search to the doc2vec trained data since this provided the best accuracy of all 3 transformations\n","grid_search.fit(fnl_dev_regressors, fnl_dev_targets)\n","best_grid = grid_search.best_estimator_\n","\n","print(grid_search.best_params_)\n","print(best_grid)\n","print(grid_search.cv_results_)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Fitting 5 folds for each of 5 candidates, totalling 25 fits\n","[CV] alpha=1.0 .......................................................\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"],"name":"stderr"},{"output_type":"stream","text":["[CV] ........................................ alpha=1.0, total=   0.8s\n","[CV] alpha=1.0 .......................................................\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV] ........................................ alpha=1.0, total=   0.8s\n","[CV] alpha=1.0 .......................................................\n","[CV] ........................................ alpha=1.0, total=   0.8s\n","[CV] alpha=1.0 .......................................................\n","[CV] ........................................ alpha=1.0, total=   0.8s\n","[CV] alpha=1.0 .......................................................\n","[CV] ........................................ alpha=1.0, total=   0.8s\n","[CV] alpha=2.0 .......................................................\n","[CV] ........................................ alpha=2.0, total=   0.8s\n","[CV] alpha=2.0 .......................................................\n","[CV] ........................................ alpha=2.0, total=   0.7s\n","[CV] alpha=2.0 .......................................................\n","[CV] ........................................ alpha=2.0, total=   0.7s\n","[CV] alpha=2.0 .......................................................\n","[CV] ........................................ alpha=2.0, total=   0.7s\n","[CV] alpha=2.0 .......................................................\n","[CV] ........................................ alpha=2.0, total=   0.8s\n","[CV] alpha=3.0 .......................................................\n","[CV] ........................................ alpha=3.0, total=   0.7s\n","[CV] alpha=3.0 .......................................................\n","[CV] ........................................ alpha=3.0, total=   0.8s\n","[CV] alpha=3.0 .......................................................\n","[CV] ........................................ alpha=3.0, total=   0.7s\n","[CV] alpha=3.0 .......................................................\n","[CV] ........................................ alpha=3.0, total=   0.7s\n","[CV] alpha=3.0 .......................................................\n","[CV] ........................................ alpha=3.0, total=   0.8s\n","[CV] alpha=4.0 .......................................................\n","[CV] ........................................ alpha=4.0, total=   0.7s\n","[CV] alpha=4.0 .......................................................\n","[CV] ........................................ alpha=4.0, total=   0.7s\n","[CV] alpha=4.0 .......................................................\n","[CV] ........................................ alpha=4.0, total=   0.7s\n","[CV] alpha=4.0 .......................................................\n","[CV] ........................................ alpha=4.0, total=   0.8s\n","[CV] alpha=4.0 .......................................................\n","[CV] ........................................ alpha=4.0, total=   0.8s\n","[CV] alpha=5.0 .......................................................\n","[CV] ........................................ alpha=5.0, total=   0.7s\n","[CV] alpha=5.0 .......................................................\n","[CV] ........................................ alpha=5.0, total=   0.8s\n","[CV] alpha=5.0 .......................................................\n","[CV] ........................................ alpha=5.0, total=   0.8s\n","[CV] alpha=5.0 .......................................................\n","[CV] ........................................ alpha=5.0, total=   0.8s\n","[CV] alpha=5.0 .......................................................\n","[CV] ........................................ alpha=5.0, total=   0.8s\n","{'alpha': 1.0}\n","BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n","{'mean_fit_time': array([0.12455645, 0.11872535, 0.11887136, 0.12011218, 0.12114744]), 'std_fit_time': array([0.00657473, 0.00111062, 0.00228002, 0.00224463, 0.0048651 ]), 'mean_score_time': array([0.65118241, 0.63013186, 0.63081613, 0.62681923, 0.63890057]), 'std_score_time': array([0.00967311, 0.00486032, 0.00912582, 0.00682194, 0.00595291]), 'param_alpha': masked_array(data=[1.0, 2.0, 3.0, 4.0, 5.0],\n","             mask=[False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'alpha': 1.0}, {'alpha': 2.0}, {'alpha': 3.0}, {'alpha': 4.0}, {'alpha': 5.0}], 'split0_test_score': array([0.95582095, 0.95297177, 0.94955169, 0.94578998, 0.94194779]), 'split1_test_score': array([0.94932404, 0.94706809, 0.94451696, 0.94132572, 0.93759028]), 'split2_test_score': array([0.96365597, 0.9612407 , 0.95835067, 0.95514111, 0.95169281]), 'split3_test_score': array([0.96680006, 0.96461282, 0.96182384, 0.95891856, 0.95557868]), 'split4_test_score': array([0.97596531, 0.9740833 , 0.97187069, 0.96920359, 0.96634878]), 'mean_test_score': array([0.96231326, 0.95999534, 0.95722277, 0.95407579, 0.95063167]), 'std_test_score': array([0.00915623, 0.00935721, 0.00956333, 0.00984537, 0.01017953]), 'rank_test_score': array([1, 2, 3, 4, 5], dtype=int32)}\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   18.9s finished\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrn971Uz859p","executionInfo":{"status":"ok","timestamp":1621719049436,"user_tz":360,"elapsed":202,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"4329209b-0c0e-4a3e-8a45-8b0d63cbd234"},"source":["'''\n","4.2 BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n","'''\n","print(best_grid)\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L705QBsM8-1F","executionInfo":{"status":"ok","timestamp":1621719082530,"user_tz":360,"elapsed":215,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"6bbf0b69-fe21-40ad-84c6-e6dd842b5a6a"},"source":["# 4.2 {'alpha': 1.0}\n","print(grid_search.best_params_)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["{'alpha': 1.0}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aXzoZONO9B8h","executionInfo":{"status":"ok","timestamp":1621719086137,"user_tz":360,"elapsed":220,"user":{"displayName":"Laura Elliott","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR_lgn2QjRHbMFy-LVQ-MZiwLY0GVJVCQPQcKj=s64","userId":"17671003486597139296"}},"outputId":"8a5ac236-3ba5-430d-d453-c8c802c1181f"},"source":["print(grid_search.cv_results_)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["{'mean_fit_time': array([0.12455645, 0.11872535, 0.11887136, 0.12011218, 0.12114744]), 'std_fit_time': array([0.00657473, 0.00111062, 0.00228002, 0.00224463, 0.0048651 ]), 'mean_score_time': array([0.65118241, 0.63013186, 0.63081613, 0.62681923, 0.63890057]), 'std_score_time': array([0.00967311, 0.00486032, 0.00912582, 0.00682194, 0.00595291]), 'param_alpha': masked_array(data=[1.0, 2.0, 3.0, 4.0, 5.0],\n","             mask=[False, False, False, False, False],\n","       fill_value='?',\n","            dtype=object), 'params': [{'alpha': 1.0}, {'alpha': 2.0}, {'alpha': 3.0}, {'alpha': 4.0}, {'alpha': 5.0}], 'split0_test_score': array([0.95582095, 0.95297177, 0.94955169, 0.94578998, 0.94194779]), 'split1_test_score': array([0.94932404, 0.94706809, 0.94451696, 0.94132572, 0.93759028]), 'split2_test_score': array([0.96365597, 0.9612407 , 0.95835067, 0.95514111, 0.95169281]), 'split3_test_score': array([0.96680006, 0.96461282, 0.96182384, 0.95891856, 0.95557868]), 'split4_test_score': array([0.97596531, 0.9740833 , 0.97187069, 0.96920359, 0.96634878]), 'mean_test_score': array([0.96231326, 0.95999534, 0.95722277, 0.95407579, 0.95063167]), 'std_test_score': array([0.00915623, 0.00935721, 0.00956333, 0.00984537, 0.01017953]), 'rank_test_score': array([1, 2, 3, 4, 5], dtype=int32)}\n"],"name":"stdout"}]}]}